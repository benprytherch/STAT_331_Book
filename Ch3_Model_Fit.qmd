# Chapter 3: Assessing and improving model fit

## Part 1: assumptions and assumption violations

## Outline of notes:

1.  Regression assumptions
2.  Linearity
3.  Normality of residuals
4.  Homogeneity of variance
5.  Influential observations
6.  (Multi)collinearity

### Violating model assumptions

‚Ä¢ The previous notes outlined the fundamentals of specifying, fitting, and interpreting a linear regression model.

‚Ä¢ These notes focus on the assumptions that our models are based upon, and how we check to see if they are being violated.

‚Ä¢ If model assumptions are violated, DON'T PANIC! There is nearly always a remedy. In these notes we will look at how to spot violations of assumptions. In the next set of notes we will look at ways to remedy these violations, and how to decide if they pose a serious problem to the usefulness of the model.

### The regression model and what it assumes

‚Ä¢ Once again, here is the regression model:

$$
Y_i = \beta_0 + \beta_1x_{1i} + \beta_2x_{2i} + \dots + \beta_px_{pi} + \epsilon_i, \text{ where } \epsilon_i \sim Normal(0,\sigma)
$$

-   This assumes:

    -   That the response variable is a linear (straight line) function of the predictor variables

    -   That the residuals will be normally distributed

    -   That the standard deviation of the residuals does not vary

    -   That the residuals are independent

### Linearity

‚Ä¢ Remember the "simple" (i.e. single predictor) regression model

$$
Y_i = \beta_0 + \beta_1x_{i} + \epsilon_i, \text{ where } \epsilon_i \sim Normal(0,\sigma)
$$

‚Ä¢ This is linear in that it fits a straight line to the two-dimensional data.

‚Ä¢ A two-predictor model would fit a flat plane to the three-dimensional data, and so on

‚Ä¢ Here's a bad idea: fitting a linear model to non-linear data!

*INSERT IMAGE*

### Diagnosing non-linearity

‚Ä¢ When running "Linear Regression" in jamovi, a "residuals by predicted" plot can be created by selecting "Residual plots" under "Assumption Checks"

‚Ä¢ The residuals are the differences between each observed values of the response variable and the value that the model predicts:

$$
residual_i = y_i - \hat{y}_i = y_i - (\hat{\beta}_0 + \hat{\beta}_1x_1 + \hat\beta_2x_2 + \dots)
$$

*INSERT IMAGE*

‚Ä¢ For simple regression, this plot just looks like the regression plot with the line turned horizontally.

‚Ä¢ For multiple regression, there is no (two dimensional) "regression plot", so the residual plot will be very useful!

*INSERT IMAGE*

‚Ä¢ In this example, there is clear curvature in the data. A straight line model is not appropriate.

‚Ä¢ Here's an example of what a linear relationship might look like:

*INSERT IMAGE*

‚Ä¢ When there is non-linearity, you will see the residuals mostly on one side of zero, then on the other size of zero, then back again, and so on.

‚Ä¢ When there is linearity, the residuals should randomly fall on either side of zero.

*INSERT IMAGE*

### What to look for in a residual plot

‚Ä¢ We will look at many more examples of residual plots in these notes.

-   We want a residual plot that appears to agree with the model assumptions:

    -   Straight line relationship between the predictors and response

    -   Normally distributed random residuals around this line

    -   Equal variance in residuals across line

### Normality of residuals

‚Ä¢ The "error term" in a regression model is that $+ \epsilon_i$ on the end

‚Ä¢ When we write $\epsilon_i \sim Normal(0, \sigma)$, we are saying that the errors (aka residuals) are normally distributed, with mean zero and some standard deviation $\sigma$.

‚Ä¢ This can be assessed visually: run the regression plot the residuals to see if they appear roughly normally distributed.

#### The QQ plot

‚Ä¢ When fitting a model using "Linear Regression" in jamovi, there is an option to save residuals. This will create a new column with a residual for each row.

‚Ä¢ The option is under the last drop-down menu, under "Save".

*INSERT IMAGE*

‚Ä¢ To create a plot of the residuals, select "Q-Q plot of residuals" under "Assumption Checks" in "Linear Regression"

‚Ä¢ The Normal Quantile plot is also known as the QQ plot, for "quantile quantile".

‚Ä¢ It is easier to assess normality with a QQ plot than with a histogram.

*INSERT IMAGE*

#### Assessing normality with a QQ plot

‚Ä¢ On Canvas under Simulations there is a "QQ plot generator" app.

‚Ä¢ This app allows you to manipulate a distribution, sample from it, and then view a histogram next to a QQ plot. It is meant to give you an idea of how QQ plots work.

‚Ä¢ By default, it draws data from a normal distribution. But, you can add "skewedness" or "peakedness" (aka kurtosis) to make the distribution non-normal. You can also adjust sample size.

I*NSERT IMAGE*

I*NSERT IMAGE*

‚Ä¢ A QQ plot shows you how much the distribution of your data "agree" with a normal distribution.

‚Ä¢ The horizonal axis gives the distribution data would follow if it were perfectly normal.

‚Ä¢ The vertical axis gives the distribution your data actually follows.

‚Ä¢ The diagonal line shows perfect agreement between the two.

I*NSERT IMAGE*

*NSERT IMAGE*

‚Ä¢ The big advantage of the QQ plot vs. the histogram is that very often data that come from a normal distribution don't look normal, especially if the sample size is small.

‚Ä¢ In this case, the histogram isn't clearly normal. But, on the QQ plot the data are close to the line.

*NSERT IMAGE*

*NSERT IMAGE*

*NSERT IMAGE*

‚Ä¢ Notice that the data still veer from the diagonal line to some extent, even though we know for a fact they came from a normal distribution.

#### Limitations of QQ plots

‚Ä¢ As you can see from the app, sometimes data that come from a normal distribution don't sit right on the line.

‚Ä¢ Sometimes data that come from a skewed distribution look similar to data that come from a normal distribution

‚Ä¢ It's easier to assess normality when sample sizes are larger.

‚Ä¢ As it turns out, the assumption of normality is not vital to the validity of a regression model. If the QQ plot is vague, you're probably fine. We only worry when we see extreme non-normality.

#### Tests for normality (not recommended)

‚Ä¢ There are statistical tests, such as "Shapiro-Wilks" or "Kolmgorov-Smirnov", for which the null hypothesis is that the data come from some specified distribution, like the normal.

‚Ä¢ Rejecting this null means that the data "significantly" disagree with the assumption of normality.

‚Ä¢ I do not recommend using this test. The problem is that, when the sample size is large enough, even small deviations from normality will be statistically significant. But small deviations from normality are OK. Only major deviations are concerning.

### The homogeneity of variance assumption

‚Ä¢ Back to the error term:

$\epsilon_i \sim Normal(0, \sigma)$

‚Ä¢ Notice that $\sigma$ is just one number. This suggests that the standard deviation of the residuals should be the same across all values of predictor variables. In other words, there is homogeneity of variance.

‚Ä¢ Another name for this is "homoscedasticity". If this assumption is violated, then we have "heteroscedasticity".

#### Heterogeneity of variance

‚Ä¢ To show heterogeneity of variance, I'll simulate data that is a function of an X variable, plus random values from a normal distribution with standard deviation equal to X.

‚Ä¢ Thus, the standard deviation of residuals will get bigger as X gets bigger:

*INSERT IMAGE*

#### The residuals vs fitted plot

‚Ä¢ Here is the regression plot and residual plot when this simulated variable (called "W" here) is the response and X is the predictor:

*INSERT IMAGE*

‚Ä¢ Notice that the residuals are more spread out for larger X

‚Ä¢ We also see "heavy tails" when plotting the residuals with a histogram and QQ plot:

*INSERT IMAGE*

‚Ä¢ Heavy tails refers to a distribution with outliers on both ends.

‚Ä¢ This shows up on the QQ plot as the residuals being too flat in the middle and then curving out on both ends.

### Influential observations (outliers)

‚Ä¢ Outliers in regression can be seen on a residual plot, or on a QQ plot, or on just a regular plot of the data.

‚Ä¢ Example: the Florida election data.

*INSERT IMAGE*

‚Ä¢ Outliers can "pull" on the regression line, especially if they are far away from the mean of the predictor(s).

‚Ä¢ There are many statistics that assess influence. jamovi will calculate one of the most popular: a Cook's Distance

### Cook's Distances

‚Ä¢ As we saw in the Florida election example, removing the outlier (Palm Beach County) had a substantial effect on the regression results.

‚Ä¢ The logic behind Cook's Distance is to quantify what happens to the regression model when a single observation is removed. This is sometimes referred to as a "leave one out" method.

‚Ä¢ Cook's Distances quantify how much the predicted values of the response variable change when an observation is removed.

‚Ä¢ Recall that, in simple regression, the predicted values are the values on the regression line.

‚Ä¢ It is hard to interpret the actual values for Cook's Distances. Values greater than 1 are often considered "influential".

‚Ä¢ The formula shows that it is based on the sum of the differences in predicted values between a model with the data point included and a model with it removed:

$$
\text{Cook's Distance for data point } "i" = D_i = \frac{\sum^n_{j=1}(\hat{y}_i - \hat{y}_{j(i)})^2}{MSE * p}
$$

‚Ä¢ Where $\hat{y}_{j(i)}$ is the predicted value of the response variable when the model is re- fit with the $i^{th}$ data point removed, and $p$ is the number of predictor variables in

‚Ä¢ A Cook's Distance is calculated for every data point. The option to do this in jamovi is under "Save" in "Linear Regression". This creates a new column with a Cook's distance for each row.

‚Ä¢ The saved Cook's Distances can then be plotted. Any possibly influential points will usually stand out clearly from the rest.

*INSERT IMAGE*

‚Ä¢ To quickly narrow in on the influential counties, we can filter out all the small Cook's distances.

*INSERT IMAGE*

‚Ä¢ After implementing the filter, we can see that all rows which do not meet the criteria of the filter are excluded.

*INSERT IMAGE*

*INSERT IMAGE*

‚Ä¢ Only rows 13 and 50 should be highlighted for Dade and Palmbeach county which have cook's distances of 1.983 and 3.786.

### (Multi)collinearity

‚Ä¢ In regression analysis, we want our predictor variables to be correlated with the response variable.

‚Ä¢ But we don't want our predictor variables to be (highly) correlated with one another!

‚Ä¢ When two predictor variables are highly correlated, we say our model has "collinearity."

‚Ä¢ When more than two predictor variables are mutually highly correlated, we say our model as "multicollinearity".

### Why don't we want correlated predictors?

‚Ä¢ To understand why we don't want correlated predictors, recall that multiple regression models estimate the association between each individual predictor variable and the response, $\textit{while holding all other predictor variables constant}$.

‚Ä¢ This can be thought of as asking "what is the difference in the response variable when we observe data points that have different values of one predictor but the same values of the other predictors?"

‚Ä¢ If $Y$ is the response and $x_1$ and $x_2$ are predictors, we want to know how different $Y$ is when $x_1$ values differ but $x_2$ values are the same, or vice versa.

‚Ä¢ But, if $x_1$ and $x_2$ are highly correlated, then we don't get to observe cases where values of one variable differ substantially while values of the other are the same! Consider instances of no correlation vs. heavy correlation:

*INSERT IMAGE*

*INSERT IMAGE*

‚Ä¢ When $x_1$ and $x_2$ are uncorrelated, we see lots of instances of $x_1$ values differing a lot when $x_2$ values are equal.

*INSERT IMAGE*

‚Ä¢ When $x_1$ and $x_2$ are highly correlated, we never see instances where $x_2$ values are equal but $x_1$ values are highly correlated.

*INSERT IMAGE*

‚Ä¢ The upshot is that, when $x_1$ and $x_2$ are highly correlated, the regression procedure has a difficult time distinguishing between the "effect" of $x_1$ on $Y$ and the "effect" of $x_2$ on $Y$.

‚Ä¢ Extreme example: if we had degrees Celsius and degrees Fahrenheit as predictors in the same model, it would be impossible to tell their effects apart. You can't change Celsius while holding Fahrenheit constant!

‚Ä¢ The practical consequence is that the standard errors for the slopes of (multi)collinear predictors are much larger than they would be if it were not for the (multi)collinearity.

‚Ä¢ If two or more predictors are perfectly correlated ($r=1$), then the model cannot be fit and jamovi produces an error:

*INSERT IMAGE*

‚Ä¢ Here, $X1$ and $X2$ are perfectly correlated. jamovi cannot estimate a slope for $X2$.

### Collinearity example: Florida election data

‚Ä¢ In the Florida election data, we used total votes for each county as our predictor variable.

‚Ä¢ There is another variable called "Total_Reg". This is the total number of registered voters in each county.

‚Ä¢ Unsurprisingly, Total_Votes and Total_Reg are highly correlated:

*INSERT IMAGE*

‚Ä¢ If we run two separate simple regression models, we get very similar results:

$$
Buchanin_i = \beta_0 + \beta_1Total\_Votes_i + \epsilon_i
$$

*INSERT IMAGE*

$$
Buchanin_i = \beta_0 + \beta_1Total\_Reg_i + \epsilon_i
$$

*INSERT IMAGE*

‚Ä¢ But look what happens if we use Total_Votes and Total_Reg as predictors in the same model:

$$
Buchanin_i = \beta_0 + \beta_1Total\_Votes_i + \beta_2Total\_Reg_i + \epsilon_i
$$

*INSERT IMAGE*

‚Ä¢ Two important things to note:

‚Ä¢ P-values on slopes are much larger than for the individual models

‚Ä¢ $R^2$ is larger than on either individual model!

‚Ä¢ Looking at the estimates and standard errors in all three models, we see that the standard errors are much larger in the multiple regression model. These estimates are "unstable" -- their values will change a lot if the data change a little.

*INSERT IMAGE* x3

‚Ä¢ We know the association between Total_Reg are Buchanan is actually positive. But with so high a standard error, the slope for Total_Reg turned out negative!

#### Variance inflation factor (VIF)

‚Ä¢ (Multi)collinearity can be assessed using a "Variance Inflation Factor", or VIF. A VIF is calculated for the $j^{th}$ predictor variable as:

$$
VIF_j = \frac{1}{1-R^2_j}
$$

‚Ä¢ Where $R^2_j$ is the $R^2$ from a regression model with predictor $j$ as the response variable and all other predictors still as predictors.

‚Ä¢ In the Florida election example, the VIF for Total_Votes can be found using the $R^2$ for the model:

$$
Total\_Votes_i = \beta_0 + \beta_1Total\_Reg_i + \epsilon_i
$$

*INSERT IMAGE*

\
‚Ä¢ This $R^2$ is huge! Plugging it into the formula:

$$
VIF_{Total\_Votes}=\frac{1}{1-0.997} = 333.33
$$

‚Ä¢ Thankfully we don't have to do this by hand. In jamovi, under "Linear Regression" select "Collinearity statistics":

*INSERT IMAGE x2*

‚Ä¢ VIF \> 10 typically is considered large (note that this would imply $R^2 = 0.9$ between predictor variables).

‚Ä¢ The most obvious thing to do in the presence of (multi)collinearity is to remove one or more correlated predictor variable. From a scientific standpoint, you also may not want highly correlated predictors in the same model.

#### When should we worry about (multi)collinearity?

‚Ä¢ (Multi)collinearity is a potentially huge problem if the goal of the regression model is to interpret the estimated slopes.

‚Ä¢ This is because it increases the standard error of these slopes, making their values less reliable. Some people say it makes slopes "unstable".

‚Ä¢ It may also complicate the interpretation of slopes: you are trying to statistically "hold constant" a predictor variable that doesn't naturally stay constant when the other predictor varies. This isn't necessarily a problem, but it is something to be aware of.

‚Ä¢ However, (multi)collinearity does not negatively impact the predicted values themselves. Remember that it didn't hurt the $R^2$ value in the Florida election example. $R^2$ tells you how good your predictions are.

‚Ä¢ So, if the model is only for predicting, you probably don't need to worry about using correlated predictor variables. Just beware when interpreting the slopes.

## Part 2: Improving models

### Summary of part 1

-    In the last set of notes, we looked at some things that can go wrong in regression modeling, including:

    -    Non-linear relationships between predictor(s) and response

    -   Non-normality of residuals

    -   Non-constant (heterogeneous) variance of residuals

    -   Influential outliers

    -   Multicollinearity

‚Ä¢ In these notes, we'll look at some tools available for dealing with these problems.

### Transforming variables

‚Ä¢ Recall the regression model:

$$
Y_i = \beta_0 + \beta_1x_{1i} + \beta_2x_{2i} + \dots + \beta_px_{pi} + \epsilon_i, \text{ where } \epsilon_i \sim Normal(0,\sigma)
$$

‚Ä¢ Sometimes we can correct violations of model assumptions by applying a mathematical transformation to the response or predictor variables.

‚Ä¢ The most common transformation in Statistics is the log transformation:

$$
ln(x) = log_e(x)
$$

### Log transformation

‚Ä¢ $ln(ùë•)$ is the inverse function of $e^ùë•$, where $ùëí = 2.718 \dots$

‚Ä¢ In other words, $ln(e^x) = x$

‚Ä¢ Example: $ùëí^3 = 20.086; ln(20.086) = 3$

‚Ä¢ So, the natural log of $x$ is the number you would have to raise $e$ to so that you'd get $x$.

‚Ä¢ Note: in statistics, when we say "log", we usually mean "natural log". It turns out that the distinction is not very important. I'll say "log transform"

#### Why log transform?

-   There are two main reasons for log transforming a variable:

    -   To correct for skew in data or residuals

    -   To interpret increases in a variable as multiplicative rather than additive.

‚Ä¢ Both can be understood by recognizing an important property of logarithms; they "turn addition into multiplication"

$$
log(ùê¥) + log(ùêµ) = log(ùê¥ùêµ)
$$

‚Ä¢ In this sense, logarithms turn addition into multiplication.

‚Ä¢ Example: suppose we have data for a skewed variable $X_1$:

*INSERT IMAGE* x2

‚Ä¢ Now we define $x_2 = ln(x_1)$:

*INSERT IMAGE*

‚Ä¢ This is a toy "data set". I chose $x_1$ so that $x_2 = ln(x_1)$ would just be the integers $1$ through $10$.

‚Ä¢ Note: there is no more skew.

*INSERT IMAGE*

‚Ä¢ Also note: increasing $x_2$ by one unit results in multiplying $x_1$ by $e$. Addition in $x_2 = ln(x_1)$ is the same thing as multiplication in $x_1$.

#### Same again, with log base 2

‚Ä¢ Even simpler: define $x_2$ as log base $2$ of $x_1$, i.e. $log_2(x_1)$

*INSERT IMAGE*

‚Ä¢ Now increasing $x_2$ by one unit is equivalent to multiplying $x_1$ by $2$.

Addition in $x_2 = log_2(x_1)$ is the same thing as multiplication in $x_1$.

#### Log transforming right-skewed data

-    Skewed data can be bad for regression, in that it can lead to:

    -   Non-linear relationship between X and Y

    -   Influential outliers

    -   Non-normal residuals

    -   Non-constant variance in residuals

‚Ä¢ So a simple log transformation can sometimes go a long way toward making the regression model fit the better!

‚Ä¢ It is most common to log transform a response variable, because assumptions about residuals apply to $Y$, not $X$.

‚Ä¢ But if $X$ is skewed, the model can benefit from a log transformation of $X$.

‚Ä¢ Bear in mind that log transformation will affect the interpretation of slope coefficients!

‚Ä¢ If $X$ is log transformed, then a one unit increase in $ln(ùëã)$ corresponds to multiplying $X$ by $e \approx 2.72$. So the slope for $ln(ùëã)$ tells you how much $Y$ increases when $X$ is multiplied by $2.72$. Or, even better, use log base $2$ and the slope will give how much $Y$ changes when $X$ is doubled.

‚Ä¢ If $Y$ is log transformed, then the interpretations of slopes get more complicated. Here's the math, with the error term omitted for convenience:

$$
ln(y_i) = \beta_0 + \beta_1X_i
$$

$$
\therefore y_i = e^{\beta_0 + \beta_1X_i}
$$

‚Ä¢ Increase $X$ by $1 \dots$

$$
y_i^* = e^{\beta_0 + \beta_1(X_i + 1)} = e^{\beta_0 + \beta_1X_i} \cdot e^{\beta_1}
$$

‚Ä¢ So, when $Y$ is log transformed, a one unit increase in $X$ multiplies predicted $Y$ by $e^{\beta_1}$

#### Interpreting slope as a % change in outcome

‚Ä¢ Recall the heights vs. wages data from group project 1. The paper reported this estimated model:

$$
ln(\text{wage}) = \hat{\beta}_0 + 0.002\text{(Adult Height)} + 0.027\text{(Youth Height)} + 0.024\text{(Age)}
$$

‚Ä¢ So, when comparing two adults $1$ inch apart in height but with the same youth height and age predicted wage is multiplied by $e^{0.027} = 1.027$ for the taller adult.

‚Ä¢ Multiplying by $1.027$ can be thought of as increasing by $2.7\%$

#### Log transformation applied example

‚Ä¢ Here is the percent change formula:

$$
\%\text{ change (from A to B)} = \frac{B-A}{A}*100\%
$$

\% ùëê‚Ñéùëéùëõùëîùëí (ùëìùëüùëúùëö ùê¥ ùë°ùëú ùêµ) =

‚Ä¢ If B is $1.027*$A, then

$$
\%\text{ change} = \frac{1.027*A - A}{A}*100 = \frac{0.027A}{A}*100 = 2.7\%
$$

‚Ä¢ So, when comparing two adults 1 inch apart in height but with the same youth height and age, predicted wage is $2.7\%$ higher for the taller adult.

#### Log transformation in $Y$ vs. in $X$

‚Ä¢ Remember that log transformation "turns addition into multiplication". So, to keep track of how log transforming $Y$ vs. log transforming $X$ affects your model:

$$
\text{log}(Y_i) = \beta_0 + \beta_1X_i + \epsilon_i
$$

$$
\text{vs.}
$$

$$
Y_i = \beta_0 + \beta_1\text{log}(X_i) + \epsilon_i
$$

‚Ä¢ If you log transform $Y$ but not $X$, your model estimates the multiplicative change in predicted $Y$ for an additive change in $X$.

‚Ä¢ If you log transform $X$ but not $Y$, your model estimates the additive change in predicted $Y$ for a multiplicative change in $X$.

### Non-linearity

‚Ä¢ Sometimes data show obvious curvature, in the sense that $Y$ is clearly not a straight line function of $X$.

‚Ä¢ This will be visible on a plot of $Y$ vs. $X$. It will also be visible on a residuals vs. predicted values plot after running a regression.

‚Ä¢ If there is curvature in the relationship between $Y$ and $X$, then it might be sensible to add a polynomial $X$ term:

$$
Y_i = \beta_0 + \beta_1x_i + \beta_2x_i^2 + \epsilon_i
$$

#### "Polynomial" review

‚Ä¢ A "polynomial" expression is typically one in which variables are included at different powers. For example, a generic third degree polynomial equation might look like:

$$
y = a + bx + cx^2 + dx^3
$$

‚Ä¢ A "second degree" polynomial is one in which an $x$ and $x^2$ term are both included. This is by far the most common type of polynomial seen in regression models.

#### $2^{nd}$ degree and $3^{rd}$ degree polynomials

‚Ä¢ $2^{nd}$ degree polynomials are often called "quadratic". $3^{rd}$ degree polynomials are often called "cubic". Here are visual examples of simulated quadratic and cubic relationships between $Y$ and $X$:

*INSERT IMAGE*

#### Curvature in residuals

‚Ä¢ Here is regression output comparing a linear model to a quadratic model when the relationship between $Y$ and $X$ is quadratic:

*INSERT IMAGE*

#### Example: Florida election data

‚Ä¢ Here is what the Florida election data look like with the Palm Beach County outlier removed, along with regression results for the simple linear regression model:

$$
Buchanan_i = \beta_0 + \beta_1Total\_Votes_i + \epsilon_i
$$

*INSERT IMAGE*

‚Ä¢ Now we will fit a quadratic polynomial model to the same data:

$$
Buchanan_i = \beta_0 + \beta_1Total\_Votes_i + \beta_2Total\_Votes^2_i + \epsilon_i
$$

‚Ä¢ To create this polynomial predictor in jamovi we first need to install the GAMLj module. Then, select "Generalized Linear Models" and last our "Dependent Variable" and "Covariates".

‚Ä¢ Under the "Model" drop down menu, click on Total_Votes in the "Components" table.

‚Ä¢ An up and down arrow will appear with the degree of Total_Votes, click the up arrow so the 1 changes to 2. Then click the right arrow to add Total_Votes2 to "Model Terms"

*INSERT IMAGE*

$$
Buchanan_i = \beta_0 + \beta_1Total\_Votes_i + \beta_2Total\_Votes^2_i + \epsilon_i
$$

*INSERT IMAGE*

‚Ä¢ This is better, but we still see curvature in the residual plot.

‚Ä¢ Let's try a cubic model:

$$
Buchanan_i = \beta_0 + \beta_1Total\_Votes_i + \beta_2Total\_Votes^2_i +\beta_3Total\_Votes_i^3 + \epsilon_i
$$

*INSERT IMAGE*

$$
Buchanan_i = \beta_0 + \beta_1Total\_Votes_i + \beta_2Total\_Votes^2_i +\beta_3Total\_Votes_i^3 + \epsilon_i
$$

*INSERT IMAGE*

#### Example: Florida election data: check note

‚Ä¢ It's debatable whether this is much better. For one, the $Total\_Votes^2$ term is non-significant.

‚Ä¢ But think back to multicollinearity. Each polynomial term will be correlated with the other terms -- after all, $Total\_Votes$, $Total\_Votes^2$, and $Total\_Votes^3$ must all be correlated.

‚Ä¢ Note that jamovi will not produce VIFs in GLM. Intuitively we know these will be highly correlated with each other.

‚Ä¢ It turns out that centering helps in polynomial models:

*INSERT IMAGE*

-   Compare to:

*INSERT IMAGE*

‚Ä¢ By default jamovi centers polynomials which makes their standard errors smaller than if they were not centered. But, these estimates are not interpretable; you cannot hold $Total\_Votes^2$ constant while increasing $Total\_Votes$.

‚Ä¢ In this example, the two counties with the highest total votes are heavily pulling on the regression line.

*INSERT IMAGE*

### Over-fitting

‚Ä¢ This model might be "over-fit".

‚Ä¢ Over-fitting is when a model fits the data so well that it ends up fitting random variation that is not of interest.

*INSERT IMAGE*

‚Ä¢ Imagine if the two data points on the right had slightly higher vote counts for Buchanan. Or if the next two to their left had slightly lower vote counts. The curve would look very different.

‚Ä¢ At this point, we might just be modelling noise.

‚Ä¢ Here is an extreme example of over-fitting: fitting a "smoother" curve to data and giving it permission to move dramatically up and down through the data.

*INSERT IMAGE*

‚Ä¢ This line fits the data very well, but does it represent the general trend between total votes and votes for Buchanan? Definitely not!

‚Ä¢ (Side note: "smoothers" are great tools for visualizing and summarizing data, but they can be extremely sensitive to degree of smoothing. We won't use them in STAT 331)

‚Ä¢ Compare the over-fit model to the linear model.

*INSERT IMAGE x2*

‚Ä¢ The linear model may be missing out on some curvature. But it might also make better predictions.

‚Ä¢ If we were to observe a new county with $450,000$ total votes, would we be better guessing that votes for Buchanan fall on the highly curved line or on the straight line?

### Back to basics: is the model sensible?

-   Back to basics: regression models are typically used for two purposes:

    -   Predicting values of the response variable, using the predictor variables. This is done by plugging values for the predictor variables into the estimated model.

    -   Estimating the association between each individual predictor variable and the outcome while statistically holding other predictor variables constant. This is done by interpreted estimated slope coefficients.

#### If you just want to make predictions

‚Ä¢ $R^2$ is the easiest to understand statistic for assessing how well your model makes predictions. The closer to $1$, the better.

‚Ä¢ Multicollinearity isn't an issue. It doesn't affect predicted values.

‚Ä¢ BUT -- beware of overfitting! The more complex your model, the more risk you take of modeling noise instead of signal.

‚Ä¢ Also, be aware that $R^2$ can never go down when adding predictors. You can add complete nonsense predictor variables, and the worst that will happen to $R^2$ is that it stays the same.

#### If you want to interpret slopes

‚Ä¢ Always remember that each slope is interpreted under the assumption that all other predictor variables are being held constant, i.e. "controlled for".

‚Ä¢ The more predictor variables in the model, the less sense this will make.

‚Ä¢ Example: wage vs. height study:

*INSERT IMAGE*

‚Ä¢ In model 4, the estimated slope for youth height can be interpreted as:

"The predicted difference in ln(wage) for two people one inch apart in youth height, but equal in adult height, whose mothers have the same \# of years of schooling and are both in either skilled fields or work or not, whose fathers have the same \# of years of schooling and are both in either skilled fields or work or not, and who have the same number of siblings."

*INSERT IMAGE*

‚Ä¢ Maybe this is the best way to think about the association between youth height and wages. But it is fairly complicated.

‚Ä¢ If you're going to try to make "real world" sense out of regression results, your model should be informed by theory.

‚Ä¢ This is necessarily subjective! You have to choose which variables you think are important. You have to think about what makes sense.

-   This might require:

    -   Log transforming a variable solely because you like the multiplicative interpretation better than the additive interpretation.

    -   Keeping a variable in a model even though it isn't statistically significant.

    -   Removing a variable you are interested in, because it doesn't make sense to "hold it constant" when estimating slopes for other variables.

#### Is the model missing something important?

‚Ä¢ There is another variable in the Florida election data set that could be worth including: "Reg_Reform": the total number of voters registered with the Reform Party. Pat Buchanan was the Reform Party candidate. Let's add it to the model:

$$
Buchanan_i = \beta_0 + \beta_1Total\_Votes_i + \beta_2Reg\_Reform_i + \epsilon_i
$$

*INSERT IMAGE*

‚Ä¢ This residual plot looks great!

*INSERT IMAGE*

‚Ä¢ It turns out that the curvature in the previous residual plots went away when adding an important variable to the model. No need to mess with polynomials after all!

*INSERT IMAGE*

‚Ä¢ Also, the $R^2$ is roughly the same as in the cubic model using only total votes as a predictor.

‚Ä¢ So we have roughly equal fit, without having to worry about overfitting, and without having to give up interpretability of the slopes.

‚Ä¢ One downside: there is some collinearity. Look at the VIFs.

*INSERT IMAGE*

‚Ä¢ VIF of about $5$ implies $\frac{1}{1-R^2}\approx 5$ when using the $R^2$ from:

$$
Total\_Votes_i = \beta_0 + \beta_1Reg\_Reform_i + \epsilon_i
$$

‚Ä¢ So, this $R^2$ is about $1 ‚àí \frac{1}{5} = 0.8$. And so $r = \sqrt{0.8} =0.89$ . These predictors are strongly correlated.

*INSERT IMAGE*

‚Ä¢ Note also that total votes is not significant.

‚Ä¢ But: the slope for $Reg\_Reform$ has a nice interpretation:

When comparing two counties with the same number of votes cast in the election, a county with an additional registered Reform Party member is estimated to have $2.24$ additional votes, on average, for Pat Buchanan.

‚Ä¢ Should total votes be taken out of the model? This is a subjective decision.

#### What would you like to "control" for?

‚Ä¢ In regression analysis, we usually emphasize (correctly) that correlation does not imply causation.

‚Ä¢ However, if you have knowledge or beliefs about causal direction, you should take these into account when choosing your variables!

‚Ä¢ Example: in the rheumatoid arthritis study, we were looking at the effect of drugs on inflammation level. In particular, we were comparing how effective they were at reducing inflammation. Suppose we also asked patients to rate their mobility level (RA tends to reduce mobility).

‚Ä¢ Our model might be:

$$
Difference_i = \beta_0 + \beta_1age_i + \beta_2drug_i + \beta_3(age*drug)_i + \beta_4mobility_i + \epsilon_i
$$

‚Ä¢ Now, when interpreting the previous slopes, I am comparing average reduction in inflammation ("difference", the response variable), between two people who have the same mobility level. But if they have the same mobility level, then they will have more similar inflammation levels than if we allowed mobility level to differ.

‚Ä¢ In other words, because the drug reduces inflammation *and* improves mobility, "controlling" for mobility will make it look like the drugs are less effective than they really are.

#### Beware the "kitchen sink" approach

‚Ä¢ There's an old saying: "taking everything but the kitchen sink".

‚Ä¢ It can be tempting to toss everything but the kitchen sink into a regression model, especially when you have loads of variables that all seem like they'd be associated with the response.

‚Ä¢ But beware! Adding in one predictor can have a dramatic effect on the slopes of other predictors, as well as on their standard errors.

‚Ä¢ It really really *really* matters that each slope is estimated as though all other predictors are held constant. This can reveal otherwise unseen effects, but it can also obscure otherwise obvious effects or induce apparent effects that aren't real. There is no substitute for scientific reasoning when choosing a model.

#### The model is simpler than what's being modeled

‚Ä¢ Let's take a step back and ask: why are we fitting data to models?

‚Ä¢ Well, we are interested in the real world. And the real world in incredibly complicated. Maybe incomprehensibly complicated.

‚Ä¢ So, we simplify things using models. We hope that the model captures the essence of what we care about in the real world. But we know it is a simplification; perhaps an extreme simplification.

‚Ä¢ "All models are wrong; some are useful" -- George Box

‚Ä¢ Consider how the regression model describes where data comes from:

$$
Y_i = \beta_0 + \beta_1x_{i} + \epsilon_i, \text{ where } \epsilon_i \sim Normal(0,\sigma)
$$

‚Ä¢ This says that to get data, we pick a value for X, go to a line, then randomly draw a value from a normal distribution and add this to the value on the line. And that's where data comes from!

‚Ä¢ Except, that's not where data comes from. This is a model. It is a simplification of reality. We use these models because we think they will help us answer questions we care about (e.g. make predictions, identify associations between variables). Don't forget that the model is not the thing itself.

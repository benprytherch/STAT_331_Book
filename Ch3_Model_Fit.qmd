# Chapter 3: Assessing and improving model fit


## Part 1: assumptions and assumption violations
 
Outline of notes

â€¢	Regression assumptions
â€¢	Linearity
â€¢	Normality of residuals
â€¢	Homogeneity of variance
â€¢	Influential observations
â€¢	(Multi)collinearity
 

### Violating model assumptions



â€¢	The previous notes outlined the fundamentals of specifying, fitting, and interpreting a linear regression model.

â€¢	These notes focus on the assumptions that our models are based upon, and how we check to see if they are being violated.

â€¢	If model assumptions are violated, DONâ€™T PANIC! There is nearly always a remedy. In these notes we will look at how to spot violations of assumptions. In the next set of notes we will look at ways to remedy these violations, and how to decide if they pose a serious problem to the usefulness of the model.
 



### The regression model and what it assumes



â€¢	Once again, here is the regression model:

ğ‘Œğ‘– = ğ›½0 + ğ›½1ğ‘¥1ğ‘– + ğ›½2ğ‘¥2ğ‘– + â‹¯ + ğ›½ğ‘ğ‘¥ğ‘ğ‘– + ğœ€ğ‘–	ğœ€ğ‘–~ğ‘ğ‘œğ‘Ÿğ‘šğ‘ğ‘™


â€¢	This assumes:

â€¢	That the response variable is a linear (straight line) function of the predictor variables
â€¢	That the residuals will be normally distributed
â€¢	That the standard deviation of the residuals does not vary
â€¢	That the residuals are independent
 


â€¢	Remember the â€œsimpleâ€ (i.e. single predictor) regression model:



ğ‘Œğ‘– = ğ›½0 + ğ›½1ğ‘¥ğ‘– + ğœ€ğ‘–	ğœ€ğ‘–~ğ‘ğ‘œğ‘Ÿğ‘šğ‘ğ‘™

â€¢	This is linear in that it fits a straight line to the two-dimensional data.

â€¢	A two-predictor model would fit a flat plane to the three-dimensional data, and so on
 



 

â€¢	Hereâ€™s a bad idea: fitting a linear model to non-linear data!
 



 


â€¢	When running â€œLinear Regressionâ€ in jamovi, a
â€œresiduals by predictedâ€ plot can be created by
selecting â€œResidual plotsâ€ under â€œAssumption Checksâ€


â€¢	The residuals are the differences between each observed values of the response variable and the value that the model predicts:


 
ğ‘Ÿğ‘’ğ‘ ğ‘–ğ‘‘ğ‘¢ğ‘ğ‘™ğ‘– = ğ‘¦ğ‘– âˆ’
 
ğ‘¦à·œğ‘– = ğ‘¦ğ‘– âˆ’ (ğ›½áˆ˜
 
+ ğ›½áˆ˜
 
ğ‘¥1
 
+ ğ›½áˆ˜
 
ğ‘¥2
 
+ â‹¯ )
 


â€¢	For simple regression, this plot just looks like the regression plot with the line turned horizontally.

â€¢	For multiple regression, there is no (two dimensional) â€œregression plotâ€, so
the residual plot will be very useful!
 
 

â€¢	In this example, there is clear curvature in the data. A straight line model is not appropriate.

â€¢	Hereâ€™s an example of what a linear relationship might look like:
 



 

â€¢	When there is non-linearity, you will see the residuals mostly on one side of zero, then on the other size of zero, then back again, and so on.

â€¢	When there is linearity, the residuals should randomly fall on either side of zero.
 



### What to look for in a residual plot



â€¢	We will look at many more examples of residual plots in these notes.

â€¢	We want a residual plot that appears to agree with the model assumptions:

o	Straight line relationship between the predictors and response

o	Normally distributed random residuals around this line

o	Equal variance in residuals across line
 
### The normality assumption
 

â€¢	The â€œerror termâ€ in a regression model is that + ğœ€ğ‘– on the end


 
â€¢	When we write ğœ€ğ‘–~ğ‘ğ‘œğ‘Ÿğ‘šğ‘ğ‘™
 
, we are saying that the errors (aka
 
residuals) are normally distributed, with mean zero and some standard
deviation Ïƒ.

â€¢	This can be assessed visually: run the regression plot the residuals to see if they appear roughly normally distributed.
 



#### The QQ plot 

â€¢	When fitting a model using â€œLinear Regressionâ€ in jamovi, there is an option to save residuals. This will create a new column with a residual for each row.
â€¢	The option is under the last drop-down menu, under â€œSaveâ€.
 



 

â€¢	To create a plot of the residuals, select â€œQ-Q plot of residualsâ€ under â€œAssumption Checksâ€ in â€œLinear Regressionâ€

â€¢	The Normal Quantile plot is also known as the QQ plot, for â€œquantile quantileâ€.

â€¢	It is easier to assess normality with a QQ plot than with a histogram.
 



 

â€¢	On Canvas under Simulations there is a â€œQQ plot generatorâ€ app.

â€¢	This app allows you to manipulate a distribution, sample from it, and then view a histogram next to a QQ plot. It is meant to give you an idea of how QQ plots work.

â€¢	By default, it draws data from a normal distribution. But, you can add â€œskewednessâ€ or â€œpeakednessâ€ (aka kurtosis) to make the distribution non-normal. You can also adjust sample size.
 

 



 

â€¢	A QQ plot shows you how much the distribution of your data â€œagreeâ€ with a normal distribution.


â€¢	The horizonal axis gives the distribution data would follow if it were perfectly normal.
â€¢	The vertical axis gives the distribution your data actually follows.

â€¢	The diagonal line shows perfect agreement between the two.
 
 
â€¢	The big advantage of the QQ plot vs. the histogram is that very often data that come from a normal distribution donâ€™t look normal, especially if the sample size is small.
â€¢	In this case, the histogram isnâ€™t clearly normal. But, on the QQ plot the data
are close to the line.
 

 


 

â€¢	Notice that the data still veer from the diagonal line to some extent, even though we know for a fact they came from a normal distribution.
 



#### Limitations of QQ plots



â€¢	As you can see from the app, sometimes data that come from a normal
distribution donâ€™t sit right on the line.

â€¢	Sometimes data that come from a skewed distribution look similar to data that come from a normal distribution

â€¢	Itâ€™s easier to assess normality when sample sizes are larger.

â€¢	As it turns out, the assumption of normality is not vital to the validity of a regression model. If the QQ plot is vague, youâ€™re probably fine. We only worry when we see extreme non-normality.
 
#### Tests for normality (not recommended)

â€¢	There are statistical tests, such as â€œShapiro-Wilksâ€ or â€œKolmgorov-Smirnovâ€, for which the null hypothesis is that the data come from some specified distribution, like the normal.

â€¢	Rejecting this null means that the data â€œsignificantlyâ€ disagree with the
assumption of normality.

â€¢	I do not recommend using this test. The problem is that, when the sample size is large enough, even small deviations from normality will be statistically significant. But small deviations from normality are OK. Only major deviations are concerning.



### The homogeneity of variance assumption



â€¢	Back to the error term:

ğœ€ğ‘–~ğ‘ğ‘œğ‘Ÿğ‘šğ‘ğ‘™

â€¢	Notice that Ïƒ is just one number. This suggests that the standard deviation of the residuals should be the same across all values of predictor variables. In other words, there is homogeneity of variance.

â€¢	Another name for this is â€œhomoscedasticityâ€. If this assumption is violated, then we have â€œheteroscedasticityâ€.
 



#### Heterogeneity of variance



â€¢	To show heterogeneity of variance, Iâ€™ll simulate data that is a function of an X variable, plus random values from a normal distribution with standard deviation equal to X.

â€¢	Thus, the standard deviation of residuals will get bigger as X gets bigger:





 



#### The residuals vs fitted plot

â€¢	Here is the regression plot and residual plot when this simulated variable
(called â€œWâ€ here) is the response and X is the predictor:
â€¢	Notice that the residuals are more spread out for larger X
 



 

â€¢	We also see â€œheavy tailsâ€ when plotting the residuals with a histogram and
QQ plot:

â€¢	Heavy tails refers to a distribution with outliers on both ends.

â€¢	This shows up on the QQ plot as the residuals being too flat in the middle and then curving out on both ends.
 
### Influential observations


â€¢	Outliers in regression can be seen on a residual plot, or on a QQ plot, or on just a regular plot of the data.

â€¢	Example: the Florida election data.

â€¢	Outliers can â€œpullâ€ on the regression line, especially if they are far away from the mean of the predictor(s).

â€¢	There are many statistics that assess influence. jamovi will calculate one of
the most popular: a Cookâ€™s Distance
 



 

â€¢	As we saw in the Florida election example, removing the outlier (Palm Beach County) had a substantial effect on the regression results.

â€¢	The logic behind Cookâ€™s Distance is to quantify what happens to the regression model when a single observation is removed. This is sometimes referred to as a â€œleave one outâ€ method.

â€¢	Cookâ€™s Distances quantify how much the predicted values of the response
variable change when an observation is removed.

â€¢	Recall that, in simple regression, the predicted values are the values on the
 



 

â€¢	It is hard to interpret the actual values for Cookâ€™s Distances. Values greater than 1 are often considered â€œinfluentialâ€.

â€¢	The formula shows that it is based on the sum of the differences in predicted values between a model with the data point included and a model with it removed:
 

ğ¶ğ‘œğ‘œğ‘˜â€²ğ‘  ğ·ğ‘–ğ‘ ğ‘¡ğ‘ğ‘›ğ‘ğ‘’ ğ‘“ğ‘œğ‘Ÿ ğ‘‘ğ‘ğ‘¡ğ‘ ğ‘ğ‘œğ‘–ğ‘›ğ‘¡ "ğ‘–" = ğ·ğ‘– =
 
ğ‘›	2
ğ‘—=1

 
ğ‘€ğ‘†ğ¸ âˆ— ğ‘
 


â€¢	Where ğ‘¦à·œğ‘— ğ‘–	is the predicted value of the response variable when the model is re-
fit with the ğ‘–ğ‘¡â„ data point removed, and p is the number of predictor variables in
 



 

â€¢	A Cookâ€™s Distance is calculated for every data point. The option to do this in jamovi is under â€œSaveâ€ in
â€œLinear Regressionâ€. This creates a new column with a Cookâ€™s distance for each row.

â€¢	The saved Cookâ€™s Distances can then be plotted. Any possibly influential points will usually stand out clearly from the rest.
 



 

â€¢	To quickly narrow in on the influential counties, we can filter out all the small Cookâ€™s distances.

â€¢	After implementing the filter, we can see that all rows which do not meet the criteria of the filter are excluded.

â€¢	Only rows 13 and 50 should be highlighted for Dade and Palmbeach county which have cookâ€™s distances of 1.983 and 3.786.
 
### (Multi)collinearity



â€¢	In regression analysis, we want our predictor variables to be correlated with the response variable.

â€¢	But we donâ€™t want our predictor variables to be (highly) correlated with
one another!

â€¢	When two predictor variables are highly correlated, we say our model
has â€œcollinearityâ€

â€¢	When more than two predictor variables are mutually highly
correlated, we say our model as â€œmulticollinearityâ€.
 



 

â€¢	To understand why we donâ€™t want correlated predictors, recall that multiple regression models estimate the association between each individual predictor variable and the response, while holding all other predictor variables constant.

â€¢	This can be thought of as asking â€œwhat is the difference in the response variable when we observe data points that have different values of one predictor but the same values of the other predictors?

â€¢	If ğ‘Œ is the response and ğ‘¥1 and ğ‘¥2 are predictors, we want to know how different ğ‘Œ is when ğ‘¥1 values differ but ğ‘¥2 values are the same, or vice
 



 

â€¢	But, if ğ‘¥1 and ğ‘¥2 are highly correlated, then we donâ€™t get to observe cases where values of one variable differ substantially while values of the other are the same! Consider instances of no correlation vs. heavy correlation:
 
#### Why donâ€™t we want correlated predictors?



â€¢	When ğ‘¥1 and ğ‘¥2 are uncorrelated, we see lots of instances of ğ‘¥1 values differing a lot when ğ‘¥2 values are equal.


â€¢	When ğ‘¥1 and ğ‘¥2 are highly correlated, we never see instances where ğ‘¥2 values are equal but ğ‘¥1 values are highly correlated.
 



 

â€¢	The upshot is that, when ğ‘¥1 and ğ‘¥2 are highly correlated, the regression procedure has a difficult time distinguishing between the â€œeffectâ€ of ğ‘¥1 on ğ‘Œ and the â€œeffectâ€ of ğ‘¥2 on ğ‘Œ.

â€¢	Extreme example: if we had degrees Celsius and degrees Fahrenheit as predictors in the same model, it would be impossible to tell their effects apart. You canâ€™t change Celsius while holding Fahrenheit constant!
 



 

â€¢	The practical consequence is that the standard errors for the slopes of (multi)collinear predictors are much larger than they would be if it were not for the (multi)collinearity.

â€¢	If two or more predictors are perfectly correlated (ğ‘Ÿ = 1), then the model cannot be fit and jamovi produces an error:


â€¢	Here, X1 and X2 are perfectly correlated. jamovi cannot estimate a slope for X2.
 



 

â€¢	In the Florida election data, we used total votes for each county as our predictor variable.

â€¢	There is another variable called â€œTotal_Regâ€. This is the total number of
registered voters in each county.

â€¢	Unsurprisingly, Total_Votes and Total_Reg are highly correlated:
 



 


â€¢	If we run two separate simple regression models, we get very similar results:


ğµğ‘¢ğ‘â„ğ‘ğ‘›ğ‘ğ‘›ğ‘– = ğ›½0 + ğ›½1ğ‘‡ğ‘œğ‘¡ğ‘ğ‘™ _ğ‘‰ğ‘œğ‘¡ğ‘’ğ‘ ğ‘– + ğœ€ğ‘–	ğµğ‘¢ğ‘â„ğ‘ğ‘›ğ‘ğ‘›ğ‘– = ğ›½0 + ğ›½1ğ‘‡ğ‘œğ‘¡ğ‘ğ‘™ _ğ‘…ğ‘’ğ‘”ğ‘– + ğœ€ğ‘–





 



 

â€¢	But look what happens if we use Total_Votes and Total_Reg as predictors in the same model:


ğµğ‘¢ğ‘â„ğ‘ğ‘›ğ‘ğ‘›ğ‘– = ğ›½0 + ğ›½1ğ‘‡ğ‘œğ‘¡ğ‘ğ‘™_ğ‘‰ğ‘œğ‘¡ğ‘’ğ‘ ğ‘–	+ ğ‘‡ğ‘œğ‘¡ğ‘ğ‘™_ğ‘…ğ‘’ğ‘”ğ‘– + ğœ€ğ‘–
â€¢	Two important things to note:

â€¢	P-values on slopes are much larger than for the individual models

â€¢	ğ‘…2 is larger than on either individual model!
 



 



â€¢	Looking at the estimates and standard errors in all three models, we see that the standard errors are much larger in the multiple regression model. These estimates are â€œunstableâ€ â€“ their values will change a lot if the data change a little.

â€¢	We know the association between Total_Reg are Buchanan is actually positive. But with so high a standard error, the slope for Total_Reg turned out negative!
 



 
#### Variance inflation factor (VIF)


â€¢	(Multi)collinearity can be assessed using a â€œVariance Inflation Factorâ€, or
VIF. A VIF is calculated for the ğ‘—ğ‘¡â„ predictor variable as:


 
ğ‘‰ğ¼ğ¹ğ‘— =
 
1

1 âˆ’ ğ‘…2
 



â€¢	Where ğ‘…2 is the ğ‘…2 from a regression model with predictor j as the response variable and all other predictors still as predictors.
 



 



â€¢	In the Florida election example, the VIF for Total_Votes can be found using the ğ‘…2 for the model:
ğ‘‡ğ‘œğ‘¡ğ‘ğ‘™ _ğ‘‰ğ‘œğ‘¡ğ‘’ğ‘ ğ‘– = ğ›½0 + ğ›½1ğ‘‡ğ‘œğ‘¡ğ‘ğ‘™ _ğ‘…ğ‘’ğ‘”ğ‘– + ğœ€ğ‘–	 
â€¢	This ğ‘…2 is huge! Plugging it into the formula:



 
ğ‘‰ğ¼ğ¹ğ‘‡ğ‘œğ‘¡ğ‘ğ‘™_ğ‘‰ğ‘œğ‘¡ğ‘’ğ‘  =
 
1
= 333.33
1 âˆ’ 0.997
 





â€¢	Thankfully we donâ€™t have to do this by hand. In jamovi, under â€œLinear Regressionâ€ select â€œCollinearity statisticsâ€:

â€¢	VIF > 10 typically is considered large (note that this would imply ğ‘…2 = 0.9 between predictor variables).

â€¢	The most obvious thing to do in the presence of (multi)collinearity is to remove one or more correlated predictor variable. From a scientific standpoint, you also may not want highly correlated predictors in the same model.
 



 
#### When should we worry about (multi)collinearity?


â€¢	(Multi)collinearity is a potentially huge problem if the goal of the regression model is to interpret the estimated slopes.

â€¢	This is because it increases the standard error of these slopes, making their
values less reliable. Some people say it makes slopes â€œunstableâ€.

â€¢	It may also complicate the interpretation of slopes: you are trying to statistically â€œhold constantâ€ a predictor variable that doesnâ€™t naturally stay constant when the other predictor varies. This isnâ€™t necessarily a problem, but it is something to be aware of.
 



 




â€¢	However, (multi)collinearity does not negatively impact the predicted values themselves. Remember that it didnâ€™t hurt the ğ‘…2 value in the Florida election example. ğ‘…2 tells you how good your predictions are.

â€¢	So, if the model is only for predicting, you probably donâ€™t need to worry about using correlated predictor variables. Just beware when interpreting the slopes.
 

## Part 2: Improving models


### Summary of part 1



â€¢	In the last set of notes, we looked at some things that can go wrong in regression modeling, including:

â€¢	Non-linear relationships between predictor(s) and response
â€¢	Non-normality of residuals
â€¢	Non-constant (heterogeneous) variance of residuals
â€¢	Influential outliers
â€¢	Multicollinearity

â€¢	In these notes, weâ€™ll look at some tools available for dealing with these
problems.
 
### Transforming variables



â€¢	Recall the regression model:

ğ‘Œğ‘– = ğ›½0 + ğ›½1ğ‘¥1ğ‘– + ğ›½2ğ‘¥2ğ‘– + â‹¯ + ğ›½ğ‘ğ‘¥ğ‘ğ‘– + ğœ€ğ‘–	ğœ€ğ‘–~ğ‘ğ‘œğ‘Ÿğ‘šğ‘ğ‘™


â€¢	Sometimes we can correct violations of model assumptions by applying a mathematical transformation to the response or predictor variables.

â€¢	The most common transformation in Statistics is the log transformation:

ln(ğ‘¥) = logğ‘’(ğ‘¥)
 



### Log transformation



â€¢	ln(ğ‘¥) is the inverse function of ğ‘’ğ‘¥, where ğ‘’ = 2.718 â€¦


 
â€¢	In other words, ln
 
= ğ‘¥
 


â€¢	Example: ğ‘’3 = 20.086; ln(20.086) = 3

â€¢	So, the natural log of ğ‘¥ is the number you would have to raise ğ‘’ to so that
youâ€™d get ğ‘¥.

â€¢	Note: in statistics, when we say â€œlogâ€, we usually mean â€œnatural logâ€. It turns out that the distinction is not very important. Iâ€™ll say â€œlog transformâ€
 



#### Why log transform?


â€¢	There are two main reasons for log transforming a variable:

â€¢	To correct for skew in data or residuals
â€¢	To interpret increases in a variable as multiplicative rather than additive.

â€¢	Both can be understood by recognizing an important property of logarithms; they
â€œturn addition into multiplicationâ€

log(ğ´) + log(ğµ)  =  log(ğ´ğµ)

â€¢	In this sense, logarithms turn addition into multiplication.
 



 

â€¢	Example: suppose we have data for a skewed variable ğ‘‹1:

 



 

â€¢	Now we define ğ‘¥2 = ln(ğ‘¥1):
â€¢	This is a toy â€œdata setâ€. I chose ğ‘¥1 so that ğ‘¥2 = ln
 
would just be the integers 1 through 10.

â€¢	Note: there is no more skew.

â€¢	Also note: increasing ğ‘¥2 by one unit results in multiplying ğ‘¥1 by ğ‘’.
 
Addition in ğ‘¥2 = ln
 
is the same
 
thing as multiplication in ğ‘¥1.	
 


Same again, with log base 2



â€¢	Even simpler: define ğ‘¥2 as log base 2 of ğ‘¥1, i.e. log2(ğ‘¥1)

â€¢	Now increasing ğ‘¥2 by one unit is equivalent to multiplying ğ‘¥1 by 2.
 
Addition in ğ‘¥2 = ğ‘™ğ‘œğ‘”2
 
is the same
 
thing as multiplication in ğ‘¥1.
 



#### Log transforming right-skewed data



â€¢	Skewed data can be bad for regression, in that it can lead to:

â€¢	Non-linear relationship between X and Y
â€¢	Influential outliers
â€¢	Non-normal residuals
â€¢	Non-constant variance in residuals

â€¢	So a simple log transformation can sometimes go a long way toward making the regression model fit the better!
 



 

â€¢	It is most common to log transform a response variable, because assumptions about residuals apply to Y, not X.

â€¢	But if X is skewed, the model can benefit from a log transformation of X.

â€¢	Bear in mind that log transformation will affect the interpretation of slope coefficients!

â€¢	If X is log transformed, then a one unit increase in ln(ğ‘‹) corresponds to multiplying X by ğ‘’ â‰ˆ 2.72. So the slope for ln(ğ‘‹) tells you how much Y increases when X is multiplied by 2.72. Or, even better, use log base 2 and the
 



 

â€¢	If Y is log transformed, then the interpretations of slopes get more complicated.
Hereâ€™s the math, with the error term omitted for convenience:


 
ln
 
= ğ›½0 + ğ›½1ğ‘‹ğ‘–
 


 

â€¢	Increase X by 1â€¦
 
âˆ´ ğ‘¦ğ‘– = ğ‘’ğ›½0+ğ›½1ğ‘‹ğ‘–
 


ğ‘¦âˆ— = ğ‘’ğ›½0+ğ›½1(ğ‘‹ğ‘–+1) = ğ‘’ğ›½0+ğ›½1(ğ‘‹ğ‘–) âˆ™ ğ‘’ğ›½1

â€¢	So, when Y is log transformed, a one unit increase in X multiplies predicted Y by
 



#### Interpreting slope as a % change in outcome




â€¢	Recall the heights vs. wages data from group project 1. The paper reported this estimated model:


 
ln
 
= ğ›½áˆ˜
 
+ 0.002
 
+ 0.027
 
+ 0.024(Age)
 


â€¢	So, when comparing two adults 1 inch apart in height but with the same youth height and age predicted wage is multiplied by ğ‘’0.027 = 1.027 for the taller adult.

â€¢	Multiplying by 1.027 can be thought of as increasing by 2.7%
 



#### Log transformation applied example




â€¢	Here is the percent change formula:


 
% ğ‘â„ğ‘ğ‘›ğ‘”ğ‘’ (ğ‘“ğ‘Ÿğ‘œğ‘š ğ´ ğ‘¡ğ‘œ ğµ) =

â€¢	If B is 1.027*A, then
 
ğµ âˆ’ ğ´

 
ğ´
 
âˆ— 100%
 


 
% ğ‘â„ğ‘ğ‘›ğ‘”ğ‘’ =
 
1.027ğ´ âˆ’ 

 
ğ´
 
âˆ— 100 =
 
0.027ğ´

 
ğ´
 
âˆ— 100 = 2.7%
 

â€¢	So, when comparing two adults 1 inch apart in height but with the same youth height and age, predicted wage is 2.7% higher for the taller adult.
 



#### Log transformation in Y vs. in X




â€¢	Remember that log transformation â€œturns addition into multiplicationâ€. So, to keep track of how log transforming Y vs. log transforming X affects your model:
 
log
 
= ğ›½0 + ğ›½1ğ‘‹ğ‘– + ğœ€ğ‘–
vs.
 
ğ‘Œğ‘– = ğ›½0 + ğ›½1log(ğ‘‹ğ‘–) + ğœ€ğ‘–

â€¢	If you log transform Y but not X, your model estimates the multiplicative change in predicted Y for an additive change in X.

â€¢	If you log transform X but not Y, your model estimates the additive change
 
### Non-linearity



â€¢	Sometimes data show obvious curvature, in the sense that Y is clearly not a straight line function of X.

â€¢	This will be visible on a plot of Y vs. X. It will also be visible on a residuals vs. predicted values plot after running a regression.

â€¢	If there is curvature in the relationship between Y and X, then it might be sensible to add a polynomial X term:

ğ‘Œğ‘– = ğ›½0 + ğ›½1ğ‘¥ğ‘– + ğ›½2ğ‘¥2 + ğœ€ğ‘–
 



#### â€œPolynomialâ€ review



â€¢	A â€œpolynomialâ€ expression is typically one in which variables are included at different powers. For example, a generic third degree polynomial equation might look like:

ğ‘¦ = ğ‘ + ğ‘ğ‘¥ + ğ‘ğ‘¥2 + ğ‘‘ğ‘¥3

â€¢	A â€œsecond degreeâ€ polynomial is one in which an ğ‘¥ and ğ‘¥2 term are both included. This is by far the most common type of polynomial seen in regression models.
 



2nd degree and 3rd degree polynomials



â€¢	2nd degree polynomials are often called â€œquadraticâ€. 3rd degree polynomials are often called â€œcubicâ€. Here are visual examples of simulated quadratic and cubic relationships between Y and X:

 



#### Curvature in residuals



â€¢	Here is regression output comparing a linear model to a quadratic model when the relationship between Y and X is quadratic:

 



 

â€¢	Here is what the Florida election data look like with the Palm Beach County outlier removed, along with regression results for the simple linear regression model:
ğµğ‘¢ğ‘â„ğ‘ğ‘›ğ‘ğ‘›ğ‘– = ğ›½0 + ğ›½1ğ‘‡ğ‘œğ‘¡ğ‘ğ‘™_ğ‘‰ğ‘œğ‘¡ğ‘’ğ‘ ğ‘– + ğœ€ğ‘–

 



 

â€¢	Now we will fit a quadratic polynomial model to the same data:

ğµğ‘¢ğ‘â„ğ‘ğ‘›ğ‘ğ‘›ğ‘– = ğ›½0 + ğ›½1ğ‘‡ğ‘œğ‘¡ğ‘ğ‘™_ğ‘‰ğ‘œğ‘¡ğ‘’ğ‘ ğ‘– + ğ›½2ğ‘‡ğ‘œğ‘¡ğ‘ğ‘™_ğ‘‰ğ‘œğ‘¡ğ‘’ğ‘ 2 + ğœ€ğ‘–



â€¢	To create this polynomial predictor in jamovi we first need to install the GAMLj module. Then, select â€œGeneralized Linear Modelsâ€ and last our â€œDependent Variableâ€ and â€œCovariatesâ€.

â€¢	Under the â€œModelâ€ drop down menu, click on Total_Votes in the
â€œComponentsâ€ table.
 



 

â€¢	An up and down arrow will appear with the degree of Total_Votes, click the up arrow so the 1 changes to 2. Then click the right arrow to add Total_Votes2 to â€œModel Termsâ€


 



 

ğµğ‘¢ğ‘â„ğ‘ğ‘›ğ‘ğ‘›ğ‘– = ğ›½0 + ğ›½1ğ‘‡ğ‘œğ‘¡ğ‘ğ‘™_ğ‘‰ğ‘œğ‘¡ğ‘’ğ‘ ğ‘– + ğ›½2ğ‘‡ğ‘œğ‘¡ğ‘ğ‘™_ğ‘‰ğ‘œğ‘¡ğ‘’ğ‘ 2 + ğœ€ğ‘–
 



 

â€¢	This is better, but we still see curvature in the residual plot.

â€¢	Letâ€™s try a cubic model:

ğµğ‘¢ğ‘â„ğ‘ğ‘›ğ‘ğ‘›ğ‘– = ğ›½0 + ğ›½1ğ‘‡ğ‘œğ‘¡ğ‘ğ‘™_ğ‘‰ğ‘œğ‘¡ğ‘’ğ‘ ğ‘– + ğ›½2ğ‘‡ğ‘œğ‘¡ğ‘ğ‘™_ğ‘‰ğ‘œğ‘¡ğ‘’ğ‘ 2 + ğ›½3ğ‘‡ğ‘œğ‘¡ğ‘ğ‘™_ğ‘‰ğ‘œğ‘¡ğ‘’ğ‘ 3 + ğœ€ğ‘–
 



 



ğµğ‘¢ğ‘â„ğ‘ğ‘›ğ‘ğ‘›ğ‘– = ğ›½0 + ğ›½1ğ‘‡ğ‘œğ‘¡ğ‘ğ‘™_ğ‘‰ğ‘œğ‘¡ğ‘’ğ‘ ğ‘– + ğ›½2ğ‘‡ğ‘œğ‘¡ğ‘ğ‘™_ğ‘‰ğ‘œğ‘¡ğ‘’ğ‘ 2 + ğ›½3ğ‘‡ğ‘œğ‘¡ğ‘ğ‘™_ğ‘‰ğ‘œğ‘¡ğ‘’ğ‘ 3 + ğœ€ğ‘–
 





â€¢	Itâ€™s debatable whether this is much better. For one, the ğ‘‡ğ‘œğ‘¡ğ‘ğ‘™_ğ‘‰ğ‘œğ‘¡ğ‘’ğ‘ 2 term is non-significant.

â€¢	But think back to multicollinearity. Each polynomial term will be correlated with the other terms â€“ after all, ğ‘‡ğ‘œğ‘¡ğ‘ğ‘™_ğ‘‰ğ‘œğ‘¡ğ‘’ğ‘ , ğ‘‡ğ‘œğ‘¡ğ‘ğ‘™_ğ‘‰ğ‘œğ‘¡ğ‘’ğ‘ 2, and
ğ‘‡ğ‘œğ‘¡ğ‘ğ‘™_ğ‘‰ğ‘œğ‘¡ğ‘’ğ‘ 3 must all be correlated.

â€¢	Note that jamovi will not produce VIFs in GLM. Intuitively we know these will be highly correlated with each other.
 



 

â€¢	It turns out that centering helps in polynomial models:
Compare to:
 	 

â€¢	By default jamovi centers polynomials which makes their standard errors smaller than if they were not centered. But, these estimates are not interpretable; you cannot hold Total_Votes2 constant while increasing Total_Votes.
 



 

â€¢	In this example, the two counties with the highest total votes are heavily pulling on the regression line.


 


### Over-fitting
 

â€¢	This model might be â€œover-fitâ€.

â€¢	Over-fitting is when a model fits the data so well that it ends up fitting random variation that is not of interest.

â€¢	Imagine if the two data points on the right had slightly higher vote counts for Buchanan. Or if the next two to their left had slightly lower vote counts. The curve would look very different.

â€¢	At this point, we might just be modelling noise.
 



 

â€¢	Here is an extreme example of over-fitting: fitting a â€œsmootherâ€ curve to data and giving it permission to move dramatically up and down through the data.

â€¢	This line fits the data very well, but does it represent the general trend between total votes and votes for Buchanan? Definitely not!



â€¢	(Side note: â€œsmoothersâ€ are great tools for visualizing and summarizing data, but they can be extremely sensitive to degree of smoothing. We wonâ€™t use them in STAT 331)
 



 

â€¢	Compare the over-fit model to the linear model.

â€¢	The linear model may be missing out on some curvature. But it might also make better predictions.

â€¢	If we were to observe a new county with 450,000 total votes, would we be better guessing that votes for Buchanan fall on the highly curved line or on the straight line?


### Back to basics: is the model sensible?



â€¢	Back to basics: regression models are typically used for two purposes:

o	Predicting values of the response variable, using the predictor variables. This is done by plugging values for the predictor variables into the estimated model.

o	Estimating the association between each individual predictor variable and the outcome while statistically holding other predictor variables constant. This is done by interpreted estimated slope coefficients.
 



#### If you just want to make predictions



â€¢	ğ‘…2 is the easiest to understand statistic for assessing how well your model makes predictions. The closer to 1, the better.

â€¢	Multicollinearity isnâ€™t an issue. It doesnâ€™t affect predicted values.

â€¢	BUT â€“ beware of overfitting! The more complex your model, the more risk you take of modeling noise instead of signal.

â€¢	Also, be aware that ğ‘…2 can never go down when adding predictors. You can add complete nonsense predictor variables, and the worst that will happen to ğ‘…2 is that it stays the same.
 


#### If you want to interpret slopes
 

â€¢	Always remember that each slope is interpreted under the assumption that all other predictor variables are being held constant, i.e. â€œcontrolled forâ€.

â€¢	The more predictor variables in the model, the less sense this will make.

â€¢	Example: wage vs. height study:
 



 

â€¢	In model 4, the estimated slope for youth height can be interpreted as:

â€œThe predicted difference in ln(wage) for two people one inch apart in youth height, but equal in adult height, whose mothers have the same # of years of schooling and are both in either skilled fields or work or not, whose fathers have the same # of years of schooling and are both in either skilled fields or work or not, and who have the same
number of siblings.â€


â€¢	Maybe this is the best way to think about the association between youth height and wages. But it is fairly complicated.
 



 

â€¢	If youâ€™re going to try to make â€œreal worldâ€ sense out of regression results, your model
should be informed by theory.

â€¢	This is necessarily subjective! You have to choose which variables you think are
important. You have to think about what makes sense.

â€¢	This might require:

o	Log transforming a variable solely because you like the multiplicative interpretation better than the additive interpretation.
o	Keeping a variable in a model even though it isnâ€™t statistically significant.
o	Removing a variable you are interested in, because it doesnâ€™t make sense to â€œhold it constantâ€ when estimating slopes for other variables.
 

#### Is the model missing something important?
 

â€¢	There is another variable in the Florida election data set that could be worth including: â€œReg_Reformâ€: the total number of voters registered with the Reform Party. Pat Buchanan was the Reform Party candidate. Letâ€™s add it to the model:

ğµğ‘¢ğ‘â„ğ‘ğ‘›ğ‘ğ‘›ğ‘– = ğ›½0 + ğ›½1ğ‘‡ğ‘œğ‘¡ğ‘ğ‘™_ğ‘‰ğ‘œğ‘¡ğ‘’ğ‘ ğ‘– + ğ›½2ğ‘…ğ‘’ğ‘”_ğ‘…ğ‘’ğ‘“ğ‘œğ‘Ÿğ‘šğ‘– + ğœ€ğ‘–




 

â€¢	This residual plot looks great!

â€¢	It turns out that the curvature in the previous residual plots went away when adding an important variable to the model. No need to mess with polynomials after all!

â€¢	Also, the ğ‘…2 is roughly the same as in the cubic model using only total votes as a predictor.


â€¢	So we have roughly equal fit, without having to worry about overfitting, and without having to give up
interpretability of the slopes.	40
 



 



â€¢	One downside: there is some collinearity. Look at the VIFs.


 
â€¢	VIF of about 5 implies	1
1âˆ’ğ‘…
 
â‰ˆ 5 when using the ğ‘…2 from:
 


ğ‘‡ğ‘œğ‘¡ğ‘ğ‘™_ğ‘‰ğ‘œğ‘¡ğ‘’ğ‘ ğ‘– = ğ›½0 + ğ›½1ğ‘…ğ‘’ğ‘”_ğ‘…ğ‘’ğ‘“ğ‘œğ‘Ÿğ‘šğ‘– + ğœ€ğ‘–
â€¢	2	1
 
So, this ğ‘…
 
is about 1 âˆ’
5
 
= 0.8. And so ğ‘Ÿ =
 
= 0.89. These predictors are
 
strongly correlated.
 



 



â€¢	Note also that total votes is not significant.

â€¢	But: the slope for Reg_Reform has a nice interpretation:

When comparing two counties with the same number of votes cast in the election, a county with an additional registered Reform Party member is estimated to have 2.24 additional votes, on average, for Pat Buchanan.

â€¢	Should total votes be taken out of the model? This is a subjective decision.
 


#### What would you like to "control" for?
 

â€¢	In regression analysis, we usually emphasize (correctly) that correlation does not imply causation.

â€¢	However, if you have knowledge or beliefs about causal direction, you should take these into account when choosing your variables!


â€¢	Example: in the rheumatoid arthritis study, we were looking at the effect of drugs on inflammation level. In particular, we were comparing how effective they were at reducing inflammation. Suppose we also asked patients to rate their mobility level (RA tends to reduce mobility).
 



 

â€¢	Our model might be:


ğ·ğ‘–ğ‘“ğ‘“ğ‘’ğ‘Ÿğ‘’ğ‘›ğ‘ğ‘’ğ‘– = ğ›½0 + ğ›½1ğ‘ğ‘”ğ‘’ğ‘– + ğ›½2ğ‘‘ğ‘Ÿğ‘¢ğ‘”ğ‘– + ğ›½3	ğ‘– + ğ›½4ğ‘šğ‘œğ‘ğ‘–ğ‘™ğ‘–ğ‘¡ğ‘¦ğ‘–	+ ğœ€ğ‘–


â€¢	Now, when interpreting the previous slopes, I am comparing average reduction in inflammation (â€œdifferenceâ€, the response variable), between two people who have the same mobility level. But if they have the same mobility level, then they will have more similar inflammation levels than if we allowed mobility level to differ.

â€¢	In other words, because the drug reduces inflammation *and* improves mobility, â€œcontrollingâ€ for mobility will make it look like the drugs are less effective than they really are.
 



#### Beware the â€œkitchen sinkâ€ approach



â€¢	Thereâ€™s an old saying: â€œtaking everything but the kitchen sinkâ€.

â€¢	It can be tempting to toss everything but the kitchen sink into a regression model, especially when you have loads of variables that all seem like theyâ€™d be associated with the response.

â€¢	But beware! Adding in one predictor can have a dramatic effect on the slopes of other predictors, as well as on their standard errors.

â€¢	It really really really matters that each slope is estimated as though all other predictors are held constant. This can reveal otherwise unseen effects, but it can also obscure otherwise obvious effects or induce apparent effects that arenâ€™t real. There
 


#### The model is simpler than what's being modeled

â€¢	Letâ€™s take a step back and ask: why are we fitting data to models?

â€¢	Well, we are interested in the real world. And the real world in incredibly complicated. Maybe incomprehensibly complicated.

â€¢	So, we simplify things using models. We hope that the model captures the essence of what we care about in the real world. But we know it is a simplification; perhaps an extreme simplification.

â€¢	â€œAll models are wrong; some are usefulâ€ â€“ George Box
 



 


â€¢	Consider how the regression model describes where data comes from:

ğ‘Œğ‘– = ğ›½0 + ğ›½1ğ‘¥ğ‘– + ğœ€ğ‘– ,	ğœ€ğ‘–~ğ‘ğ‘œğ‘Ÿğ‘šğ‘ğ‘™(0, ğœ)

â€¢	This says that to get data, we pick a value for X, go to a line, then randomly draw a value from a normal distribution and add this to the value on the line. And thatâ€™s where data comes from!

â€¢	Except, thatâ€™s not where data comes from. This is a model. It is a simplification of reality. We use these models because we think they will help us answer questions we care about (e.g. make predictions, identify associations between variables).
Donâ€™t forget that the model is not the thing itself.
 




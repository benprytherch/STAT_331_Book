% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
\PassOptionsToPackage{dvipsnames,svgnames,x11names}{xcolor}
%
\documentclass[
  letterpaper,
  DIV=11,
  numbers=noendperiod]{scrreprt}

\usepackage{amsmath,amssymb}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
\usepackage{lmodern}
\ifPDFTeX\else  
    % xetex/luatex font selection
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\setlength{\emergencystretch}{3em} % prevent overfull lines
\setcounter{secnumdepth}{5}
% Make \paragraph and \subparagraph free-standing
\ifx\paragraph\undefined\else
  \let\oldparagraph\paragraph
  \renewcommand{\paragraph}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
  \let\oldsubparagraph\subparagraph
  \renewcommand{\subparagraph}[1]{\oldsubparagraph{#1}\mbox{}}
\fi


\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother
\newlength{\cslhangindent}
\setlength{\cslhangindent}{1.5em}
\newlength{\csllabelwidth}
\setlength{\csllabelwidth}{3em}
\newlength{\cslentryspacingunit} % times entry-spacing
\setlength{\cslentryspacingunit}{\parskip}
\newenvironment{CSLReferences}[2] % #1 hanging-ident, #2 entry spacing
 {% don't indent paragraphs
  \setlength{\parindent}{0pt}
  % turn on hanging indent if param 1 is 1
  \ifodd #1
  \let\oldpar\par
  \def\par{\hangindent=\cslhangindent\oldpar}
  \fi
  % set entry spacing
  \setlength{\parskip}{#2\cslentryspacingunit}
 }%
 {}
\usepackage{calc}
\newcommand{\CSLBlock}[1]{#1\hfill\break}
\newcommand{\CSLLeftMargin}[1]{\parbox[t]{\csllabelwidth}{#1}}
\newcommand{\CSLRightInline}[1]{\parbox[t]{\linewidth - \csllabelwidth}{#1}\break}
\newcommand{\CSLIndent}[1]{\hspace{\cslhangindent}#1}

\KOMAoption{captions}{tableheading}
\makeatletter
\@ifpackageloaded{tcolorbox}{}{\usepackage[skins,breakable]{tcolorbox}}
\@ifpackageloaded{fontawesome5}{}{\usepackage{fontawesome5}}
\definecolor{quarto-callout-color}{HTML}{909090}
\definecolor{quarto-callout-note-color}{HTML}{0758E5}
\definecolor{quarto-callout-important-color}{HTML}{CC1914}
\definecolor{quarto-callout-warning-color}{HTML}{EB9113}
\definecolor{quarto-callout-tip-color}{HTML}{00A047}
\definecolor{quarto-callout-caution-color}{HTML}{FC5300}
\definecolor{quarto-callout-color-frame}{HTML}{acacac}
\definecolor{quarto-callout-note-color-frame}{HTML}{4582ec}
\definecolor{quarto-callout-important-color-frame}{HTML}{d9534f}
\definecolor{quarto-callout-warning-color-frame}{HTML}{f0ad4e}
\definecolor{quarto-callout-tip-color-frame}{HTML}{02b875}
\definecolor{quarto-callout-caution-color-frame}{HTML}{fd7e14}
\makeatother
\makeatletter
\makeatother
\makeatletter
\@ifpackageloaded{bookmark}{}{\usepackage{bookmark}}
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\AtBeginDocument{%
\ifdefined\contentsname
  \renewcommand*\contentsname{Table of contents}
\else
  \newcommand\contentsname{Table of contents}
\fi
\ifdefined\listfigurename
  \renewcommand*\listfigurename{List of Figures}
\else
  \newcommand\listfigurename{List of Figures}
\fi
\ifdefined\listtablename
  \renewcommand*\listtablename{List of Tables}
\else
  \newcommand\listtablename{List of Tables}
\fi
\ifdefined\figurename
  \renewcommand*\figurename{Figure}
\else
  \newcommand\figurename{Figure}
\fi
\ifdefined\tablename
  \renewcommand*\tablename{Table}
\else
  \newcommand\tablename{Table}
\fi
}
\@ifpackageloaded{float}{}{\usepackage{float}}
\floatstyle{ruled}
\@ifundefined{c@chapter}{\newfloat{codelisting}{h}{lop}}{\newfloat{codelisting}{h}{lop}[chapter]}
\floatname{codelisting}{Listing}
\newcommand*\listoflistings{\listof{codelisting}{List of Listings}}
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\@ifpackageloaded{subcaption}{}{\usepackage{subcaption}}
\makeatother
\makeatletter
\@ifpackageloaded{tcolorbox}{}{\usepackage[skins,breakable]{tcolorbox}}
\makeatother
\makeatletter
\@ifundefined{shadecolor}{\definecolor{shadecolor}{rgb}{.97, .97, .97}}
\makeatother
\makeatletter
\makeatother
\makeatletter
\makeatother
\ifLuaTeX
  \usepackage{selnolig}  % disable illegal ligatures
\fi
\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same} % disable monospaced font for URLs
\hypersetup{
  pdftitle={STAT 331},
  pdfauthor={Ben Prytherch},
  colorlinks=true,
  linkcolor={blue},
  filecolor={Maroon},
  citecolor={Blue},
  urlcolor={Blue},
  pdfcreator={LaTeX via pandoc}}

\title{STAT 331}
\author{Ben Prytherch}
\date{2023-07-04}

\begin{document}
\maketitle
\ifdefined\Shaded\renewenvironment{Shaded}{\begin{tcolorbox}[borderline west={3pt}{0pt}{shadecolor}, breakable, enhanced, sharp corners, interior hidden, boxrule=0pt, frame hidden]}{\end{tcolorbox}}\fi

\renewcommand*\contentsname{Table of contents}
{
\hypersetup{linkcolor=}
\setcounter{tocdepth}{2}
\tableofcontents
}
\bookmarksetup{startatroot}

\hypertarget{introduction-to-stat-331-intermediate-applied-statistical-methods}{%
\chapter*{Introduction to STAT 331: Intermediate Applied Statistical
Methods}\label{introduction-to-stat-331-intermediate-applied-statistical-methods}}
\addcontentsline{toc}{chapter}{Introduction to STAT 331: Intermediate
Applied Statistical Methods}

\markboth{Introduction to STAT 331: Intermediate Applied Statistical
Methods}{Introduction to STAT 331: Intermediate Applied Statistical
Methods}

\hypertarget{purpose-and-intended-audience}{%
\section*{Purpose and intended
audience}\label{purpose-and-intended-audience}}
\addcontentsline{toc}{section}{Purpose and intended audience}

\markright{Purpose and intended audience}

STAT 331, as the title states, is an ``applied'' statistics course. It
is intended for anyone who has taken at least one introductory level
statistics course, and who wants to learn more about the use of
statistical methods in quantitative research.

It covers many statistical tools that are usually considered too
advanced for an introductory level class, but are nonetheless very
popular. It also provides guidance on making data analysis decisions.

Most assignments will involve looking up a published scientific paper
for which the data are available and reproducing the main results. There
are many worked examples that go through the statistical analyses in
used in specific published papers.

STAT 331 doesn't require any coding; the software we use is jamovi, a
GUI-based (meaning ``graphical user interface'', aka
``point-and-click'') statistical analysis package. Jamovi is built on to
of R, and for those interested in using R it has the ability to display
the R code it creates under the hood.

STAT 331 is not mathematically heavy in the traditional sense, but it
isn't math-free either. My approach is to present mathematical formulas
and expressions when they are necessary or at least helpful for
understanding the statistical it's being covered. There are no
mathematical character-building exercises or examples, and we won't be
computing things by hand - the software does all the computational work.
Our job is to make sense of the results. And that usually requires
looking at formulas and figuring out what they do. We will be constantly
answering the question ``what does this number mean?''

\hypertarget{structure-of-these-notes}{%
\section*{Structure of these notes}\label{structure-of-these-notes}}
\addcontentsline{toc}{section}{Structure of these notes}

\markright{Structure of these notes}

These notes are broken up into 9 chapters (or ``modules''):

\begin{itemize}
\tightlist
\item
  Chapter 1: Review of classical inference
\item
  Chapter 2: Model building with linear regression
\item
  Chapter 3: Assessing and improving model fit
\item
  Chapter 4: ANOVA-based methods
\item
  Chapter 5: Analyzing categorical data
\item
  Chapter 6: Generalized Linear Models (GLMs)
\item
  Chapter 7: Mixed-effects models
\item
  Chapter 8: Minding the gap between science and statistics
\item
  Chapter 9: Brief looks at major topics we didn't cover
\end{itemize}

You can access chapters and subsections directly through the table of
contents.

\hypertarget{really-good-online-books}{%
\section*{Really good online books}\label{really-good-online-books}}
\addcontentsline{toc}{section}{Really good online books}

\markright{Really good online books}

These notes will frequently reference some other freely available online
statistics books:

\href{https://www.learnstatswithjamovi.com/}{Learning statistics with
jamovi}, by Danielle Navarro and David Foxcroft

\href{https://www.kellerbiostat.com/introregression/}{Introduction to
Regression Analysis in R}, by Kayleigh Keller

\href{https://crumplab.com/statistics/}{Answering questions with data},
by Matthew J. C. Crump, Danielle J. Navarro, and Jeffrey Suzuki

\href{https://drive.google.com/file/d/0B1fyuTuvj3YoaFdUR3FZaXNuNXc/view?resourcekey=0-plOmKmQ0TIyMjlfBY3OiyQ}{Statistical
Analysis with The General Linear Model}, by Jeff Miller and Patricia
Haden

\bookmarksetup{startatroot}

\hypertarget{chapter-1-review-of-classical-inference}{%
\chapter{Chapter 1: Review of classical
inference}\label{chapter-1-review-of-classical-inference}}

\hypertarget{module-1-overview}{%
\section{Module 1 overview}\label{module-1-overview}}

• These notes briefly cover material from your introductory statistics
course that will be relevant in STAT 331.

• For a more in-depth review, consult the OpenIntro text, or just do an
internet search.

\hypertarget{distributions}{%
\section{Distributions}\label{distributions}}

• The term ``distribution'' will be used a lot.

• A distribution gives the values a variable takes on, and how often it
takes them on.

• Examples: normal distribution, uniform distribution, distribution of
exam scores, distribution of heights\ldots{}

\hypertarget{commonly-used-statistics}{%
\section{Commonly used statistics}\label{commonly-used-statistics}}

\hypertarget{mean}{%
\subsection{Mean}\label{mean}}

• Mean and median identify the center of a data set or distribution.

• The mean of a variable \(X\) is denoted \(\bar{X}\).

• To calculate the mean of a data set, add up all the values of a
variable and divide by how many there are.

\[
\bar{X} = \frac{\sum^n_{i=1}x_i}{n}
\]

\hypertarget{median}{%
\subsection{Median}\label{median}}

• Median is the ``middle'' number in a data set. To find the median, put
the data values in order from smallest to largest, and identify the
number in the middle.

• If there are an even number of data points, the median is the average
of the middle two numbers:

\begin{figure}

{\centering \includegraphics[width=3.52083in,height=\textheight]{images/Mod1_1.png}

}

\end{figure}

\hypertarget{mean-vs.-median}{%
\subsection{Mean vs.~Median}\label{mean-vs.-median}}

• In statistical inference (the process of generalizing from sample to
population), we most often draw inference on the population mean.

• Sometimes, though, the median is a more sensible statistic than the
mean.

• This is usually the case when we are studying a ``skewed''
distribution.

• Skewed distributions are distributions that take on values that are
extreme (or outlying) values.

• Example: income. Most households have incomes between \$20,000/yr and
\$100,000/yr. A handful of households have incomes in the millions or
billions of dollars per year.

• The mean is affected by outliers. The median is not. This is why we
often hear about ``median household income'' rather than ``mean
household income''.

\hypertarget{jamovi-example-mean-vs.-median}{%
\subsection{jamovi example: mean
vs.~median}\label{jamovi-example-mean-vs.-median}}

• Try creating a skewed data set in jamovi, then analyzing it by
selecting for mean and median in the Statistics drop down menu in
Descriptives, found under Exploration in the Analysis tab

• To create a new dataset, enter data into the blank data table jamovi
creates by default:

\begin{figure}

\includegraphics[width=1.78125in,height=\textheight]{images/mod1_2.png} \hfill{}

\end{figure}

\begin{figure}

{\centering \includegraphics{images/mod1_3.png}

}

\end{figure}

\hypertarget{variance-and-standard-deviation}{%
\subsection{Variance and standard
deviation}\label{variance-and-standard-deviation}}

• The center of a distribution is quantified by the mean or the median.

• The variability (i.e.~spread) of a distribution is quantified by the
standard deviation, which is closely related to the variance.

• The variance of a distribution or data set is denoted \(s^2\):

\[
s^2 = \frac{\sum^n_{i=1}(x_i - \bar{x})^2}{n-1}
\]

• The standard deviation is simply the square root of the variance

\[
s = \sqrt{\frac{\sum^n_{i=1}(x_i - \bar{x})^2}{n-1}}
\]

• Think of this as the ``standard'' amount by which values deviate from
their mean.

• We will most often look at standard deviation, because it is the more
interpretable of the two statistics. It is in the same units as the
original variable.

\hypertarget{the-correlation-coefficient}{%
\subsection{The correlation
coefficient}\label{the-correlation-coefficient}}

• The correlation coefficient, 𝑟, quantifies the extent to which two
variables (call them X and Y) move together:

\[
r = (\frac{1}{n-1})\sum^n_{i=1}\frac{(x_i - \bar{x})(y_i-\bar{y})}{s_xs_y} = (\frac{1}{n-1})\sum^n_{i=1}z_{x_i}z_{y_i}
\]

• Don't worry too much about the formula. The most important things to
know are:

• When two variables move in the opposite direction (i.e.~when one gets
bigger, the other gets smaller), \(r\) is negative.

• When they move in the same direction, \(r\) is positive.

• \(r = 0\) means no correlation. \(r =1\) means perfect positive
correlation. \(r = -1\) means perfect negative correlation.

\hypertarget{statistics-and-parameters}{%
\section{Statistics and parameters}\label{statistics-and-parameters}}

• A statistic is any value calculated from data.

• A parameter is any value pertaining to a population.

• The values of unknown parameters are ``estimated'' using statistics.

• Example: a sample mean can be used to estimate a population mean.
i.e.~\(\bar{X}\) estimates \(\mu\).

\hypertarget{sampling-distributions}{%
\section{Sampling distributions}\label{sampling-distributions}}

• A sampling distribution is the distribution of values a statistic
takes on, under repeated sampling.

• For example, the Central Limit Theorem states that the sampling
distribution of \(\bar{X}\) will be normal, so long as the sample size
(\(n\)) is large enough.

• Sampling distributions are important because most methods used in
statistical inference invoke long run frequency properties.

• Example: if the sampling distribution of \(\bar{X}\) is not very
spread out, then the value of \(\bar{X}\) should not change much if we
take a new sample.

\hypertarget{standard-error}{%
\subsection{Standard error}\label{standard-error}}

• Standard error is the standard deviation of a sampling distribution.

• In other words, it is the amount of variability in the values a
statistic takes on under repeated sampling.

• So, if a statistic we calculate has a small standard error, we can
infer that the value of that statistic is close to the value of the
population parameter it is estimating. If it has a large standard error,
its value might be very far away from the value of the parameter.

• Example: the standard error of \(\bar{X}\) is \(\frac{s}{\sqrt{n}}\)
i.e.~\(s_{\bar{X}} = \frac{s}{\sqrt{n}}\)

\hypertarget{sampling-dist.-and-standard-error-visually}{%
\subsection{Sampling dist. and standard error,
visually}\label{sampling-dist.-and-standard-error-visually}}

• On Canvas there is a Central Limit Theorem simulator.

• When the sample size is large, the distribution of the sample mean is
not very spread out. In other words, its standard error is small.

• When the sample size is small, the standard error is large.

\begin{figure}

{\centering \includegraphics{images/mod1_4.png}

}

\end{figure}

\begin{figure}

{\centering \includegraphics{images/mod1_5.png}

}

\end{figure}

\hypertarget{confidence-intervals}{%
\section{Confidence intervals}\label{confidence-intervals}}

• A confidence interval (``CI'') is an interval (i.e.~a left endpoint
and a right endpoint) constructed around a statistic, when that
statistic is being treated as an estimate for the value of an unknown
parameter.

• Typical confidence intervals are constructed by adding a ``margin of
error'' to, and subtracting it from, an estimate:

𝐶𝐼 𝑓𝑜𝑟 𝑝𝑎𝑟𝑎𝑚𝑒𝑡𝑒𝑟 = 𝑒𝑠𝑡𝑖𝑚𝑎𝑡𝑒 𝑜𝑓 𝑝𝑎𝑟𝑎𝑚𝑒𝑡𝑒𝑟 ± 𝑚𝑎𝑟𝑔𝑖𝑛 𝑜𝑓 𝑒𝑟𝑟𝑜𝑟

• Typical margins of error are calculated by multiplying the standard
error of the estimate by a ``critical value''. A critical value comes
from a known distribution and is given by a confidence level.

• Example: a 95\% CI for a population mean uses a critical value from
the \(t\) distribution (we won't cover why this is).

𝐶𝐼 𝑓𝑜𝑟 𝜇 = \(\bar{x}\) ± \(𝑡_{𝑐𝑟𝑖𝑡𝑖𝑐𝑎𝑙}\) ∗ \(s_{\bar{x}}\)

• As long as \(n\) isn't tiny, the 95\% critical value from a \(t\)
distribution is approximately 2:

𝐶𝐼 𝑓𝑜𝑟 𝜇 ≈ \(\bar{x}\) ± \(2\) ∗ \(s_{\bar{x}}\)

• Confidence intervals should capture the unknown parameter value being
estimated. The confidence level gives how often the interval captures
the parameter under repeated sampling.

• Example: 95\% of all 95\% CIs for \(\mu\) capture \(\mu\) .

• The confidence level can also be thought of as the success rate of the
method being used.

• So, 95\% confidence intervals have a 95\% success rate in capturing
the value of the unknown parameter.

• Just as with sampling distributions, we are invoking repeated sampling
here. We say that a 95\% CI can be trusted because it is created using a
method that would ``work'' 95\% of the time, if we were to keep taking
new samples and keep constructing 95\% CIs.

\hypertarget{confidence-intervals-quantify-uncertainty}{%
\section{Confidence intervals quantify
uncertainty}\label{confidence-intervals-quantify-uncertainty}}

• The most important characteristic of a CI is its width.

• We are typically willing to believe that the unknown value of a
parameter lies inside the confidence interval constructed from our data.

• If the confidence interval is wide, there is a lot of uncertainty as
to the true value of the parameter.

• If the confidence interval is narrow, then our estimate for the value
of the parameter is ``precise'', in that it shouldn't be wrong by much.

• CIs are narrow when the sample size is large and / or the standard
deviation of our data is small.

• CIs are wide with the sample size is small and / or the standard
deviation of our data is large.

• IMPORTANT: CIs, like all inferential statistical methods, are created
under assumptions. We make distributional assumptions about our data
(e.g.~normality). We assume our statistic is an unbiased estimate of the
parameter, i.e.~it will not systematically differ from the parameter
value under repeated sampling.

\hypertarget{confidence-interval-simulation-apps}{%
\section{Confidence interval simulation
apps}\label{confidence-interval-simulation-apps}}

• There is a confidence interval simulation app on Canvas, that
demonstrates creating confidence intervals ``under repeated sampling''.
This app is from Brown University's ``Seeing theory'' series .

• There is also a ``sampling distribution and standard error'' app on
Canvas. It shows a population distribution for a normally distributed
variable, a sample of data from that distribution, and the sampling
distribution of the mean.

• This app also super-imposes the standard error in pink.

\begin{figure}

\includegraphics[width=2.29167in,height=\textheight]{images/mod1_6.png} \hfill{}

\end{figure}

\includegraphics[width=3.53125in,height=\textheight]{images/mod1_7.png}

\begin{figure}

\includegraphics[width=2.32292in,height=\textheight]{images/mod1_8.png} \hfill{}

\end{figure}

\includegraphics[width=3.53125in,height=\textheight]{images/mod1_9.png}

\hypertarget{confidence-intervals-in-jamovi}{%
\section{Confidence intervals in
jamovi}\label{confidence-intervals-in-jamovi}}

• Jamovi can create a 95\% CI and any other summary statistics selected
for using the Statistics menu of Descriptives

• For example, when producing summary statistics for a variable using
the Statistics drop down menu in Descriptives, you will need to select
Confidence Interval for Mean found under Mean Dispersion. Jamovi will
report ``95\% CI mean lower bound'' and ``95\% CI mean upper bound''.
These are the endpoints for the 95\% CI for \(\bar{X}\).

\begin{figure}

\hfill{} \includegraphics[width=2.08333in,height=\textheight]{images/mod1_10.png}

\end{figure}

\hypertarget{hypothesis-testing}{%
\section{Hypothesis testing}\label{hypothesis-testing}}

• Hypothesis testing is an inferential method in which a null hypothesis
(\(H_0\)) is ``tested'' against. If the data are in strong enough
disagreement with \(H_0\) , then \(H_0\) is rejected.

• \(H_0\) typically represents the proposition that ``there is nothing
of interest at the population level'', or ``the proposed research
hypothesis is not true''.

• If \(H_0\) is rejected, then the result of the test is described as
``statistically significant''.

• Example: if we have data from a controlled experiment in which
\(\mu_1\) represents the population mean for the control group and
\(\mu_2\) represents the population mean for the treatment group, then
we might test against the null hypothesis:

\[
H_0: \mu_1 = \mu_2,\text{which is equivalent to }H_0: \mu_1 − \mu_2 = 0
\]

• If we reject \(H_0\), we say that the sample means, \(\bar{x}_1\) and
\(\bar{x}_2\), are ``significantly different''. Or, equivalently, that
\(\bar{x}_1 - \bar{x}_2\) is ``signficantly different'' from zero.

\hypertarget{the-test-statistic}{%
\section{The test statistic}\label{the-test-statistic}}

• The strength of the evidence against \(H_0\) is quantified by a ``test
statistic'', from which a ``p-value'' is calculated.

• Test statistics are set up so that, the more the inconsistent the data
are with \(H_0\), the larger the test statistic will be.

• Example: when testing \(H_0: \mu_1 − \mu_2 = 0\), we use the test
statistic:

\[
t = \frac{\bar{x}_1 - \bar{x}_2}{s_{(\bar{x}_1 - \bar{x}_2)}} = \frac{\bar{x}_1 - \bar{x}_2}{\sqrt{\frac{s_1^2}{n_1} + \frac{s_2^2}{n_2}}} \text{(where } s_{(\bar{x}_1 - \bar{x}_2)}\text{ is the standard error of }\bar{x}_1 - \bar{x}_2)
\]

\hypertarget{the-p-value}{%
\section{The p-value}\label{the-p-value}}

• The p-value is defined as the probability of getting a test statistic
at least as large as the one calculated, if we assume \(H_0\) is true.

• Visually, the p-value is the area in the tail of the sampling
distribution of the test statistic under \(H_0\)

\begin{figure}

{\centering \includegraphics{images/mod1_11.png}

}

\end{figure}

• If the p-value is less than the ``level of significance''
(\(\alpha\)), then \(H_0\) is rejected.

• By far the most typical level of significance is \(\alpha = 0.05\)

\begin{figure}

{\centering \includegraphics{images/mod1_12.png}

}

\end{figure}

\hypertarget{interpreting-statistical-significance}{%
\subsection{Interpreting ``statistical
significance''}\label{interpreting-statistical-significance}}

• ``Statistically significant'' results are those that produce a small
p-value.

• Small p-values result from data that would be unlikely to be obtained
just by chance, if the null hypothesis were true.

• So, when you hear that results are ``statistically significant'', you
can interpret this as meaning ``the data we obtained don't look like the
kind of data we'd expect to see just by chance''.

\hypertarget{cautions-regarding-statistical-significance}{%
\subsection{Cautions regarding ``statistical
significance''}\label{cautions-regarding-statistical-significance}}

• As with confidence intervals, hypothesis tests require assumptions.

• These will be covered in detail in the next module.

• The most important distinction to be made right now is the distinction
between statistical significance and practical importance.

• Results can be statistically significant, but still seem weak or
unimpressive by practical standards.

• Example: this is a statistically significant correlation:

\begin{figure}

\includegraphics[width=2.70833in,height=\textheight]{images/mod1_13.png} \hfill{}

\end{figure}

(\(r = 0.22\), p-value = 0.011)

• Example: this is a statistically significant difference in means:

\begin{figure}

\includegraphics[width=2.71875in,height=\textheight]{images/mod1_14.png} \hfill{}

\end{figure}

(\(\bar{x}_1 = 19.7\), \(\bar{x}_2 = 22.7\), p-value = 0.0013)

• The use of hypothesis testing is controversial.

• I personally do not like hypothesis testing, and I think that
statistical significance is usually uninteresting.

• We'll explore the debates surrounding statistical significance in
module 2.

\hypertarget{confidence-intervals-vs.-p-values}{%
\subsection{Confidence intervals
vs.~p-values}\label{confidence-intervals-vs.-p-values}}

• For now, we'll note that in many cases, confidence intervals can be
used in place of p-values to perform a hypothesis test.

• If a 95\% CI excludes the null value (typically zero), then \(H_0\) is
rejected at the \(\alpha = 0.05\) level of significance.

• The advantage of using a confidence interval rather than a p-value is
that it is easier to make sense out of, and it quantifies uncertainty:
the wider the CI, the more uncertainty there is regarding the value of
the unknown parameter.

\hypertarget{confidence-intervals-vs.-p-values-example}{%
\subsection{Confidence intervals vs.~p-values
example}\label{confidence-intervals-vs.-p-values-example}}

\begin{figure}

\includegraphics[width=2.71875in,height=\textheight]{images/mod1_14.png} \hfill{}

\end{figure}

\(\bar{x}_1 = 19.7\), \(\bar{x}_2 = 22.7\), p-value = 0.0013

95\% CI for \(\mu_1 - \mu_2: (-4.72,-1.19)\)

\begin{figure}

\includegraphics[width=2.70833in,height=\textheight]{images/mod1_15.png} \hfill{}

\end{figure}

• Here we see 95\% CIs for differences in means, along with their
corresponding p-values.

• Note how much the p-values change for small changes in the CIs.

• Note also that different CIs can correspond to the same p-value.

\hypertarget{cis-and-p-values-in-jamovi}{%
\section{CIs and p-values in jamovi}\label{cis-and-p-values-in-jamovi}}

• CIs and p-values can be calculated for a huge variety of statistics.

• For now, we will consider testing for a difference in means.

• In jamovi, select an Independent Samples T-Test from T-Tests under the
Analyses tab

• Make Max\_Temp\_Challenge be the Dependent variable (the one
containing measurements), and make Vaccine be the Grouping variable (the
one identifying which group the measurement belongs to).

• Here is an example using the Vaccine data set. This example uses sheet
3 of the Excel file, titled ``H3N2\_Clinical\_Max'': Data will need to
be prepared for test by swapping the rows so that Vaccine occurs first.

\begin{figure}

{\centering \includegraphics{images/mod1_16.png}

}

\end{figure}

\begin{figure}

{\centering \includegraphics{images/mod1_17.png}

}

\end{figure}

\begin{figure}

{\centering \includegraphics{images/mod1_18.png}

}

\end{figure}

\begin{figure}

{\centering \includegraphics{images/mod1_19.png}

}

\end{figure}

\hypertarget{cis-and-p-values-in-jmp}{%
\section{CIs and p-values in JMP}\label{cis-and-p-values-in-jmp}}

\begin{figure}

{\centering \includegraphics{images/mod1_20.png}

}

\end{figure}

• Here, the p-value is \(0.002\)

• The 95\% CI for the difference in population mean max\_temp is
\((−2.07, −0.58)\)

• The confidence interval excludes zero and \(p < 0.05\), so the
difference in sample means is statistically significant.

\hypertarget{data-format-in-jamovi}{%
\section{Data format in jamovi}\label{data-format-in-jamovi}}

\begin{itemize}
\item
  A final note on data formatting: this data set is in "long form",
  meaning each row is a single observation and each column is a
  variable.
\item
  jamovi\textquotesingle s Independent Samples T-test requires long form
  data.
\end{itemize}

\begin{itemize}
\tightlist
\item
  Sometimes you\textquotesingle ll have data in "wide form", where each
  column is a group, and the rows do not correspond to single
  observations
\end{itemize}

\begin{itemize}
\tightlist
\item
  Example: here\textquotesingle s some fake data in wide form:
\end{itemize}

\begin{figure}

{\centering \includegraphics[width=2.21875in,height=\textheight]{images/mod1_21.png}

}

\end{figure}

\begin{itemize}
\tightlist
\item
  Fit Independent Samples T-test cannot be used to compare these means.~
  jamovi thinks there are 4 observations, each with a measurement on
  Var1 and Var 2.~
\end{itemize}

\begin{figure}

{\centering \includegraphics{images/mod1_22.png}

}

\end{figure}

\begin{itemize}
\item
  This isn\textquotesingle t what we want!
\item
  To transform the data into long form, we need to install the Rj --
  Editor module which will allow us to run R-code in jamovi
\end{itemize}

\hypertarget{installing-rj-in-jamovi}{%
\section{Installing Rj in jamovi}\label{installing-rj-in-jamovi}}

\begin{itemize}
\item
  Navigate to the Analyses tab

  \begin{itemize}
  \item
    Click on Modules in the top right of the jamovi window
  \item
    Click jamovi library
  \item
    Scroll until you see "Rj -- Editor to run R code", click install
  \item
    You should now see an R logo under the Analyses tab
  \end{itemize}
\end{itemize}

\hypertarget{wide-form-to-long-form-in-jamovi}{%
\section{Wide form to long form in
jamovi}\label{wide-form-to-long-form-in-jamovi}}

\begin{itemize}
\item
  To switch from wide to long form, we will write a simple line of
  R-code using the Rj -- Editor module:~
\item
  Click on Rj, it will open an empty window where we can enter R - code
\end{itemize}

\begin{figure}

{\centering \includegraphics{images/mod1_23.png}

}

\end{figure}

\begin{itemize}
\item
  The code below transforms the data into long form by stacking the data
  within Var1 and Var2 into a new column Data and creates a new column
  Labels to identify if data is from Var1 or Var2.
\item
  The data will output a csv file which we can import from a new session
  of jamovi
\end{itemize}

\includegraphics{images/mod1_24.png}

\begin{itemize}
\item
  Importing the transformed csv file to a new jamovi window shows our
  transformed long form data table. Note: column names will need to be
  updated
\item
  Compare the two:
\end{itemize}

\emph{``Long form''}

\begin{figure}

\includegraphics[width=1.61458in,height=\textheight]{images/mod1_25.png} \hfill{}

\end{figure}

\emph{``Wide form''}

\begin{figure}

\includegraphics[width=1.63542in,height=\textheight]{images/mod1_26.png} \hfill{}

\end{figure}

\begin{itemize}
\tightlist
\item
  Now Independent Samples T-test can be used to compare means:
\end{itemize}

\begin{figure}

\includegraphics[width=2.60417in,height=\textheight]{images/mod1_27.png} \hfill{}

\end{figure}

\begin{figure}

\hfill{} \includegraphics[width=2.60417in,height=\textheight]{images/mod1_28.png}

\end{figure}

\bookmarksetup{startatroot}

\hypertarget{chapter-2-model-building-with-linear-regression}{%
\chapter{Chapter 2: Model building with linear
regression}\label{chapter-2-model-building-with-linear-regression}}

Module 2 covers linear regression models. Parts 1 through 4 will be
mostly review for students whose introductory statistics course covered
regression. Parts 5 through 8 introduce topics not usually covered in
introductory courses.

For more details on regression, see
\href{https://crumplab.com/statistics/03-Correlation.html\#regression-a-mini-intro}{section
3.5 of Answering questions with data} and
\href{https://davidfoxcroft.github.io/lsj-book/12-Correlation-and-linear-regression.html}{chapter
12 of Learning statistics with jamovi}

\hypertarget{outline-of-notes}{%
\section{Outline of notes:}\label{outline-of-notes}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  The linear regression equation
\item
  Regression analysis in jamovi
\item
  Interpreting regression results
\item
  Applied example (``The Binary Bias'')
\item
  Interaction between variables, conceptually
\item
  Interaction between variables in a regression model
\item
  Applied example: arthritis treatment data
\item
  Centering predictor variables
\item
  Standardizing predictor variables
\end{enumerate}

\hypertarget{the-linear-regression-equation}{%
\section{The linear regression
equation}\label{the-linear-regression-equation}}

Linear regression, in its simplest form, is a method for finding the
``best fitting'' line through a set of bivariate (two variable) data:

\begin{figure}

{\centering \includegraphics{images/mod2_pt1 (1).png}

}

\end{figure}

What is meant by ``best fitting'' will be addressed shortly. For now,
think of the line as showing the underlying linear trend through a set
of data.

The vertical distance between each data point and the line is called a
``residual''. On the plot above, the red lines represent residuals. They
quantify the amount by which a data point deviates from the underlying
linear trend. Every point has a residual; the plot above only shows a
few of them.

\hypertarget{the-linear-regression-equation-as-a-statistical-model}{%
\subsection{The linear regression equation as a statistical
model}\label{the-linear-regression-equation-as-a-statistical-model}}

The line that is drawn through data comes from a \textbf{statistical
model}. A statistical model is a mathematical expression describing how
data are generated. It has a \emph{fixed} component and a \emph{random}
component. Think of the fixed component as describing the underlying
relationship between variables, and the random component as describing
any additional variability in data beyond what the fixed component
describes.

Below is the standard linear regression model. The random component is
represented by \(``\varepsilon_i"\). Everything from ``\(\beta_0\)'' up
until ``\(\varepsilon_i\)'' is the fixed component.

\[
Y_i=\beta_0+\beta_1x_{1i} + \beta_2x_{2i} + \dots + \beta_kx_{ki} + \varepsilon_i \\
i = 1, \dots, n \\
\varepsilon_i \sim \text{Normal}(0,\sigma^2) 
\]

Here's what each term represents:

\begin{itemize}
\item
  \(i\) is the index term. It counts through the data, starting at
  \(i = 1\) and going through \(i=n\), where \(n\) is the sample size.
\item
  \(Y_i\) is the \(i^{th}\) value of the outcome variable. When written
  in upper-case, \(Y_i\) is treated as a random variable whose value has
  not been observed. When written lower-case, \(y_i\) represents an
  observed data value.

  \(Y_i\) is often referred to as the ``response'' variable, or the
  ``dependent'' variable. These notes will use the term ``outcome''
  variable. I prefer this term on the grounds that the others seem to
  imply causality: if \(Y\) is ``responding'' to \(x\), or ``dependent''
  on \(x\), then it sounds like changing the value of \(x\) will induce
  a change in the value of \(Y\).
\item
  \(x_{1i}\) is the \(i^{th}\) value of the first predictor
  (i.e.~independent) variable. \(x_{2i}\) is the \(i^{th}\) value of the
  second predictor, etc. The \(x's\) are always written lower-case, and
  technically are assumed to be fixed values, either set prior to data
  collection or measured without error.
\item
  \(\beta_1\) is the slope (i.e.~regression coefficient) for the first
  predictor variable. \(\beta_2\) is the slope of the second predictor,
  etc. The \(\beta's\) are \emph{parameters}, meaning their values are
  treated as fixed (existing at the ``population'' level) but unknown.

  We use data to calculate estimated values for the \(\beta's\), and
  these estimates are written using hat notation. For example,
  \(\hat{\beta_1}\) is the estimated value for \(\beta_1\).
\item
  \(\varepsilon_i\) is the \(i^{th}\) random error value. This is the
  amount by which \(Y_i\) differs from
  \(\beta_0+\beta_1x_{1i} + \beta_2x_{2i} + \dots + \beta_kx_{ki}\),
  i.e.~the fixed component of the model.

  The amount by which \(y_i\) (the \(i^{th}\) \emph{observed} value of
  \(y\)) differs from
  \(\hat{\beta_0}+\hat{\beta_1}x_{1i} + \hat{\beta_2}x_{2i} + \dots + \hat{\beta_k}x_{ki}\)
  is called the \(i^{th}\) residual, which we can denote \(e_i\).

  The errors are modeled as random values that are drawn from a normal
  distribution with mean zero and variance \(\sigma^2\).
\end{itemize}

\begin{tcolorbox}[enhanced jigsaw, bottomtitle=1mm, coltitle=black, breakable, colback=white, colbacktitle=quarto-callout-note-color!10!white, colframe=quarto-callout-note-color-frame, rightrule=.15mm, opacitybacktitle=0.6, arc=.35mm, opacityback=0, titlerule=0mm, toprule=.15mm, bottomrule=.15mm, toptitle=1mm, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Note}, leftrule=.75mm, left=2mm]

The errors in a regression model do not have to come from a normal
distribution. This assumption is made in order to justify inferences
about the coefficients; more on this soon.

\end{tcolorbox}

When a regression model has only one predictor variable, it is called a
``simple'' regression model. If it has more than one predictor variable,
it is called a ``multiple regression'' model.

\hypertarget{assumptions-of-the-regression-model}{%
\subsection{Assumptions of the regression
model}\label{assumptions-of-the-regression-model}}

This model implies some assumptions:

\begin{itemize}
\item
  The response variable \(Y\) is an additive, linear function of the
  predictors (the \(x\) variables)
\item
  If we fix the value(s) of the \(x\) variable(s), all values of \(Y\)
  will be normally distributed. In other words, the \textbf{errors} are
  normally distributed.
\item
  The errors have the same variance regardless of the values of the
  \(x's\). This variance is denoted \(\sigma^2\)
\end{itemize}

\begin{tcolorbox}[enhanced jigsaw, bottomtitle=1mm, coltitle=black, breakable, colback=white, colbacktitle=quarto-callout-note-color!10!white, colframe=quarto-callout-note-color-frame, rightrule=.15mm, opacitybacktitle=0.6, arc=.35mm, opacityback=0, titlerule=0mm, toprule=.15mm, bottomrule=.15mm, toptitle=1mm, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Note}, leftrule=.75mm, left=2mm]

The square root of the variance is the \emph{standard deviation},
denoted \(\sigma\). Standard deviation is expressed in the same units of
the original variable, whereas variance is expressed in squared units.

For this reason, standard deviation is typically referred to when
interpreting statistical results. Variance has desirable mathematical
properties, and so is more often referred to in statistical theory

\end{tcolorbox}

Visually, this model treats values of Y as being generated randomly from
normal distributions centered on the line:

\includegraphics{images/mod2_pt1 (6).png}

\emph{(figure derived from
\href{https://openstax.org/books/introductory-business-statistics/pages/13-4-the-regression-equation}{OpenStax
Introductory Business Statistics, section 13.4})}

The ``errors'' are the distances between the line and the values
generated from the normal distributions.

The errors are treated as random and uncorrelated: knowing the value of
one error tells you nothing about the likely value of the next.

\hypertarget{regression-analysis-in-jamovi}{%
\section{Regression analysis in
jamovi}\label{regression-analysis-in-jamovi}}

\hypertarget{simulating-the-regression-model-in-jamovi}{%
\subsection{Simulating the regression model in
jamovi}\label{simulating-the-regression-model-in-jamovi}}

We noted earlier that a statistical model is \emph{data generating}. It
describes, mathematically, how values of the outcome variable \(Y\) can
be created. Consider the ``simple'' (single \(x\) ) regression model:

\[
Y_i=\beta_0+\beta_1x_{i} + \varepsilon_i \\
\varepsilon_i \sim \text{Normal}(0,\sigma^2) 
\]

If we have values for \(\beta_0\), \(\beta_1\), and \(\sigma^2\), when
we can plug in values for \(x_i\) to generate values for \(Y_i\). Let's
do this using jamovi.

In jamovi, we first create X values by double-clicking an empty column,
choosing ``New Computed Variable'' then the \(f_x\) drop down menu and
double click UNIF.

\begin{figure}

{\centering \includegraphics{images/mod2_pt1 (7).png}

}

\end{figure}

Here, we are generating 100 random values from a \(Uniform(0,100)\)
distribution. The uniform distribution is a distribution where all
values are equally likely, so we should get an even spread of values
between 0 and 100.

To simulate values of the response variable, we'll need to make up
values for each parameter in the model. Say we want to generate values
from this model:

\(Y_i=10+0.7x_i+\varepsilon_i \quad \varepsilon_i \sim Normal(0,8^2)\)

This means we've decided that \(\beta_0=10\), \(\beta_1=0.7\), and
\(\sigma=8\). And since we've generated 100 values for \(x_i\), we've
also decided that \(i=1\dots 100\)

Double click an empty column and choose ``New Computed Variable'':

\begin{figure}

{\centering \includegraphics{images/mod2_pt1 (10).png}

}

\end{figure}

Now make the formula look like the right side of the regression equation
from the previous slide:

\includegraphics{images/mod2_pt1 (9).png}

Now we can take a look using Scatterplot, a downloadable jamovi module.
Click the icon of the plus sign labeled ``Modules'' to bring up a list
of available modules you can install. We will use many modules in STAT
331.

\includegraphics{images/mod2_pt1 (11).png}

After installation, Scatterplot is available under Exploration in the
Analyses tab. You can assign the X and Y axis variables, and get a plot
that looks something like this:

\includegraphics{images/mod2_pt1 (12).png}

These are random data, so yours will look a little bit different. But
the scales of the axes and vertical spread of the data should be
similar.

Next, we'll fit a regression model to this data. In practice, we do not
know the values of the parameters in our model, so we estimate them
using data. This is known as ``fitting'' the model to the data. The
point of this simulation is to look at what kind of results we get when
fiting a regression model to fake data that was produced by a mechanism
we fully understand.

\hypertarget{fitting-a-regression-model-in-jamovi}{%
\section{Fitting a regression model in
jamovi}\label{fitting-a-regression-model-in-jamovi}}

We can use Regression / Linear Regression to fit a ``simple'' regression
model, which is a regression model with just one predictor.

\begin{figure}

{\centering \includegraphics{images/mod2_pt1 (13)-01.png}

}

\end{figure}

The response variable is ``Dependent Variable''.

The predictor variable goes under ``Covariates''.

\begin{figure}

{\centering \includegraphics{images/mod2_pt1 (14).png}

}

\end{figure}

After selecting variables, model will automatically be fit, and output
will be generated to the right under ``Results''.

\includegraphics{images/mod2_pt1 (17).png}

Based on these results, here is the estimated regression model:

\[
\hat{y_i}=\hat{\beta_0}+\hat{\beta_1}x_i=9.1+0.71x_i
\]

\[
\hat{\sigma}=\sqrt{MSE}=\sqrt{64.8}=7.93
\]

Note that it is standard to denote estimated values using ``hats''. The
``estimate'' column is where we find the values for the estimated
regression coefficients \(\hat{\beta_0}\) and \(\hat{\beta_1}\).
``RMSE'' (``root mean square error'') is the estimated standard
deviation of the errors, assumed to have come from a normal
distribution. Compare these results to the values used to generate our
fake data:

\[
\begin{align}
&\beta_0=10 &\beta_1=0.7 \quad &\sigma=8 \\
&\hat{\beta_0}=9.099 &\hat{\beta_1}=0.712 \quad &\hat{\sigma}=7.93 \\
&s_{\hat{\beta_0}}=1.776 &s_{\hat{\beta_1}}=0.034
\end{align}
\]

\hypertarget{the-r2-statistic}{%
\section{\texorpdfstring{The \(R^2\)
statistic}{The R\^{}2 statistic}}\label{the-r2-statistic}}

Note that the output tells us \(R^2=0.874\). This is a statistic
quantifying how well this model can ``predict'' the data used to fit it.
It is found from the ``sum of squares'' values in the ANOVA table,
Generically:

\[
R^2=\frac{\text{model sum of squares}}{\text{total sum of squares}}=\frac{\text{model sum of squares}}{\text{model sum of squares + residual sum of squares}}
\]

\begin{tcolorbox}[enhanced jigsaw, bottomtitle=1mm, coltitle=black, breakable, colback=white, colbacktitle=quarto-callout-note-color!10!white, colframe=quarto-callout-note-color-frame, rightrule=.15mm, opacitybacktitle=0.6, arc=.35mm, opacityback=0, titlerule=0mm, toprule=.15mm, bottomrule=.15mm, toptitle=1mm, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Note}, leftrule=.75mm, left=2mm]

``Sums of squares'' are used to quantify variance. You can think of this
as being short for ``sum of the squared distances between some values
and their mean''. For example, the variance statistic is

\[
s^2=\frac{\sum_{i=1}^n (y_i-\bar{y})}{n-1}\
\] The numerator, \(\sum_{i=1}^n (y_i-\bar{y})\), is a ``sum of
squares'' - the sum of the squared deviations between all the values of
\(y_i\) and their mean, \(\bar{y}\).

\end{tcolorbox}

In this example:

\[
R^2=\frac{29162}{29162+4211}=0.874
\]

So, in this case, \(87.4\%\) of the total variance in \(Y\) can be
accounted for using the values of \(x\). Here's the data again, with the
estimated regression line added:

\includegraphics{images/mod2_pt1 (29).png}

\begin{itemize}
\item
  The total variance in \(Y\) quantifies how much the data vary
  vertically around the horizontal red line, which is the mean of \(Y\).
\item
  The residual (or ``error'') variance quantifies how much the data vary
  vertically around the regression line.
\end{itemize}

Here, the data are relatively much closer to the regression line than to
the the horizontal mean line, and so residual sum of squares is only a
small portion of the total sum of squares, making \(R^2\) fairly large.

An alternative interpretation of \(R^2\) is that is quantifies the
\emph{proportional decrease in residual variance when using the
regression line rather than using only the mean of} \(Y\)\emph{.}

Looking at the plot above, you can imagine drawing vertical lines from
each data point to the horizontal red line representing the mean of
\(Y\). If you squared these lines and added them up, you'd have the
total sum of squares, which would also be the residual sum of squares if
you were using only the mean of \(Y\) to calculate residuals. In this
case, using the regression line instead of just the mean to calculate
residuals would represent an \(87.4\%\) decrease in residual sum of
squares.

Said differently, \(R^2\) tells you how much better your predictions for
\(Y\) would be if you use the regression line rather than only the mean.

\hypertarget{there-is-no-good-or-bad-value-for-r2}{%
\subsection{\texorpdfstring{There is no ``good'' or ``bad'' value for
\(R^2\)}{There is no ``good'' or ``bad'' value for R\^{}2}}\label{there-is-no-good-or-bad-value-for-r2}}

When residual variance in \(Y\) is larger, \(R^2\) is smaller. Visually,
when the data are more spread out around the regression line, \(R^2\) is
smaller. Is this ``bad''? I want you to resist such an interpretation. A
small \(R^2\) tells you that \(Y\) is being influenced by a lot more
than just what is in your model. And this is often to be expected.

For instance, if I'm trying to predict how many tomatoes are produced
per tomato plant in different parts of the country and my only predictor
variable is average daily outdoor temperature, I should not expect a
large \(R^2\). This is because there are many many more variables that
influence how many tomatoes will grow (e.g.~properties of soil, watering
schedule, fertilizer, pests\ldots). But, a small \(R^2\) should not be
interpreted as ``average daily outdoor temperature doesn't matter when
growing tomatoes''. It should be interpreted as ``there are way more
other things that matter when growing tomatoes, and their combined
influence is much greater than average daily outdoor temperature
alone''.

\hypertarget{sums-of-squares-and-mean-squares}{%
\section{Sums of squares and mean
squares}\label{sums-of-squares-and-mean-squares}}

• We will look at some formulas in this section. Some are based on sums
of squares, which are reported in the ANOVA table.

• Total sum of squares quantifies total variability in \(y\):

\[
SS_{Total} = \sum^n_{i=1}(y_i - \bar{y})^2
\]

• Note that this has nothing to do with the regression line, or the
predictor variable. It quantifies variability in the response variable
alone.

\hypertarget{total-sum-of-squares}{%
\subsection{Total sum of squares}\label{total-sum-of-squares}}

• Visually, \(SS_{Total}\) is the sum of the squared vertical deviations
between each data point and the mean of 𝑦, shown here as a horizontal
line.

\begin{figure}

{\centering \includegraphics[width=4.16667in,height=\textheight]{images/Mod2_pt1_1.png}

}

\end{figure}

• The two blue lines drawn are two such instances of these deviations.
If we drew these for every data point, squared them, and added them up,
we'd have \(SS_{Total}\)

• Again, note that this quantity has nothing to do with the regression
line!

\hypertarget{residual-error-sum-of-squares}{%
\subsection{Residual / Error sum of
squares}\label{residual-error-sum-of-squares}}

• Error sum of squares quantifies total variability in 𝑦 around the
regression line:

\[
SS_{Error} = \sum^n_{i=1}(y_i - \hat{y}_i)^2 = \sum^n_{i=1}[y_i - (\hat{\beta}_0 + \hat{\beta}_1)]^2
\]

• This is also known as the ``sum of the squared residuals'', where a
residual is the vertical distance between a data point and the
regression line. The values of \(\hat{\beta}_0\) and \(\hat{\beta}_1\)
are chosen so as to minimize \(SS_{Error}\).

• In other words, the regression line drawn through the data produced a
smaller \(SS_{Error}\) than any other line we could possibly draw.

• Visually, \(SS_{Error}\) is the sum of the squared vertical deviations
between each data point and the regression line.

\begin{figure}

{\centering \includegraphics[width=4.16667in,height=\textheight]{images/Mod2_pt1_2.png}

}

\end{figure}

• Here, the blue lines are two instances of these deviations

• \(SS_{Error}\) will be larger when the data are more spread out around
the line, and vice versa.

\hypertarget{mean-square-error}{%
\subsection{Mean square error}\label{mean-square-error}}

• Error mean square (aka mean square error) is given by

\[
MSE = MS_{Error} = \frac{SS_{Error}}{n-k +1}
\]

• \(k\) is the number of predictor variables. Example: for simple
regression, \(k = 1\), so \(MSE = \frac{SS_{Error}}{N-2}\)

• As seen earlier, \(\sqrt{MSE}\) is the estimate for the standard
deviation of the residuals around the line:
\(\hat{\sigma} = \sqrt{MSE}\)

\hypertarget{model-sum-of-squares}{%
\subsection{Model sum of squares}\label{model-sum-of-squares}}

• Model sum of squares (aka ``regression sum of squares'') is given by:

\[
SS_{Model} = \sum^n_{i=1}(\hat{y}_i - \bar{y})^2
\]

• This can be thought of as quantifying how much better the model is
than \(\bar{y}\) alone at accounting for variation in the values of
\(y\).

• Visually, \(SS_{Model}\) is the sum of the squared vertical deviations
between the regression line and the horizontal line, at each value of
the predictor variable.

\begin{figure}

{\centering \includegraphics[width=4.15625in,height=\textheight]{images/Mod2_pt1_3.png}

}

\end{figure}

• Here, the blue lines are two instances of these deviations, associated
with the circled blue data points.

\hypertarget{ss_total-ss_error-ss_model}{%
\subsubsection{\texorpdfstring{\(SS_{Total} = SS_{Error} + SS_{Model}\)}{SS\_\{Total\} = SS\_\{Error\} + SS\_\{Model\}}}\label{ss_total-ss_error-ss_model}}

• Total sum of squares are equal to the sum of error sum of squares and
model sum of squares.

• Note that this also implies:

\[
SS_{Model} = SS_{Total} - SS_{Error} \\
SS_{Error} = SS_{Total} - SS_{Model}
\]

\hypertarget{interpreting-regression-results}{%
\section{Interpreting regression
results}\label{interpreting-regression-results}}

• Here again is the estimated model from the previous example:

\[
\hat{y}_i = 9.1 + 0.71x_i
\]

• The intercept, \(\hat{\beta}_0 = 9.1\), gives the predicted value of
the response variable (\(y\)) when the predictor variable (\(x\)) equals
zero. This is not typically of practical interest.

• The slope, \(\hat{\beta}_1 = 0.71\), gives the predicted change in
\(y\) for a one unit increase in \(x\).

\hypertarget{more-on-interpreting-the-slope}{%
\subsubsection{More on interpreting the
slope}\label{more-on-interpreting-the-slope}}

• The slope is often of practical interest. It tells us how much the
response variable changes, on average, when the predictor variable
increases by one unit.

• This interpretation is very common. It is also dangerous, because it
is phrased in a way that suggests changes in 𝑥 cause changes in \(y\).

• Here is an alternate, non-causal sounding interpretation:

\emph{If we observe two values of} \(x\) \emph{that are one unit apart,
we estimate that their corresponding average} \(y\) \emph{values will
be} \(\hat{\beta}_1\) \emph{units apart.}

• Visually, we can choose two values of \(x\), go up to the line, and
record the values of \(y\). The slope tells us how much these \(y\)
values are expected to differ.

\begin{figure}

{\centering \includegraphics[width=4.16667in,height=\textheight]{images/Mod2_pt1_4.png}

}

\end{figure}

• Here, when we compare \(x = 20\) and \(x = 80\), we expect their
corresponding \(y\) values to differ by:

\[
(80 − 20) ∗ 0.725 = 43.5
\]

\hypertarget{inference-on-the-coefficients}{%
\subsubsection{Inference on the
coefficients}\label{inference-on-the-coefficients}}

• For each estimated coefficient (\(\hat{\beta}_0\) and
\(\hat{\beta}_0\)), jamovi reports a standard error, along with a t-test
statistic and p-value testing the null that the parameter being
estimated equals zero.

\begin{figure}

{\centering \includegraphics[width=4.16667in,height=\textheight]{images/Mod2_pt1_5.png}

}

\end{figure}

• Example: the above output shows the test of \(H_0: \beta_1 = 0\)

\[
t = \frac{\hat{\beta}_1}{s_{\hat{\beta}_1}} = \frac{0.712}{0.034} = 21.22\\
p-value < 0.01 \\
\text{"reject } H_0"
\]

• We can use these results to create an approximate 95\% confidence for
\(\beta_1\):

\[
95\% \text{ CI for } \beta_1 \approx \hat{\beta}_1 \pm 2* s_{\hat{\beta}_1} = 0.712 \pm 2*0.034 = (0.645,0.779)
\]

• This is a very narrow interval, suggesting a ``precise'' estimate of
\(\beta_1\)

• For both the hypothesis test and 95\% CI, the results depend on how
large the estimate of \(\beta_1\) is, relative to its standard error.

• In this case, \(\hat{\beta}_1\) is very large relative to
\(s_{\hat{\beta}_1}\), so the result is ``highly significant'' and the
95\% CI is narrow.

\hypertarget{formulas-for-hatbeta_1-and-s_hatbeta_1}{%
\subsubsection{\texorpdfstring{Formulas for \(\hat{\beta}_1\) and
\(s_{\hat{\beta}_1}\)}{Formulas for \textbackslash hat\{\textbackslash beta\}\_1 and s\_\{\textbackslash hat\{\textbackslash beta\}\_1\}}}\label{formulas-for-hatbeta_1-and-s_hatbeta_1}}

• Now that you've seen how \(\hat{\beta}_1\) and \(s_{\hat{\beta}_1}\)
are used, here are their formulas:

\[
\hat{\beta}_1 = r_{xy} * \frac{s_y}{s_x} \\
s_{\hat{\beta}_1} = \sqrt{\frac{MSE}{\sum^n_{i=1}(x_i - \bar{x})^2}} = \sqrt{\frac{1-R^2}{n-k-1}}*\frac{s_y}{s_x}
\]

• \(\hat{\beta}_1\) is larger when the correlation between \(x\) and
\(y\) is stronger, and when the variability in \(y\) is larger relative
to the variability in \(x\).

•\(s_{\hat{\beta}_1}\) is smaller when \(R^2\) is larger, when \(n\) is
larger, and when the variability in \(x\) is larger relative to the
variability in \(y\).

\hypertarget{applied-example-the-binary-bias}{%
\section{Applied example: ``The binary
bias''}\label{applied-example-the-binary-bias}}

• There is a paper up on Canvas, titled ``The Binary Bias: A Systematic
Distortion in the Integration of Information''. This is a 2018 paper
published in Psychological Science with open data.

• The overall hypothesis is that people tend to assess continuous
information using binary thinking. The experiments all involve testing
to see if participants will give higher or lower assessments of where an
average lies, based on the ``imbalance'' of the data: i.e.~the
comparative frequency with which very low or very high values turn up.

• Here is the section of the paper describing the results of Study 1a:

\begin{figure}

{\centering \includegraphics{images/Mod2_pt1_6.png}

}

\end{figure}

• The data:

\begin{figure}

{\centering \includegraphics{images/Mod2_pt1_7.png}

}

\end{figure}

• Telling jamovi to fit the model:

\begin{figure}

{\centering \includegraphics[width=4.16667in,height=\textheight]{images/Mod2_pt1_8.png}

}

\end{figure}

• The model, before being fit to the data:

\[
Recorded_i = \beta_0 + \beta_1Imbalance_i + \beta_2Mode_i + \beta_3First_i + \beta_4Last_i + \epsilon_i
\]

• The results in jamovi:

\begin{figure}

{\centering \includegraphics{images/Mod2_pt1_9.png}

}

\end{figure}

\begin{figure}

{\centering \includegraphics[width=4.16667in,height=\textheight]{images/Mod2_pt1_10.png}

}

\end{figure}

• Note that the estimated slope is being denoted as \(b\) rather than as
\(\hat{\beta}_1\). \(\hat{\sigma} = MSE\). Note also how the 95\% CI was
created: \(\approx 4.62 ± 2 ∗ 0.63\)

• The paper does not mention \(R^2\). We can see it in the jamovi
output, and calculate it from the ANOVA table:

\begin{figure}

{\centering \includegraphics[width=4.16667in,height=\textheight]{images/Mod2_pt1_11.png}

}

\end{figure}

• \(R^2 = 0.12\). So, about 12\% of the total variance in ``Recorded''
is being ``explained'' or ``accounted for'' in the model.

\emph{INSERT IMAGE}

• In multiple regression, each predictor is interpreted under the
assumption that the values of all other predictors are held constant
(i.e.~``controlled for'').

• So, if we were to observe two participants who were equal in terms of
``First'', ``Last'', and ``Mode'', but were one unit apart in terms of
``Imbalance'', we would expect their values for the response variable
(``Recorded'') to differ by 4.62 units on average.

• Or: The predicted (or average) difference in Recorded associated with
a one unit difference in Imbalance is 4.62 units, if all other
predictors are held constant.

\hypertarget{interaction}{%
\section{Interaction}\label{interaction}}

• ``Interaction'' is the phenomenon by which the association between a
predictor variable and a response variable is itself dependent on the
value of another predictor.

• Say we have response 𝑦, one predictor 𝑥1, and another predictor 𝑥2. We
say that 𝑥1 and 𝑥2 interact if the amount of change in 𝑦 associated with
a change in 𝑥1 is different for different values of 𝑥2, or vice versa.

• You can think of this as saying that the ``slope'' of 𝑥1 depends upon
the value of 𝑥2.

• Another way of saying it: if the answer to the question:

``How much does our estimate for 𝑦 change when 𝑥1 changes?''

is:

``It depends on the value of 𝑥2'',

then 𝑥1 and 𝑥2 interact.

\hypertarget{interaction-example}{%
\subsection{Interaction example}\label{interaction-example}}

• Suppose a drug for treating rheumatoid arthritis is more effective at
reducing inflammation for younger patients than it is for older
patients.

• If we conduct an experiment in which inflammation is the response
variable and the predictors are treatment group (drug vs.~control) and
age, then we expect treatment group and age to interact.

• This is different from saying that age and treatment both affect
inflammation. It means that the extent to which treatment affects
inflammation is different for patients of different age.

• Sometimes interaction is referred to as ``moderation''. This is common
in the social and behavioral sciences, particularly Psychology.

• So, a ``moderator'' variable is one that changes how the primary
predictor of interest relates to the response.

• Example: suppose an experiment shows that subjects holding a pen with
their teeth rate cartoons as funnier vs.~subjects holding a pen with
their lips.

• Suppose also that this ``pen in teeth'' effect disappears if subjects
see a video camera in the room.

• In this case, the presence of the video camera ``moderates'' the
effect of the pen on cartoon ratings. In the language of interaction,
the presence of the pen and the presence of the video camera
``interact''.

This is based on a real study that has generated controversy, see:
\href{https://statmodeling.stat.columbia.edu/2018/11/01/facial-feedback-findings-suggest-minute-differences-}{}

\hypertarget{interaction-in-the-regression-model}{%
\subsection{Interaction, in the regression
model}\label{interaction-in-the-regression-model}}

• Mathematically, we create an interaction variable by multiplying
predictor variables by one another.

• So, if we want to allow 𝑥1 and 𝑥2 to interact, we simply make a new
variable defined as 𝑥1 ∗ 𝑥2.

• This interaction variable will be used as an additional predictor
variable in the regression model:

𝑌𝑖= 𝛽0 + 𝛽1𝑥1𝑖 + 𝛽2𝑥2𝑖 + 𝛽3 𝑥1𝑥2 𝑖 + 𝜀𝑖, 𝜀𝑖\textasciitilde 𝑁𝑜𝑟𝑚𝑎𝑙(0, 𝜎)

• The interaction coefficient is 𝛽3 in this model:

𝑌𝑖 = 𝛽0 + 𝛽1𝑥1𝑖 + 𝛽2𝑥2𝑖 + 𝛽3 𝑥1𝑥2 𝑖 + 𝜀𝑖

𝜀𝑖\textasciitilde 𝑁𝑜𝑟𝑚𝑎𝑙(0, 𝜎)

• To interpret this, let's look at how it affects the coefficients (aka
slopes) for 𝑥1 and 𝑥2.

• We can think of the ``slope'' of a predictor as everything it is being
multiplied by.

𝑌𝑖 = 𝛽0 + 𝛽1𝑥1𝑖 + 𝛽2𝑥2𝑖 + 𝛽3 𝑖 + 𝜀𝑖

• Factoring out 𝑥1 from the above regression equation gives:

𝑌𝑖 = 𝛽0 + (𝛽1+𝛽3𝑥2𝑖)𝑥1𝑖 + 𝛽2𝑥2𝑖 + 𝜀𝑖

• Similarly, factoring out 𝑥2 gives:

𝑌𝑖 = 𝛽0 + (𝛽2+𝛽3𝑥1𝑖)𝑥2𝑖 + 𝛽1𝑥1𝑖 + 𝜀𝑖

• So, for this model, the ``slope'' of 𝑥1 is 𝛽1 + 𝛽3𝑥2, and the
``slope'' of 𝑥2 is 𝛽2 + 𝛽3𝑥1

• In other words, the slope of 𝑥1 depends on the value of 𝑥2, and vice
versa. For different values of 𝑥2, the ``predicted change in 𝑦 for a one
unit increase in 𝑥1'' (i.e.~the slope of 𝑥1) will be different.

• A simpler way of saying this is that, if two predictors interact, then
the effect of one predictor on the response depends on the value of the
other predictor.

(This is a simpler interpretation, but also potentially misleading in
that the term ``effect'' sounds causal. Nonetheless it is commonly used
language when interpreting slopes.)

\hypertarget{jamovi-example-arthritis-data}{%
\section{jamovi Example: Arthritis
data}\label{jamovi-example-arthritis-data}}

• The data set ``arthritis\_data.csv'' contains simulated data from a
(fictional) Randomized Control Trial comparing treatments for
inflammation from rheumatoid arthritis: a disease-modifying
anti-rheumatic drug (DMARD) vs.~a non-steroidal anti-inflammatory drug
(NSAID)

• The variables are:

• Drug: indicator (0 / 1) variable; 1 = DMARD, 0 = NSAID • Before:
Inflammation scan score prior to treatment (scale: 0 to 50) • After:
Inflammation scan score six months after treatment • Difference:
Difference in scores, before minus after. (Note that larger values

Before vs.~after scatterplot

• We will fit some regression models using this data, but first let's
take a look at our data using the Scatterplot module.

• Here is ``after'' vs.~``before'', with a linear regression line
superimposed. Note that most patients had greater inflammation before
than after treatment.

Differences by treatment type

• Note that the differences tend to be larger for the DMARD group: this
corresponds to a greater reduction in inflammation.

• Use Descriptives to create the boxplot. Select difference as a
Variable and Split by drug. Then select under Plots -- Box plot and Data
Jittered to superimpose data.

Before vs.~age scatterplot

• Here we see that age is positively correlated with inflammation before
the drug trial.

• jamovi gives options to superimpose regression output on a
scatterplot.

• jamovi can also plot ``standard error'' bands around the line. These
show the standard error for the average value of y (``before''), given x
(``age'').

Difference vs.~before

We don't see an association between amount of inflammation before
treatment and reduction in inflammation\ldots{}

\ldots{} but maybe we do if we add in drug! Do ``drug'' and ``before''
interact here?

Difference vs.~age

We see a small negative correlation between difference and age\ldots{}

\ldots{} but when we add drug, we see no correlation for the NSAID and
clear negative correlation for the DMARD. Definite interaction!

Now with model output\ldots{}

• The following slides show the plots again, plus the regression output
JMP produces. First up, Difference vs.~Drug, using t-test:

• And using regression:

OMG! The estimated slope for ``drug'' is the same as the difference in
means between the drugs!

Before vs.~age

Difference vs.~age, with interaction

• Here, we are fitting the model:

𝐷𝑖𝑓𝑓𝑒𝑟𝑒𝑛𝑐𝑒𝑖 = 𝛽0 + 𝛽1𝑎𝑔𝑒𝑖 + 𝛽2𝑑𝑟𝑢𝑔𝑖 + 𝛽3 𝑖 + 𝜀𝑖

• To do this, create a new column in jamovi, defined as age\emph{drug.
May as well label it ``age}drug''.

Difference vs.~age, with drug interaction

• Notice that the slope of ``age'' by itself is for when drug = 0. The
age*treatment interaction shows how much the slope of ``age'' changes
when treatment = 1.

𝑑𝑖𝑓𝑓෣𝑒𝑟𝑒𝑛𝑐𝑒 = 2.0114 + 0.0125 + 15.1586 𝑑𝑟𝑢𝑔 −0.2493(𝑎𝑔𝑒 ∗ 𝑑𝑟𝑢𝑔)

• Slope of ``age'' when drug = 1 is: 0.0125 − 0.2493 = −0.2368

Difference vs.~age, with drug interaction

• Notice also that the slope of ``age'' by itself is nowhere close to
being statistically significant (the estimate is less than half the
standard error), but the slope of the interaction is highly significant
(the estimate is 6 times as large as the standard error)

• Now take a look at the plot. There is a clear negative correlation
between age and difference when drug = 1. There is essentially none when
drug = 0.

Difference vs.~age, with drug interaction

• Another way of thinking about this interaction:

• If we ask the question: ``how does inflammation reduction differ by
age?'', the answer is ``it depends on which drug the patient took.''

• Similarly, if we ask the question: ``how does inflammation reduction
differ by drug?'', the answer is ``it depends on the

• In this example, we see a clear interaction between drug and age: the
slope for age is negative for DMARD and flat for NSAID.

• Another way of thinking about this: DMARD appears to be more effective
for younger patients than for older patients. NSAID appears to be
equally effective regardless of age.

• HOWEVER -- this does NOT mean that treatment and age are correlated!

• This should make sense, after all patients were randomly assigned to
one of the two drugs. If drug were correlated with age, there would be
bias in this study. The whole point of randomization is to remove
correlations!

• Just to confirm, here is the distribution of age, split by drug:

• Again, age and drug ``interact'' when it comes to their associations
with the differences in inflammation scores: the association between
``age'' and ``difference'' is different for the two different drugs.

• Similarly, the association between ``drug'' and ``difference'' is
different for patients of different ages.

\hypertarget{centering-predictor-variables}{%
\section{Centering predictor
variables}\label{centering-predictor-variables}}

𝑑𝑖𝑓𝑓෣𝑒𝑟𝑒𝑛𝑐𝑒 = 2.0114 + 0.0125

\begin{itemize}
\tightlist
\item
  15.1586
\end{itemize}

− 0.2493(𝑎𝑔𝑒 ∗ 𝑑𝑟𝑢𝑔)

• There is a serious challenge when interpreting the slope for ``drug'':
this slope is only 15.1586 if 𝑎𝑔𝑒 = 0. But 𝑎𝑔𝑒 = 0 is not of interest.

• Plug in mean age (52.096) and see what happens to the slope for drug:

15.1586 − 0.2493 = 15.1586 − 0.2493 ∗ 52.096 = 2.171(𝑑𝑟𝑢𝑔) • So, for
patients at mean age, the predicted difference in inflammation is 2.171
units greater under the DMARD than under the NSAID

• This process of estimating slopes at the average value of predictors
can be done via ``centering''

• Centering means subtracting the mean from a variable's distribution.

• In this example, 𝑐𝑒𝑛𝑡𝑒𝑟𝑒𝑑 𝑎𝑔𝑒 = 𝑎𝑔𝑒 − 𝑎𝑔𝑒 = 𝑎𝑔𝑒 -- 52.096

• This is very useful when using interaction terms in a regression
model.

• To center age, create a new columns called ``centered age'', defined
as age -- VMEAN(age).

• Create another new column for ``centered drug'', defined as drug --
VMEAN(drug). Now create a final column defined as centered drug *
centered age, call it ``centered age*drug''

• We can't use MEAN() to center variables in jamovi since it works
across variables, one row at a time. What we want is to take the overall
mean of a variable and subtract it from each measurement.

• So, use VMEAN() to center variables in jamovi.

• Compare the new results (on the left) to the results we saw when using
the interaction term we created manually (on the right)

\hypertarget{centered-interaction-in-jamovi}{%
\subsection{Centered interaction in
jamovi}\label{centered-interaction-in-jamovi}}

• The ``Model Fit Measures'' are identical. Centering has no effect on
𝑅2 or any sums of squares.

• Centering also had no effect on the slope for the interaction, or on
its standard error. But, look at the individual ``age'' and ``drug''
predictors.

• These slopes are different, because they are being calculated at the
mean value of the other.

• Centering also reduces the standard

• Look at the slope for drug when using a centered interaction: it's the
same value we calculated by plugging the mean of age into the
interaction term in the non-centered model!

• The slope for age in the centered model is harder to interpret. It is
calculated at the ``average'' for drug, which doesn't make real world
sense.

Only centering one predictor in jamovi

• It would be best if we could center ``age'' but not drug.

• But we already created centered age when we created the centered
interaction.

• Now we need to create the new interaction where only age is centered.
Create a new column and define it as ``drug * centered age''.

Only centering one predictor in JMP

• Here are the results. Compare them to the previous two versions:

• Only age centered:

• Age and drug centered in the interaction:

• Nothing centered:

• First, the slope for the interaction is the same in all three models.

• Second, the slope for drug is the same in both centered models, but
different in the non-centered model. It is the centering of age in the
interaction that changed the slope for drug.

• Third, the slope for age is the same in both models for which drug is
not centered. This allows us to interpret it as before: slope for age is
0.0125 for the NSAID and is 0.0125 − 0.2493 = −0.2368 for the DMARD

• Fourth, the standard error for drug is substantially smaller when age
is centered in the interaction. This is typically the case when
centering a continuous variable in an interaction.

• Finally, the intercept is different in all three models.

• Normally we don't care about the intercept, but centering allows the
intercept to be meaningfully interpreted.

• Since

𝐶𝑒𝑛𝑡𝑒𝑟

𝑎𝑔𝑒 = 𝑎𝑔𝑒 -- 𝑎𝑔𝑒, {[}𝐶𝑒𝑛𝑡𝑒𝑟{]}𝑎𝑔𝑒 = 0 when 𝑎𝑔𝑒 = 𝑎𝑔𝑒

• Remember that the intercept is interpreted as the ``predicted value of
the response when the predictors equal zero''.

• So, when centering, the intercept is the predicted value of the
response when the centered predictors equal their mean.

• Going back to our example, the predicted reduction in inflammation
(before minus after) for a patient at the average age in our data set
who got the NSAID is 2.665.

• The predicted reduction in inflammation for a patient at the average
age who got the DMARD is 2.665 + 2.171 = 4.836

\hypertarget{standardizing-predictor-variables}{%
\section{Standardizing predictor
variables}\label{standardizing-predictor-variables}}

• We will briefly consider an extension on centering: standardizing.

• Recall from your introductory statistics course the standardization
``z'' formula:

𝑧 = 𝑥−𝜇, or in words: 𝑧 = 𝑣𝑎𝑙𝑢𝑒 −𝑚𝑒𝑎𝑛 𝜎 𝑠𝑡𝑎𝑛𝑑𝑎𝑟𝑑 𝑑𝑒𝑣𝑖𝑎𝑡𝑖𝑜𝑛

• To center a variable, we subtract the mean. To standardize, we
subtract the mean and then divide by the standard deviation.

Why standardize?

• Just like with a centered variable, the mean of a standardized
variable is zero. So, standardizing has all the same benefits as
centering when it comes to interpretation of interactions.

• Standardizing has an additional potential benefit: the slope can be
interpreted as the predicted change in Y for a one standard deviation
increase in X (while holding all other predictors constant).

• Z values are interpreted as ``number of standard deviations from the
mean''. And so increasing Z by 1 implies increasing X by 1 standard
deviation.

• To standardize in jamovi we will need to create a new column defined
by (age -- VMEAN(age)) / VSTDEV(age). • We will also need to create a
column for the new interaction and define it as drug * Standardized age

• Here are the results when age is standardized:

• Compare to the results when age is centered:

• Note that the intercepts are the same. In both cases, the predicted
reduction in inflammation for a patient at average age getting NSAID is
2.665. Likewise, in both cases this prediction is 4.836 for DMARD.

• What has changed is the slope for terms involving age. Now, a one unit
increase in standardized age is a one standard deviation increase in
age.

• So, for NSAID, the predicted difference in inflammation reduction for
two people whose ages are one standard deviation apart is 0.14. For
DMARD, it is 0.14 -- 2.79 = -2.64.

• How much is a standard deviation? We can look it up\ldots{}

Interaction and centering / standardizing summary

• This has been a long example. I encourage you to load up the data
yourself and play around with it in jamovi. At the minimum, make sure
you can re-create the results in these notes.

• The most important take-aways:

• Interaction terms allow the slope of predictor to change for different
values of another predictor.

• Centering helps make regression coefficients (slopes and intercepts)
more interpretable. Standardizing allows you to interpret them in terms
of one standard deviation changes, rather than ``one unit'' changes.

\bookmarksetup{startatroot}

\hypertarget{chapter-3-assessing-and-improving-model-fit}{%
\chapter{Chapter 3: Assessing and improving model
fit}\label{chapter-3-assessing-and-improving-model-fit}}

\hypertarget{part-1-assumptions-and-assumption-violations}{%
\section{Part 1: assumptions and assumption
violations}\label{part-1-assumptions-and-assumption-violations}}

\hypertarget{outline-of-notes-1}{%
\section{Outline of notes:}\label{outline-of-notes-1}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Regression assumptions
\item
  Linearity
\item
  Normality of residuals
\item
  Homogeneity of variance
\item
  Influential observations
\item
  (Multi)collinearity
\end{enumerate}

\hypertarget{violating-model-assumptions}{%
\subsection{Violating model
assumptions}\label{violating-model-assumptions}}

• The previous notes outlined the fundamentals of specifying, fitting,
and interpreting a linear regression model.

• These notes focus on the assumptions that our models are based upon,
and how we check to see if they are being violated.

• If model assumptions are violated, DON'T PANIC! There is nearly always
a remedy. In these notes we will look at how to spot violations of
assumptions. In the next set of notes we will look at ways to remedy
these violations, and how to decide if they pose a serious problem to
the usefulness of the model.

\hypertarget{the-regression-model-and-what-it-assumes}{%
\subsection{The regression model and what it
assumes}\label{the-regression-model-and-what-it-assumes}}

• Once again, here is the regression model:

\[
Y_i = \beta_0 + \beta_1x_{1i} + \beta_2x_{2i} + \dots + \beta_px_{pi} + \epsilon_i, \text{ where } \epsilon_i \sim Normal(0,\sigma)
\]

\begin{itemize}
\item
  This assumes:

  \begin{itemize}
  \item
    That the response variable is a linear (straight line) function of
    the predictor variables
  \item
    That the residuals will be normally distributed
  \item
    That the standard deviation of the residuals does not vary
  \item
    That the residuals are independent
  \end{itemize}
\end{itemize}

\hypertarget{linearity}{%
\subsection{Linearity}\label{linearity}}

• Remember the ``simple'' (i.e.~single predictor) regression model

\[
Y_i = \beta_0 + \beta_1x_{i} + \epsilon_i, \text{ where } \epsilon_i \sim Normal(0,\sigma)
\]

• This is linear in that it fits a straight line to the two-dimensional
data.

• A two-predictor model would fit a flat plane to the three-dimensional
data, and so on

• Here's a bad idea: fitting a linear model to non-linear data!

\begin{figure}

{\centering \includegraphics[width=4.16667in,height=\textheight]{images/mod4_pt1_1.png}

}

\end{figure}

\hypertarget{diagnosing-non-linearity}{%
\subsection{Diagnosing non-linearity}\label{diagnosing-non-linearity}}

• When running ``Linear Regression'' in jamovi, a ``residuals by
predicted'' plot can be created by selecting ``Residual plots'' under
``Assumption Checks''

• The residuals are the differences between each observed values of the
response variable and the value that the model predicts:

\[
residual_i = y_i - \hat{y}_i = y_i - (\hat{\beta}_0 + \hat{\beta}_1x_1 + \hat\beta_2x_2 + \dots)
\]

\begin{figure}

{\centering \includegraphics[width=3.98958in,height=\textheight]{images/mod4_pt1_2.png}

}

\end{figure}

• For simple regression, this plot just looks like the regression plot
with the line turned horizontally.

• For multiple regression, there is no (two dimensional) ``regression
plot'', so the residual plot will be very useful!

\begin{figure}

{\centering \includegraphics{images/mod4_pt1_3.png}

}

\end{figure}

• In this example, there is clear curvature in the data. A straight line
model is not appropriate.

• Here's an example of what a linear relationship might look like:

\begin{figure}

{\centering \includegraphics{images/mod4_pt1_4.png}

}

\end{figure}

• When there is non-linearity, you will see the residuals mostly on one
side of zero, then on the other size of zero, then back again, and so
on.

• When there is linearity, the residuals should randomly fall on either
side of zero.

\begin{figure}

{\centering \includegraphics{images/mod4_pt1_5.png}

}

\end{figure}

\hypertarget{what-to-look-for-in-a-residual-plot}{%
\subsection{What to look for in a residual
plot}\label{what-to-look-for-in-a-residual-plot}}

• We will look at many more examples of residual plots in these notes.

\begin{itemize}
\item
  We want a residual plot that appears to agree with the model
  assumptions:

  \begin{itemize}
  \item
    Straight line relationship between the predictors and response
  \item
    Normally distributed random residuals around this line
  \item
    Equal variance in residuals across line
  \end{itemize}
\end{itemize}

\hypertarget{normality-of-residuals}{%
\subsection{Normality of residuals}\label{normality-of-residuals}}

• The ``error term'' in a regression model is that \(+ \epsilon_i\) on
the end

• When we write \(\epsilon_i \sim Normal(0, \sigma)\), we are saying
that the errors (aka residuals) are normally distributed, with mean zero
and some standard deviation \(\sigma\).

• This can be assessed visually: run the regression plot the residuals
to see if they appear roughly normally distributed.

\hypertarget{the-qq-plot}{%
\subsubsection{The QQ plot}\label{the-qq-plot}}

• When fitting a model using ``Linear Regression'' in jamovi, there is
an option to save residuals. This will create a new column with a
residual for each row.

• The option is under the last drop-down menu, under ``Save''.

\begin{figure}

{\centering \includegraphics[width=2.89583in,height=\textheight]{images/mod4_pt1_6.png}

}

\end{figure}

• To create a plot of the residuals, select ``Q-Q plot of residuals''
under ``Assumption Checks'' in ``Linear Regression''

• The Normal Quantile plot is also known as the QQ plot, for ``quantile
quantile''.

• It is easier to assess normality with a QQ plot than with a histogram.

\begin{figure}

{\centering \includegraphics[width=2.85417in,height=\textheight]{images/mod4_pt1_7.png}

}

\end{figure}

\hypertarget{assessing-normality-with-a-qq-plot}{%
\subsubsection{Assessing normality with a QQ
plot}\label{assessing-normality-with-a-qq-plot}}

• On Canvas under Simulations there is a ``QQ plot generator'' app.

• This app allows you to manipulate a distribution, sample from it, and
then view a histogram next to a QQ plot. It is meant to give you an idea
of how QQ plots work.

• By default, it draws data from a normal distribution. But, you can add
``skewedness'' or ``peakedness'' (aka kurtosis) to make the distribution
non-normal. You can also adjust sample size.

\includegraphics{images/mod4_pt1_8.png}

• A QQ plot shows you how much the distribution of your data ``agree''
with a normal distribution.

\begin{figure}

{\centering \includegraphics[width=3.36458in,height=\textheight]{images/mod4_pt1_9.png}

}

\end{figure}

• The horizonal axis gives the distribution data would follow if it were
perfectly normal.

• The vertical axis gives the distribution your data actually follows.

• The diagonal line shows perfect agreement between the two.

\begin{figure}

\includegraphics[width=3.25in,height=\textheight]{images/mod4_pt1_10.png} \hfill{}

\end{figure}

\begin{figure}

\hfill{} \includegraphics[width=3.70833in,height=\textheight]{images/mod4_pt1_11.png}

\end{figure}

• The big advantage of the QQ plot vs.~the histogram is that very often
data that come from a normal distribution don't look normal, especially
if the sample size is small.

• In this case, the histogram isn't clearly normal. But, on the QQ plot
the data are close to the line.

\begin{figure}

{\centering \includegraphics{images/mod4_pt1_12.png}

}

\end{figure}

\begin{figure}

{\centering \includegraphics{images/mod4_pt1_13.png}

}

\end{figure}

\begin{figure}

{\centering \includegraphics{images/mod4_pt1_14.png}

}

\end{figure}

• Notice that the data still veer from the diagonal line to some extent,
even though we know for a fact they came from a normal distribution.

\hypertarget{limitations-of-qq-plots}{%
\subsubsection{Limitations of QQ plots}\label{limitations-of-qq-plots}}

• As you can see from the app, sometimes data that come from a normal
distribution don't sit right on the line.

• Sometimes data that come from a skewed distribution look similar to
data that come from a normal distribution

• It's easier to assess normality when sample sizes are larger.

• As it turns out, the assumption of normality is not vital to the
validity of a regression model. If the QQ plot is vague, you're probably
fine. We only worry when we see extreme non-normality.

\hypertarget{tests-for-normality-not-recommended}{%
\subsubsection{Tests for normality (not
recommended)}\label{tests-for-normality-not-recommended}}

• There are statistical tests, such as ``Shapiro-Wilks'' or
``Kolmgorov-Smirnov'', for which the null hypothesis is that the data
come from some specified distribution, like the normal.

• Rejecting this null means that the data ``significantly'' disagree
with the assumption of normality.

• I do not recommend using this test. The problem is that, when the
sample size is large enough, even small deviations from normality will
be statistically significant. But small deviations from normality are
OK. Only major deviations are concerning.

\hypertarget{the-homogeneity-of-variance-assumption}{%
\subsection{The homogeneity of variance
assumption}\label{the-homogeneity-of-variance-assumption}}

• Back to the error term:

\(\epsilon_i \sim Normal(0, \sigma)\)

• Notice that \(\sigma\) is just one number. This suggests that the
standard deviation of the residuals should be the same across all values
of predictor variables. In other words, there is homogeneity of
variance.

• Another name for this is ``homoscedasticity''. If this assumption is
violated, then we have ``heteroscedasticity''.

\hypertarget{heterogeneity-of-variance}{%
\subsubsection{Heterogeneity of
variance}\label{heterogeneity-of-variance}}

• To show heterogeneity of variance, I'll simulate data that is a
function of an X variable, plus random values from a normal distribution
with standard deviation equal to X.

• Thus, the standard deviation of residuals will get bigger as X gets
bigger:

\begin{figure}

{\centering \includegraphics[width=2.66667in,height=\textheight]{images/mod4_pt1_15.png}

}

\end{figure}

\hypertarget{the-residuals-vs-fitted-plot}{%
\subsubsection{The residuals vs fitted
plot}\label{the-residuals-vs-fitted-plot}}

• Here is the regression plot and residual plot when this simulated
variable (called ``W'' here) is the response and X is the predictor:

\includegraphics{images/mod4_pt1_16.png}

• Notice that the residuals are more spread out for larger X

• We also see ``heavy tails'' when plotting the residuals with a
histogram and QQ plot:

\begin{figure}

{\centering \includegraphics[width=3.45833in,height=\textheight]{images/mod4_pt1_17.png}

}

\end{figure}

• Heavy tails refers to a distribution with outliers on both ends.

• This shows up on the QQ plot as the residuals being too flat in the
middle and then curving out on both ends.

\hypertarget{influential-observations-outliers}{%
\subsection{Influential observations
(outliers)}\label{influential-observations-outliers}}

• Outliers in regression can be seen on a residual plot, or on a QQ
plot, or on just a regular plot of the data.

• Example: the Florida election data.

\begin{figure}

{\centering \includegraphics[width=4.28125in,height=\textheight]{images/mod4_pt1_18.png}

}

\end{figure}

• Outliers can ``pull'' on the regression line, especially if they are
far away from the mean of the predictor(s).

• There are many statistics that assess influence. jamovi will calculate
one of the most popular: a Cook's Distance

\hypertarget{cooks-distances}{%
\subsection{Cook's Distances}\label{cooks-distances}}

• As we saw in the Florida election example, removing the outlier (Palm
Beach County) had a substantial effect on the regression results.

• The logic behind Cook's Distance is to quantify what happens to the
regression model when a single observation is removed. This is sometimes
referred to as a ``leave one out'' method.

• Cook's Distances quantify how much the predicted values of the
response variable change when an observation is removed.

• Recall that, in simple regression, the predicted values are the values
on the regression line.

• It is hard to interpret the actual values for Cook's Distances. Values
greater than 1 are often considered ``influential''.

• The formula shows that it is based on the sum of the differences in
predicted values between a model with the data point included and a
model with it removed:

\[
\text{Cook's Distance for data point } "i" = D_i = \frac{\sum^n_{j=1}(\hat{y}_i - \hat{y}_{j(i)})^2}{MSE * p}
\]

• Where \(\hat{y}_{j(i)}\) is the predicted value of the response
variable when the model is re- fit with the \(i^{th}\) data point
removed, and \(p\) is the number of predictor variables in

• A Cook's Distance is calculated for every data point. The option to do
this in jamovi is under ``Save'' in ``Linear Regression''. This creates
a new column with a Cook's distance for each row.

• The saved Cook's Distances can then be plotted. Any possibly
influential points will usually stand out clearly from the rest.

\begin{figure}

{\centering \includegraphics[width=4.09375in,height=\textheight]{images/mod4_pt1_19.png}

}

\end{figure}

• To quickly narrow in on the influential counties, we can filter out
all the small Cook's distances.

• After implementing the filter, we can see that all rows which do not
meet the criteria of the filter are excluded.

\begin{figure}

{\centering \includegraphics[width=3.78125in,height=\textheight]{images/mod4_pt1_20.png}

}

\end{figure}

• Only rows 13 and 50 should be highlighted for Dade and Palmbeach
county which have cook's distances of 1.983 and 3.786.

\hypertarget{multicollinearity}{%
\subsection{(Multi)collinearity}\label{multicollinearity}}

• In regression analysis, we want our predictor variables to be
correlated with the response variable.

• But we don't want our predictor variables to be (highly) correlated
with one another!

• When two predictor variables are highly correlated, we say our model
has ``collinearity.''

• When more than two predictor variables are mutually highly correlated,
we say our model as ``multicollinearity''.

\hypertarget{why-dont-we-want-correlated-predictors}{%
\subsection{Why don't we want correlated
predictors?}\label{why-dont-we-want-correlated-predictors}}

• To understand why we don't want correlated predictors, recall that
multiple regression models estimate the association between each
individual predictor variable and the response,
\(\textit{while holding all other predictor variables constant}\).

• This can be thought of as asking ``what is the difference in the
response variable when we observe data points that have different values
of one predictor but the same values of the other predictors?''

• If \(Y\) is the response and \(x_1\) and \(x_2\) are predictors, we
want to know how different \(Y\) is when \(x_1\) values differ but
\(x_2\) values are the same, or vice versa.

• But, if \(x_1\) and \(x_2\) are highly correlated, then we don't get
to observe cases where values of one variable differ substantially while
values of the other are the same! Consider instances of no correlation
vs.~heavy correlation:

\begin{figure}

{\centering \includegraphics{images/mod4_pt1_21.png}

}

\end{figure}

• When \(x_1\) and \(x_2\) are uncorrelated, we see lots of instances of
\(x_1\) values differing a lot when \(x_2\) values are equal.

\begin{figure}

{\centering \includegraphics[width=3.84375in,height=\textheight]{images/mod4_pt1_22.0.png}

}

\end{figure}

• When \(x_1\) and \(x_2\) are highly correlated, we never see instances
where \(x_2\) values are equal but \(x_1\) values are highly correlated.

\begin{figure}

{\centering \includegraphics[width=3.51042in,height=\textheight]{images/mod4_pt1_22.5.png}

}

\end{figure}

• The upshot is that, when \(x_1\) and \(x_2\) are highly correlated,
the regression procedure has a difficult time distinguishing between the
``effect'' of \(x_1\) on \(Y\) and the ``effect'' of \(x_2\) on \(Y\).

• Extreme example: if we had degrees Celsius and degrees Fahrenheit as
predictors in the same model, it would be impossible to tell their
effects apart. You can't change Celsius while holding Fahrenheit
constant!

• The practical consequence is that the standard errors for the slopes
of (multi)collinear predictors are much larger than they would be if it
were not for the (multi)collinearity.

• If two or more predictors are perfectly correlated (\(r=1\)), then the
model cannot be fit and jamovi produces an error:

\begin{figure}

{\centering \includegraphics[width=3.27083in,height=\textheight]{images/mod4_pt1_23.png}

}

\end{figure}

• Here, \(X1\) and \(X2\) are perfectly correlated. jamovi cannot
estimate a slope for \(X2\).

\hypertarget{collinearity-example-florida-election-data}{%
\subsection{Collinearity example: Florida election
data}\label{collinearity-example-florida-election-data}}

• In the Florida election data, we used total votes for each county as
our predictor variable.

• There is another variable called ``Total\_Reg''. This is the total
number of registered voters in each county.

• Unsurprisingly, Total\_Votes and Total\_Reg are highly correlated:

\begin{figure}

{\centering \includegraphics[width=3.69792in,height=\textheight]{images/mod4_pt1_24.png}

}

\end{figure}

• If we run two separate simple regression models, we get very similar
results:

\[
Buchanin_i = \beta_0 + \beta_1Total\_Votes_i + \epsilon_i
\]

\begin{figure}

\includegraphics[width=3.41667in,height=\textheight]{images/mod4_pt1_25.png} \hfill{}

\end{figure}

\[
Buchanin_i = \beta_0 + \beta_1Total\_Reg_i + \epsilon_i
\]

\begin{figure}

\includegraphics[width=3.28125in,height=\textheight]{images/mod4_pt1_27.png} \hfill{}

\end{figure}

• But look what happens if we use Total\_Votes and Total\_Reg as
predictors in the same model:

\[
Buchanin_i = \beta_0 + \beta_1Total\_Votes_i + \beta_2Total\_Reg_i + \epsilon_i
\]

\begin{figure}

\includegraphics[width=3.25in,height=\textheight]{images/mod4_pt1_27-01.png} \hfill{}

\end{figure}

• Two important things to note:

• P-values on slopes are much larger than for the individual models

• \(R^2\) is larger than on either individual model!

• Looking at the estimates and standard errors in all three models, we
see that the standard errors are much larger in the multiple regression
model. These estimates are ``unstable'' -- their values will change a
lot if the data change a little.

\begin{figure}

\includegraphics[width=3.55208in,height=\textheight]{images/mod4_pt1_28.png} \hfill{}

\end{figure}

• We know the association between Total\_Reg are Buchanan is actually
positive. But with so high a standard error, the slope for Total\_Reg
turned out negative!

\hypertarget{variance-inflation-factor-vif}{%
\subsubsection{Variance inflation factor
(VIF)}\label{variance-inflation-factor-vif}}

• (Multi)collinearity can be assessed using a ``Variance Inflation
Factor'', or VIF. A VIF is calculated for the \(j^{th}\) predictor
variable as:

\[
VIF_j = \frac{1}{1-R^2_j}
\]

• Where \(R^2_j\) is the \(R^2\) from a regression model with predictor
\(j\) as the response variable and all other predictors still as
predictors.

• In the Florida election example, the VIF for Total\_Votes can be found
using the \(R^2\) for the model:

\[
Total\_Votes_i = \beta_0 + \beta_1Total\_Reg_i + \epsilon_i
\]

\begin{figure}

\includegraphics[width=2.79167in,height=\textheight]{images/mod4_pt1_29.png} \hfill{}

\end{figure}

\hfill\break
• This \(R^2\) is huge! Plugging it into the formula:

\[
VIF_{Total\_Votes}=\frac{1}{1-0.997} = 333.33
\]

• Thankfully we don't have to do this by hand. In jamovi, under ``Linear
Regression'' select ``Collinearity statistics'':

\begin{figure}

\includegraphics[width=3.4375in,height=\textheight]{images/mod4_pt1_30.png} \hfill{}

\end{figure}

• VIF \textgreater{} 10 typically is considered large (note that this
would imply \(R^2 = 0.9\) between predictor variables).

• The most obvious thing to do in the presence of (multi)collinearity is
to remove one or more correlated predictor variable. From a scientific
standpoint, you also may not want highly correlated predictors in the
same model.

\hypertarget{when-should-we-worry-about-multicollinearity}{%
\subsubsection{When should we worry about
(multi)collinearity?}\label{when-should-we-worry-about-multicollinearity}}

• (Multi)collinearity is a potentially huge problem if the goal of the
regression model is to interpret the estimated slopes.

• This is because it increases the standard error of these slopes,
making their values less reliable. Some people say it makes slopes
``unstable''.

• It may also complicate the interpretation of slopes: you are trying to
statistically ``hold constant'' a predictor variable that doesn't
naturally stay constant when the other predictor varies. This isn't
necessarily a problem, but it is something to be aware of.

• However, (multi)collinearity does not negatively impact the predicted
values themselves. Remember that it didn't hurt the \(R^2\) value in the
Florida election example. \(R^2\) tells you how good your predictions
are.

• So, if the model is only for predicting, you probably don't need to
worry about using correlated predictor variables. Just beware when
interpreting the slopes.

\hypertarget{part-2-improving-models}{%
\section{Part 2: Improving models}\label{part-2-improving-models}}

\hypertarget{summary-of-part-1}{%
\subsection{Summary of part 1}\label{summary-of-part-1}}

\begin{itemize}
\tightlist
\item
  In the last set of notes, we looked at some things that can go wrong
  in regression modeling, including:
\end{itemize}

\begin{verbatim}
-    Non-linear relationships between predictor(s) and response

-   Non-normality of residuals

-   Non-constant (heterogeneous) variance of residuals

-   Influential outliers

-   Multicollinearity
\end{verbatim}

• In these notes, we'll look at some tools available for dealing with
these problems.

\hypertarget{transforming-variables}{%
\subsection{Transforming variables}\label{transforming-variables}}

• Recall the regression model:

\[
Y_i = \beta_0 + \beta_1x_{1i} + \beta_2x_{2i} + \dots + \beta_px_{pi} + \epsilon_i, \text{ where } \epsilon_i \sim Normal(0,\sigma)
\]

• Sometimes we can correct violations of model assumptions by applying a
mathematical transformation to the response or predictor variables.

• The most common transformation in Statistics is the log
transformation:

\[
ln(x) = log_e(x)
\]

\hypertarget{log-transformation}{%
\subsection{Log transformation}\label{log-transformation}}

• \(ln(𝑥)\) is the inverse function of \(e^𝑥\), where
\(𝑒 = 2.718 \dots\)

• In other words, \(ln(e^x) = x\)

• Example: \(𝑒^3 = 20.086; ln(20.086) = 3\)

• So, the natural log of \(x\) is the number you would have to raise
\(e\) to so that you'd get \(x\).

• Note: in statistics, when we say ``log'', we usually mean ``natural
log''. It turns out that the distinction is not very important. I'll say
``log transform''

\hypertarget{why-log-transform}{%
\subsubsection{Why log transform?}\label{why-log-transform}}

\begin{itemize}
\item
  There are two main reasons for log transforming a variable:

  \begin{itemize}
  \item
    To correct for skew in data or residuals
  \item
    To interpret increases in a variable as multiplicative rather than
    additive.
  \end{itemize}
\end{itemize}

• Both can be understood by recognizing an important property of
logarithms; they ``turn addition into multiplication''

\[
log(𝐴) + log(𝐵) = log(𝐴𝐵)
\]

• In this sense, logarithms turn addition into multiplication.

• Example: suppose we have data for a skewed variable \(X_1\):

\begin{figure}

{\centering \includegraphics{images/mod4_pt2_1.png}

}

\end{figure}

• Now we define \(x_2 = ln(x_1)\):

\begin{figure}

\includegraphics[width=2.25in,height=\textheight]{images/mod4_pt2_2.png} \hfill{}

\end{figure}

• This is a toy ``data set''. I chose \(x_1\) so that \(x_2 = ln(x_1)\)
would just be the integers \(1\) through \(10\).

• Note: there is no more skew.

• Also note: increasing \(x_2\) by one unit results in multiplying
\(x_1\) by \(e\). Addition in \(x_2 = ln(x_1)\) is the same thing as
multiplication in \(x_1\).

\hypertarget{same-again-with-log-base-2}{%
\subsubsection{Same again, with log base
2}\label{same-again-with-log-base-2}}

• Even simpler: define \(x_2\) as log base \(2\) of \(x_1\),
i.e.~\(log_2(x_1)\)

\begin{figure}

\includegraphics[width=1.6875in,height=\textheight]{images/mod4_pt2_3.png} \hfill{}

\end{figure}

• Now increasing \(x_2\) by one unit is equivalent to multiplying
\(x_1\) by \(2\). Addition in \(x_2 = log_2(x_1)\) is the same thing as
multiplication in \(x_1\).

\hypertarget{log-transforming-right-skewed-data}{%
\subsubsection{Log transforming right-skewed
data}\label{log-transforming-right-skewed-data}}

\begin{itemize}
\tightlist
\item
  Skewed data can be bad for regression, in that it can lead to:
\end{itemize}

\begin{verbatim}
-   Non-linear relationship between X and Y

-   Influential outliers

-   Non-normal residuals

-   Non-constant variance in residuals
\end{verbatim}

• So a simple log transformation can sometimes go a long way toward
making the regression model fit the better!

• It is most common to log transform a response variable, because
assumptions about residuals apply to \(Y\), not \(X\).

• But if \(X\) is skewed, the model can benefit from a log
transformation of \(X\).

• Bear in mind that log transformation will affect the interpretation of
slope coefficients!

• If \(X\) is log transformed, then a one unit increase in \(ln(𝑋)\)
corresponds to multiplying \(X\) by \(e \approx 2.72\). So the slope for
\(ln(𝑋)\) tells you how much \(Y\) increases when \(X\) is multiplied by
\(2.72\). Or, even better, use log base \(2\) and the slope will give
how much \(Y\) changes when \(X\) is doubled.

• If \(Y\) is log transformed, then the interpretations of slopes get
more complicated. Here's the math, with the error term omitted for
convenience:

\[
ln(y_i) = \beta_0 + \beta_1X_i
\]

\[
\therefore y_i = e^{\beta_0 + \beta_1X_i}
\]

• Increase \(X\) by \(1 \dots\)

\[
y_i^* = e^{\beta_0 + \beta_1(X_i + 1)} = e^{\beta_0 + \beta_1X_i} \cdot e^{\beta_1}
\]

• So, when \(Y\) is log transformed, a one unit increase in \(X\)
multiplies predicted \(Y\) by \(e^{\beta_1}\)

\hypertarget{interpreting-slope-as-a-change-in-outcome}{%
\subsubsection{Interpreting slope as a \% change in
outcome}\label{interpreting-slope-as-a-change-in-outcome}}

• Recall the heights vs.~wages data from group project 1. The paper
reported this estimated model:

\[
ln(\text{wage}) = \hat{\beta}_0 + 0.002\text{(Adult Height)} + 0.027\text{(Youth Height)} + 0.024\text{(Age)}
\]

• So, when comparing two adults \(1\) inch apart in height but with the
same youth height and age predicted wage is multiplied by
\(e^{0.027} = 1.027\) for the taller adult.

• Multiplying by \(1.027\) can be thought of as increasing by \(2.7\%\)

\hypertarget{log-transformation-applied-example}{%
\subsubsection{Log transformation applied
example}\label{log-transformation-applied-example}}

• Here is the percent change formula:

\[
\%\text{ change (from A to B)} = \frac{B-A}{A}*100\%
\]

• If B is \(1.027*\)A, then

\[
\%\text{ change} = \frac{1.027*A - A}{A}*100 = \frac{0.027A}{A}*100 = 2.7\%
\]

• So, when comparing two adults 1 inch apart in height but with the same
youth height and age, predicted wage is \(2.7\%\) higher for the taller
adult.

\hypertarget{log-transformation-in-y-vs.-in-x}{%
\subsubsection{\texorpdfstring{Log transformation in \(Y\) vs.~in
\(X\)}{Log transformation in Y vs.~in X}}\label{log-transformation-in-y-vs.-in-x}}

• Remember that log transformation ``turns addition into
multiplication''. So, to keep track of how log transforming \(Y\)
vs.~log transforming \(X\) affects your model:

\[
\text{log}(Y_i) = \beta_0 + \beta_1X_i + \epsilon_i
\]

\[
\text{vs.}
\]

\[
Y_i = \beta_0 + \beta_1\text{log}(X_i) + \epsilon_i
\]

• If you log transform \(Y\) but not \(X\), your model estimates the
multiplicative change in predicted \(Y\) for an additive change in
\(X\).

• If you log transform \(X\) but not \(Y\), your model estimates the
additive change in predicted \(Y\) for a multiplicative change in \(X\).

\hypertarget{non-linearity}{%
\subsection{Non-linearity}\label{non-linearity}}

• Sometimes data show obvious curvature, in the sense that \(Y\) is
clearly not a straight line function of \(X\).

• This will be visible on a plot of \(Y\) vs.~\(X\). It will also be
visible on a residuals vs.~predicted values plot after running a
regression.

• If there is curvature in the relationship between \(Y\) and \(X\),
then it might be sensible to add a polynomial \(X\) term:

\[
Y_i = \beta_0 + \beta_1x_i + \beta_2x_i^2 + \epsilon_i
\]

\hypertarget{polynomial-review}{%
\subsubsection{``Polynomial'' review}\label{polynomial-review}}

• A ``polynomial'' expression is typically one in which variables are
included at different powers. For example, a generic third degree
polynomial equation might look like:

\[
y = a + bx + cx^2 + dx^3
\]

• A ``second degree'' polynomial is one in which an \(x\) and \(x^2\)
term are both included. This is by far the most common type of
polynomial seen in regression models.

\hypertarget{nd-degree-and-3rd-degree-polynomials}{%
\subsubsection{\texorpdfstring{\(2^{nd}\) degree and \(3^{rd}\) degree
polynomials}{2\^{}\{nd\} degree and 3\^{}\{rd\} degree polynomials}}\label{nd-degree-and-3rd-degree-polynomials}}

• \(2^{nd}\) degree polynomials are often called ``quadratic''.
\(3^{rd}\) degree polynomials are often called ``cubic''. Here are
visual examples of simulated quadratic and cubic relationships between
\(Y\) and \(X\):

\begin{figure}

{\centering \includegraphics{images/mod4_pt2_4.png}

}

\end{figure}

\hypertarget{curvature-in-residuals}{%
\subsubsection{Curvature in residuals}\label{curvature-in-residuals}}

• Here is regression output comparing a linear model to a quadratic
model when the relationship between \(Y\) and \(X\) is quadratic:

\begin{figure}

{\centering \includegraphics{images/mod4_pt2_5.png}

}

\end{figure}

\hypertarget{example-florida-election-data}{%
\subsubsection{Example: Florida election
data}\label{example-florida-election-data}}

• Here is what the Florida election data look like with the Palm Beach
County outlier removed, along with regression results for the simple
linear regression model:

\[
Buchanan_i = \beta_0 + \beta_1Total\_Votes_i + \epsilon_i
\]

\begin{figure}

{\centering \includegraphics{images/mod4_pt2_6.png}

}

\end{figure}

• Now we will fit a quadratic polynomial model to the same data:

\[
Buchanan_i = \beta_0 + \beta_1Total\_Votes_i + \beta_2Total\_Votes^2_i + \epsilon_i
\]

• To create this polynomial predictor in jamovi we first need to install
the GAMLj module. Then, select ``Generalized Linear Models'' and last
our ``Dependent Variable'' and ``Covariates''.

• Under the ``Model'' drop down menu, click on Total\_Votes in the
``Components'' table.

• An up and down arrow will appear with the degree of Total\_Votes,
click the up arrow so the 1 changes to 2. Then click the right arrow to
add Total\_Votes2 to ``Model Terms''

\begin{figure}

{\centering \includegraphics[width=4.82292in,height=\textheight]{images/mod4_pt2_7.png}

}

\end{figure}

\[
Buchanan_i = \beta_0 + \beta_1Total\_Votes_i + \beta_2Total\_Votes^2_i + \epsilon_i
\]

\begin{figure}

{\centering \includegraphics{images/mod4_pt2_8.png}

}

\end{figure}

• This is better, but we still see curvature in the residual plot.

• Let's try a cubic model:

\[
Buchanan_i = \beta_0 + \beta_1Total\_Votes_i + \beta_2Total\_Votes^2_i +\beta_3Total\_Votes_i^3 + \epsilon_i
\]

\begin{figure}

{\centering \includegraphics[width=4.34375in,height=\textheight]{images/mod4_pt2_9.png}

}

\end{figure}

\[
Buchanan_i = \beta_0 + \beta_1Total\_Votes_i + \beta_2Total\_Votes^2_i +\beta_3Total\_Votes_i^3 + \epsilon_i
\]

\begin{figure}

{\centering \includegraphics{images/mod4_pt2_10.png}

}

\end{figure}

\hypertarget{example-florida-election-data-check-note}{%
\subsubsection{Example: Florida election data: check
note}\label{example-florida-election-data-check-note}}

• It's debatable whether this is much better. For one, the
\(Total\_Votes^2\) term is non-significant.

• But think back to multicollinearity. Each polynomial term will be
correlated with the other terms -- after all, \(Total\_Votes\),
\(Total\_Votes^2\), and \(Total\_Votes^3\) must all be correlated.

• Note that jamovi will not produce VIFs in GLM. Intuitively we know
these will be highly correlated with each other.

• It turns out that centering helps in polynomial models:

\begin{figure}

{\centering \includegraphics{images/mod4_pt2_11.png}

}

\end{figure}

• By default jamovi centers polynomials which makes their standard
errors smaller than if they were not centered. But, these estimates are
not interpretable; you cannot hold \(Total\_Votes^2\) constant while
increasing \(Total\_Votes\).

• In this example, the two counties with the highest total votes are
heavily pulling on the regression line.

\begin{figure}

{\centering \includegraphics[width=3.97917in,height=\textheight]{images/mod4_pt2_12.png}

}

\end{figure}

\hypertarget{over-fitting}{%
\subsection{Over-fitting}\label{over-fitting}}

• This model might be ``over-fit''.

• Over-fitting is when a model fits the data so well that it ends up
fitting random variation that is not of interest.

\begin{figure}

{\centering \includegraphics[width=3.57292in,height=\textheight]{images/mod4_pt2_13.png}

}

\end{figure}

• Imagine if the two data points on the right had slightly higher vote
counts for Buchanan. Or if the next two to their left had slightly lower
vote counts. The curve would look very different.

• At this point, we might just be modelling noise.

• Here is an extreme example of over-fitting: fitting a ``smoother''
curve to data and giving it permission to move dramatically up and down
through the data.

\begin{figure}

{\centering \includegraphics[width=3.71875in,height=\textheight]{images/mod4_pt2_14.png}

}

\end{figure}

• This line fits the data very well, but does it represent the general
trend between total votes and votes for Buchanan? Definitely not!

• (Side note: ``smoothers'' are great tools for visualizing and
summarizing data, but they can be extremely sensitive to degree of
smoothing. We won't use them in STAT 331)

• Compare the over-fit model to the linear model.

\begin{figure}

{\centering \includegraphics[width=3.6875in,height=\textheight]{images/mod4_pt2_15.png}

}

\end{figure}

• The linear model may be missing out on some curvature. But it might
also make better predictions.

• If we were to observe a new county with \(450,000\) total votes, would
we be better guessing that votes for Buchanan fall on the highly curved
line or on the straight line?

\hypertarget{back-to-basics-is-the-model-sensible}{%
\subsection{Back to basics: is the model
sensible?}\label{back-to-basics-is-the-model-sensible}}

\begin{itemize}
\item
  Back to basics: regression models are typically used for two purposes:

  \begin{itemize}
  \item
    Predicting values of the response variable, using the predictor
    variables. This is done by plugging values for the predictor
    variables into the estimated model.
  \item
    Estimating the association between each individual predictor
    variable and the outcome while statistically holding other predictor
    variables constant. This is done by interpreted estimated slope
    coefficients.
  \end{itemize}
\end{itemize}

\hypertarget{if-you-just-want-to-make-predictions}{%
\subsubsection{If you just want to make
predictions}\label{if-you-just-want-to-make-predictions}}

• \(R^2\) is the easiest to understand statistic for assessing how well
your model makes predictions. The closer to \(1\), the better.

• Multicollinearity isn't an issue. It doesn't affect predicted values.

• BUT -- beware of overfitting! The more complex your model, the more
risk you take of modeling noise instead of signal.

• Also, be aware that \(R^2\) can never go down when adding predictors.
You can add complete nonsense predictor variables, and the worst that
will happen to \(R^2\) is that it stays the same.

\hypertarget{if-you-want-to-interpret-slopes}{%
\subsubsection{If you want to interpret
slopes}\label{if-you-want-to-interpret-slopes}}

• Always remember that each slope is interpreted under the assumption
that all other predictor variables are being held constant,
i.e.~``controlled for''.

• The more predictor variables in the model, the less sense this will
make.

• Example: wage vs.~height study:

\begin{figure}

{\centering \includegraphics[width=5.22917in,height=\textheight]{images/mod4_pt2_16.png}

}

\end{figure}

• In model 4, the estimated slope for youth height can be interpreted
as:

``The predicted difference in ln(wage) for two people one inch apart in
youth height, but equal in adult height, whose mothers have the same \#
of years of schooling and are both in either skilled fields or work or
not, whose fathers have the same \# of years of schooling and are both
in either skilled fields or work or not, and who have the same number of
siblings.''

\begin{figure}

\includegraphics[width=2.25in,height=\textheight]{images/mod4_pt2_17.png} \hfill{}

\end{figure}

• Maybe this is the best way to think about the association between
youth height and wages. But it is fairly complicated.

• If you're going to try to make ``real world'' sense out of regression
results, your model should be informed by theory.

• This is necessarily subjective! You have to choose which variables you
think are important. You have to think about what makes sense.

\begin{itemize}
\item
  This might require:

  \begin{itemize}
  \item
    Log transforming a variable solely because you like the
    multiplicative interpretation better than the additive
    interpretation.
  \item
    Keeping a variable in a model even though it isn't statistically
    significant.
  \item
    Removing a variable you are interested in, because it doesn't make
    sense to ``hold it constant'' when estimating slopes for other
    variables.
  \end{itemize}
\end{itemize}

\hypertarget{is-the-model-missing-something-important}{%
\subsubsection{Is the model missing something
important?}\label{is-the-model-missing-something-important}}

• There is another variable in the Florida election data set that could
be worth including: ``Reg\_Reform'': the total number of voters
registered with the Reform Party. Pat Buchanan was the Reform Party
candidate. Let's add it to the model:

\[
Buchanan_i = \beta_0 + \beta_1Total\_Votes_i + \beta_2Reg\_Reform_i + \epsilon_i
\]

\begin{figure}

{\centering \includegraphics{images/mod4_pt2_18.png}

}

\end{figure}

• This residual plot looks great!

\begin{figure}

{\centering \includegraphics[width=2.86458in,height=\textheight]{images/mod4_pt2_19.png}

}

\end{figure}

• It turns out that the curvature in the previous residual plots went
away when adding an important variable to the model. No need to mess
with polynomials after all!

• Also, the \(R^2\) is roughly the same as in the cubic model using only
total votes as a predictor.

• So we have roughly equal fit, without having to worry about
overfitting, and without having to give up interpretability of the
slopes.

• One downside: there is some collinearity. Look at the VIFs.

\begin{figure}

{\centering \includegraphics{images/mod4_pt2_20.png}

}

\end{figure}

• VIF of about \(5\) implies \(\frac{1}{1-R^2}\approx 5\) when using the
\(R^2\) from:

\[
Total\_Votes_i = \beta_0 + \beta_1Reg\_Reform_i + \epsilon_i
\]

• So, this \(R^2\) is about \(1 − \frac{1}{5} = 0.8\). And so
\(r = \sqrt{0.8} =0.89\) . These predictors are strongly correlated.

\includegraphics{images/mod4_pt2_21.png}

• Note also that total votes is not significant.

• But: the slope for \(Reg\_Reform\) has a nice interpretation:

When comparing two counties with the same number of votes cast in the
election, a county with an additional registered Reform Party member is
estimated to have \(2.24\) additional votes, on average, for Pat
Buchanan.

• Should total votes be taken out of the model? This is a subjective
decision.

\hypertarget{what-would-you-like-to-control-for}{%
\subsubsection{What would you like to ``control''
for?}\label{what-would-you-like-to-control-for}}

• In regression analysis, we usually emphasize (correctly) that
correlation does not imply causation.

• However, if you have knowledge or beliefs about causal direction, you
should take these into account when choosing your variables!

• Example: in the rheumatoid arthritis study, we were looking at the
effect of drugs on inflammation level. In particular, we were comparing
how effective they were at reducing inflammation. Suppose we also asked
patients to rate their mobility level (RA tends to reduce mobility).

• Our model might be:

\[
Difference_i = \beta_0 + \beta_1age_i + \beta_2drug_i + \beta_3(age*drug)_i + \beta_4mobility_i + \epsilon_i
\]

• Now, when interpreting the previous slopes, I am comparing average
reduction in inflammation (``difference'', the response variable),
between two people who have the same mobility level. But if they have
the same mobility level, then they will have more similar inflammation
levels than if we allowed mobility level to differ.

• In other words, because the drug reduces inflammation \emph{and}
improves mobility, ``controlling'' for mobility will make it look like
the drugs are less effective than they really are.

\hypertarget{beware-the-kitchen-sink-approach}{%
\subsubsection{Beware the ``kitchen sink''
approach}\label{beware-the-kitchen-sink-approach}}

• There's an old saying: ``taking everything but the kitchen sink''.

• It can be tempting to toss everything but the kitchen sink into a
regression model, especially when you have loads of variables that all
seem like they'd be associated with the response.

• But beware! Adding in one predictor can have a dramatic effect on the
slopes of other predictors, as well as on their standard errors.

• It really really \emph{really} matters that each slope is estimated as
though all other predictors are held constant. This can reveal otherwise
unseen effects, but it can also obscure otherwise obvious effects or
induce apparent effects that aren't real. There is no substitute for
scientific reasoning when choosing a model.

\hypertarget{the-model-is-simpler-than-whats-being-modeled}{%
\subsubsection{The model is simpler than what's being
modeled}\label{the-model-is-simpler-than-whats-being-modeled}}

• Let's take a step back and ask: why are we fitting data to models?

• Well, we are interested in the real world. And the real world in
incredibly complicated. Maybe incomprehensibly complicated.

• So, we simplify things using models. We hope that the model captures
the essence of what we care about in the real world. But we know it is a
simplification; perhaps an extreme simplification.

• ``All models are wrong; some are useful'' -- George Box

• Consider how the regression model describes where data comes from:

\[
Y_i = \beta_0 + \beta_1x_{i} + \epsilon_i, \text{ where } \epsilon_i \sim Normal(0,\sigma)
\]

• This says that to get data, we pick a value for X, go to a line, then
randomly draw a value from a normal distribution and add this to the
value on the line. And that's where data comes from!

• Except, that's not where data comes from. This is a model. It is a
simplification of reality. We use these models because we think they
will help us answer questions we care about (e.g.~make predictions,
identify associations between variables). Don't forget that the model is
not the thing itself.

\bookmarksetup{startatroot}

\hypertarget{chapter-4-anova-based-methods}{%
\chapter{Chapter 4: ANOVA-based
methods}\label{chapter-4-anova-based-methods}}

\hypertarget{what-is-anova}{%
\section{What is ANOVA?}\label{what-is-anova}}

• ANOVA stands for ``analysis of variance''

• ANOVA is regression with categorical predictors. That's it.

\hypertarget{anova-is-regression-presented-differently}{%
\subsection{ANOVA is regression presented
differently}\label{anova-is-regression-presented-differently}}

• OK, there's more to say about ANOVA than just ``regression with
categorical predictors.''

• ANOVA is typically used to analyze data from experiments. In
experiments, the categorical predictors are usually groups to which
experimental units (aka subjects) are assigned. • ANOVA tends to focus
on comparing means of different groups to one another. • Although ANOVA
is ``just'' regression, there are conventions for reporting ANOVA
results that are simpler and cleaner than what we've seen for
regression.

\hypertarget{indicator-variables}{%
\subsection{Indicator variables}\label{indicator-variables}}

• We've seen how to incorporate a categorical predictor into a
regression model when the predictor takes on two values. We create an
``indicator'' (aka ``dummy'' aka ``binary'') variable that takes on the
values 0 or 1. For example:

\[
Y_i=\beta_0+\beta_1x_{1i} + \epsilon_i
\]

\[
x_i = 1 \text{ if "treatment;"} x_i = 0 \text{ if "control"}
\]

• Here, \(\hat{y}_i = \hat{\beta}_0\) for control, and
\(\hat{y}_i = \hat{\beta}_0 + \hat{\beta}_1\) for treatment.

• What if our categorical predictor takes on more than two categories?
Suppose we have three groups: treatment 1, treatment 2, and control. We
can add another indicator:

\[
Y_i=\beta_0+\beta_1x_{1i} + \beta_2x_{2i} + \epsilon_i
\]

\[
x_{1i} = 1 \text{ if treatment }1; x_{1i} = 0 \text{ otherwise} \\
x_{2i} = 1 \text{ if treatment }2; x_{1i} = 0 \text{ otherwise} 
\]

• \(\hat{y}_i = \hat{\beta}_0\) for control,
\(\hat{y}_i = \hat{\beta}_0 + \hat{\beta}_1\) for treatment 1, and
\(\hat{y}_i = \hat{\beta}_0 + \hat{\beta}_1 + \hat{\beta}_2\) for
treatment 2.

• Control is serving as the ``baseline'' category, represented by the
intercept.

\hypertarget{one-way-anova}{%
\section{One-Way ANOVA}\label{one-way-anova}}

• One-way ANOVA models have a single categorical predictor variable.
They can be written as:

\[
Y_i = \mu + \alpha_j + \epsilon_{ij}, \text{ where } \epsilon_{ij} \sim Normal(0,\sigma^2) \text{ and } i = 1, \dots ,n_j, j = 1, \dots, p
\]

• Where there are ``\(p\)'' groups (i.e.~the categorical predictor takes
on ``\(p\)'' values), ``\$n\_j\$'' is the sample size of the \(j^{th}\)
group, and \(y_{ij}\) is the \(i^{th}\) observation in the \(j^{th}\)
group.

• Here, \(\mu\) is the overall mean and \(\alpha_j\) is the deviation of
the \(j^{th}\) group mean from the overall mean.

• Suppose we have 4 groups. The ANOVA model is:

\[
Y_i = \mu + \alpha_j + \epsilon_{ij}, \text{ where } \epsilon_{ij} \sim Normal(0,\sigma^2) \text{ and } i = 1, \dots ,n_j, j = 1, \dots, 4
\]

• Written as a regression model instead:

\[
Y_i = \beta_0 +\beta_1Group1 + \beta_2Group2 + \beta_3Group3 + \epsilon_{i}, \text{ where } \epsilon_{i} \sim Normal(0,\sigma^2) \text{ and } i = 1, \dots ,n
\]

• Here, the ``Group'' predictors are \(0\) / \(1\) indicator variables.
For group \(4\), the mean of \(y\) is the intercept, \(\beta_0\).

\hypertarget{factorial-anova}{%
\section{Factorial ANOVA}\label{factorial-anova}}

• If we have more than one categorical predictor variable, and
interactions between the predictors, we have a factorial ANOVA model.
Generically:

\[
Y_{ijk} = \mu + \alpha_j + \beta_k + (\alpha\beta)_{jk} + \epsilon_{ijk}, \text{ where } \epsilon_{ijk} \sim Normal(0,\sigma^2) \\
\text{ and } i = 1, \dots ,n_{jk}, j = 1, \dots, p, k = 1, \dots,q
\]

• Now predictor \(1\) is \(\alpha\) and predictor \(2\) is \(\beta\) and
they interact. There are ``\$p\$'' groups for predictor \(1\) and
``\$q\$'' groups for predictor \(2\).

• The subscripts start getting messy pretty fast.

\hypertarget{factorial-anova-example}{%
\subsection{Factorial ANOVA example}\label{factorial-anova-example}}

• Here's an example of a ``5x2 factorial'' ANOVA, meaning that one
variable has 5 groups and the other has 2.

• The study is on memory: how many words, on average, do people recall
when given certain processing tasks?

• This example is where the graph on our Canvas home page comes from. It
uses data simulated to mimic data from a 1974 Hans Eysenck study.

\begin{itemize}
\item
  100 subjects were split into 5 recall groups:

  \begin{itemize}
  \item
    ``Counting'': subjects counted how many letters were in each
    presented word
  \item
    ``Rhyming'' subjects thought of words that rhymed with each
    presented word
  \item
    ``Adjective'': subjects thought of an adjective that could be used
    to modify each presented word
  \item
    ``Imagery'': subjects were told to form vivid images of each word
  \item
    ``Intentional'': subjects were told to memorize the word for later
    recall
  \end{itemize}
\end{itemize}

• Counting and rhyming are lower level processing tasks, so the
hypothesis was that this group would recall fewer words than the others.
Subjects were also classified as ``young'' or ``old''.

• Here's the ANOVA model:

\[
Recall_{ijk} = \mu + Group_j + Age_k + (Group \cdot Age)_{jk} + \epsilon_{ijk}, \text{ where } \epsilon_{ijk} \sim Normal(0,\sigma^2) \\
\text{ and } j = 1, \dots ,5, k = 1, 2
\]

• Jamovi has an ``ANOVA'' function and a ``regression'' function. We'll
use both the analyze these data.

\begin{figure}

\includegraphics[width=1.41667in,height=\textheight]{images/mod5_1.png} \hfill{}

\end{figure}

• First we'll use ANOVA, using items recalled as the response
(dependent) variable, and recall condition and age as factors. Note that
jamovi automatically includes their interaction.

\begin{figure}

{\centering \includegraphics{images/Mod5_2.png}

}

\end{figure}

• I am usually not interested in the sums of squares or mean squares in
the ANOVA table, as these are not interpretable. We do have p-values for
each ``main effect'' along with the interaction. More importantly, we
have effect size statistics: eta-squared (\(\eta^2\)) and partial
eta-squared (\(\eta^2p\))

\hypertarget{anova-effect-sizes-eta2-and-partial-eta2}{%
\subsection{\texorpdfstring{ANOVA effect sizes: \(\eta^2\) and
partial-\(\eta^2\)}{ANOVA effect sizes: \textbackslash eta\^{}2 and partial-\textbackslash eta\^{}2}}\label{anova-effect-sizes-eta2-and-partial-eta2}}

• \(\eta^2\) is akin to \(R^2\) for one factor (main effect or
interaction). It gives the proportion of variance in the response
attributable to the factor in question:

\[
\eta^2 = \frac{SS_{factor}}{SS_{total}}
\]

\begin{figure}

{\centering \includegraphics[width=4.83333in,height=\textheight]{images/Mod5_3.png}

}

\end{figure}

• Here, we see that recall condition explains the most variance by far,
followed by age, followed by their interaction.

• \(\eta^2p\) (partial eta-squared) is like \(\eta^2\), but with the
variance accounted for by the other factors removed from the
denominator:

\[
\eta^2p = \frac{SS_{factor}}{SS_{factor} + SS_{residuals}}
\] \includegraphics{images/Mod5_4.png}

• Example: for recall condition,

\[
\eta^2p = \frac{1515}{1515 + 722} = 0.677, \text{ and } \eta^2 = \frac{1515}{1515 + 240 + 190 + 722} = 0.568
\]

• I personally prefer \(\eta^2\), as their sum cannot exceed \(1\). Some
prefer \(\eta^2p\), because it quantifies the ``effect'' of a factor
relative to remaining unexplained variance,

\hypertarget{the-means-interaction-plot}{%
\subsection{The means / interaction
plot}\label{the-means-interaction-plot}}

• We can make a nice plot under ``estimated marginal means'':

\begin{figure}

{\centering \includegraphics{images/Mod5_5.png}

}

\end{figure}

• This plot shows every combination of means across recall condition and
age. It also has 95\% CIs around each mean, and raw data displayed.

\begin{figure}

{\centering \includegraphics[width=4.65625in,height=\textheight]{images/Mod5_6.png}

}

\end{figure}

• This is sometimes called a ``means plot'' or an ``interaction plot''.
The interaction is represented by non-parallel lines, e.g.~a positive
change going from ``Imagery'' to ``Intention'' for young people, but a
negative change for old people.

• The previous plot showed means for recall condition, with separate
lines for age. If we flip the order of the variables, we get this:

\begin{figure}

{\centering \includegraphics[width=5.03125in,height=\textheight]{images/Mod5_7.png}

}

\end{figure}

• Here we see that mean items recalled for young people is substantially
higher than for old people in the last three conditions, but differs
only slightly in the first two conditions.

\hypertarget{anova-diagnostics}{%
\subsection{ANOVA diagnostics}\label{anova-diagnostics}}

• We can also run diagnostic tests, under ``Assumption checks'':

\begin{figure}

\includegraphics[width=1.52083in,height=\textheight]{images/Mod5_8.png} \hfill{}

\end{figure}

• The QQ plot looks pretty good.

\begin{figure}

{\centering \includegraphics{images/Mod5_9.png}

}

\end{figure}

• I personally do not recommend paying attention to Levene's
``homogeneity of variances'' test, nor to the Shapiro-Wilk normality
tests

\begin{figure}

{\centering \includegraphics[width=3.05208in,height=\textheight]{images/Mod5_10.png}

}

\end{figure}

• These tests use null hypotheses of ``population variance is the same
in all groups'', or ``the residuals were drawn from a normal
distribution''. As I don't think these model assumptions could be
literally true, I am not interested in whether they can be rejected by
the data.

• A ``significant'' violation of modeling assumptions does not imply a
consequential violation. In particular, if sample size is large, trivial
violations of assumption

\hypertarget{doing-all-of-this-as-regression}{%
\subsection{Doing all of this as
regression}\label{doing-all-of-this-as-regression}}

• The beginning of these slides claimed that ANOVA is just regression
with categorical predictors. Let's see what our results look like if we
use jamovi's ``linear regression'' function rather than ``ANOVA''.

\begin{figure}

{\centering \includegraphics[width=2.52083in,height=\textheight]{images/Mod5_11.png}

}

\end{figure}

• The predictor variables are entered as ``factors'' here, rather than
as ``covariates'', to ensure they are treated as categorical rather than
as quantitative variables.

\begin{figure}

{\centering \includegraphics[width=5in,height=\textheight]{images/Mod5_12.png}

}

\end{figure}

• Finally, the interaction must be specified under ``model builder'';
jamovi does not create regression interactions by default.

• There are five recall condition groups, giving four indicator
variables, all compared against the baseline group ``counting'', whose
mean is represented by the intercept.

• There are two ``age'' groups; ``young'' is the baseline group.

\begin{figure}

\includegraphics[width=5.3125in,height=\textheight]{images/Mod5_13.png} \hfill{}

\end{figure}

• So the intercept of \(6.500\) is the mean words recalled for a young
person in the ``counting'' condition.

• The first indicator under RecallCondition is ``Rhyming -- Counting''.
Its slope of \(1.1\) is the difference in mean recall for these two
groups, when Age = Young

• The interaction slope for ``Rhyming -- Counting'' is \(-1.2\).

• So, for Age = Old, the difference in mean recall is
\(1.1 +(-1.2) = −0.1\)

• The interaction terms are all indicators, that ``turn on'' when Age =
Old, and ``turn off'' when Age = Young.

• Notice that the slope for Age (``Old -- Young'') is 0.5, suggesting
greater recall for older participants.

• However, the interaction slopes are all larger negative values.

• So, when RecallCondition = Counting, the mean items recalled for Age =
Old is larger than for Age = Young. But for all other recall conditions,
the interaction slopes turn this negative, and mean items recalled is
larger for younger participants.

\hypertarget{main-regression-results}{%
\subsubsection{Main regression results}\label{main-regression-results}}

• ``Estimated Marginal Means'' under regression will produce a similar
plot to the one made under ANOVA, just without the connecting lines and
raw data:

\begin{figure}

{\centering \includegraphics[width=4.11458in,height=\textheight]{images/Mod5_15.png}

}

\end{figure}

• If you look carefully, you should be able to see how the regression
results correspond to this plot. For instance, we see that the only
condition where Old \textgreater{} Young is Counting.

\includegraphics{images/Mod5_16.png}

\hypertarget{residual-plot}{%
\subsubsection{Residual plot}\label{residual-plot}}

• We can get a plot of residuals vs.~fitted values.

\begin{figure}

{\centering \includegraphics{images/Mod5_17.png}

}

\end{figure}

• Notice that the residuals are all vertically stacked? This is to be
expected when predictor variables are categorical.

• In this case, there are 5x2 = 10 possible combinations of groups that
participants could be assigned to. And so there are only 10 possible
``fitted'' (i.e.~predicted) values that the model can produce.

\hypertarget{ancova}{%
\section{ANCOVA}\label{ancova}}

• ANCOVA (analysis of covariance) is ANOVA with an additional continuous
predictor variable.

• Typically, this additional continuous predictor is not of primary
interest; the primary interest is still comparing group means.

• The continuous predictor is often thought of as a ``covariate'' -- a
variable that should be accounted for when drawing inference on the
other variables.

• A common use of ANCOVA is for modeling an outcome when ``baseline'' or
``pre-study'' or ``pre-test'' scores are available.

• For instance, consider testing different educational models on
different sections of a class. Some get traditional lecture, some are
completely ``flipped'', and some are a combination of the two.

• In this study, a preliminary quiz is given on the first day of class.
Score on the preliminary quiz will be the covariate. Score on an end of
semester quiz (``post'') will be the response variable.

\[
Post_{ij} = \mu + type_j + \beta(pre_{ij} - \overline{pre}) + \epsilon_{ij}
\]

• Here, \(j\) goes from \(1\) to \(3\), for the three teaching types
being compared.

• The pre-test predictor is centered (note that \(\overline{pre}\) is
mean for pretests).

• Imagine we believe that the mean differences between teaching types
will be larger for students with lower pretest scores. To account for
this possibility, let type and (centered) pretest interact:

\[
Course\_avg_{ij} = \mu + type_j + \beta(pre_{ij} - \bar{pre}) + \gamma_j(type_j)(pre_j - \overline{pre}) + \epsilon_{ij}
\]

\hypertarget{looking-at-the-data}{%
\subsubsection{Looking at the data}\label{looking-at-the-data}}

• The data file is called ``test\_pretest''. Here is the formatting:

\begin{figure}

{\centering \includegraphics{images/Mod5_18.png}

}

\end{figure}

• This plot was made using Analyses / Exploration / Scatterplot. Density
curves are there, just for fun:

\begin{figure}

\includegraphics[width=4.45833in,height=\textheight]{images/Mod5_19.png} \hfill{}

\end{figure}

\begin{figure}

\hfill{} \includegraphics[width=4.97917in,height=\textheight]{images/Mod5_20.png}

\end{figure}

• The three lines look pretty close to parallel, so there either isn't
an obvious interaction here, or it's small.

\begin{figure}

{\centering \includegraphics[width=3.46875in,height=\textheight]{images/Mod5_21.png}

}

\end{figure}

\hypertarget{analysis-using-ancova}{%
\subsubsection{Analysis using ANCOVA}\label{analysis-using-ancova}}

• Notice that the effect size for pre- score is far greater than the
effect size for type, or for the interaction. This is not surprising.

\begin{figure}

{\centering \includegraphics[width=6.52083in,height=\textheight]{images/Mod5_22.png}

}

\end{figure}

• Type is still significant; it's \(\eta^2p\) value is much larger than
its \(\eta^2\) value.

• Careful interpreting the ``Estimated marginal means'' plot -- it takes
into account only ``post'' scores!

\hypertarget{analysis-using-regression-w-indicators}{%
\subsubsection{Analysis using regression w/
indicators}\label{analysis-using-regression-w-indicators}}

• For the regression analysis, we'll use two indicator variables. We
don't have to do it this way; if we just include ``type'', jamovi will
create the indicators for us, using the first class type listed in the
data.

\begin{figure}

{\centering \includegraphics[width=4.05208in,height=\textheight]{images/Mod5_23.png}

}

\end{figure}

• Notice that the interaction estimates are small relative to their
standard errors (thus producing large p- values).

\begin{figure}

{\centering \includegraphics[width=5.30208in,height=\textheight]{images/Mod5_24.png}

}

\end{figure}

• According to this regression model, there is not a ``significant''
interaction.

\hypertarget{analysis-using-regression-wfactor}{%
\subsubsection{Analysis using regression
w/factor}\label{analysis-using-regression-wfactor}}

• Here's what happens if we put in ``type'' as a factor variable rather
than making our own indicators. The results are the same.

\begin{figure}

{\centering \includegraphics[width=5.3125in,height=\textheight]{images/Mod5_25.png}

}

\end{figure}

• The QQ plot and residual plot both look great

\begin{figure}

{\centering \includegraphics{images/Mod5_26.png}

}

\end{figure}

\hypertarget{analyzing-paired-data-ancova-vs-anova}{%
\subsection{Analyzing paired data: ANCOVA vs
ANOVA}\label{analyzing-paired-data-ancova-vs-anova}}

• Another approach we could take would be to compute the differences in
the two scores for each person, then do a regular ANOVA or regression
analysis on those.

\begin{figure}

{\centering \includegraphics[width=4.13542in,height=\textheight]{images/Mod5_27.png}

}

\end{figure}

• We see that there are significant differences in mean test score
change between teaching types (\$F=8.94, p\textless0.001\$)

\begin{figure}

{\centering \includegraphics[width=4.5625in,height=\textheight]{images/Mod5_28.png}

}

\end{figure}

\begin{figure}

{\centering \includegraphics[width=3.46875in,height=\textheight]{images/Mod5_29.png}

}

\end{figure}

\hypertarget{analyze-differences-regression-approach}{%
\subsubsection{Analyze differences, regression
approach}\label{analyze-differences-regression-approach}}

• This agrees with the ANOVA results from the previous slide: we have a
statistically significant difference in mean test score differences
(post -- pre) when comparing ``combo'' to ``flipped'' or
``traditional''. And \(R^2 = \eta^2\)!

\begin{figure}

{\centering \includegraphics{images/Mod5_30.png}

}

\end{figure}

• It turns out here that taking the post -- pre differences first and
then comparing mean differences across teaching types produces similar
results to predicting post-test scores using pre-test and teaching type
as predictors.

• But, these methods are not answering the exact same question. Using
pre-test as a covariate, we answer the question ``what difference do I
expect in post-test scores when comparing two students with the same
pre- test score but different teaching types''?

• When differencing first and then doing the analysis, we answer the
question ``what differences in the mean post-pre score change do I
expect when comparing class types''?

• These question sound similar, but they aren't the same! Whether to use
ANCOVA or do the differencing first is a matter of subjective judgement,
and the experts don't all agree (see ``Lord's Paradox'' for more fun on
this).

\bookmarksetup{startatroot}

\hypertarget{chapter-5-analyzing-categorical-data}{%
\chapter{Chapter 5: Analyzing categorical
data}\label{chapter-5-analyzing-categorical-data}}

\hypertarget{confidence-intervals-and-hypothesis-tests-for-proportions}{%
\section{Confidence intervals and hypothesis tests for
proportions}\label{confidence-intervals-and-hypothesis-tests-for-proportions}}

• At the beginning of the class we reviewed confidence intervals and
hypothesis tests for means. These methods can also be used for
proportions

• A proportion is just a special kind of mean, where the data are all
ones and zeros.

• Example: 8 out of 10 people say ``yes'' to the question ``Is politics
too polarized?'' Let yes = 1 and no = 0. Average is:

𝑥ҧ

1 + 1 + 1 + 1 + 1 + 1 + 1 + 1 + 0 + 0 8 = =

= 0.8

• We make confidence intervals and perform hypothesis tests for
proportions using the exact same methods used for means. Only
differences are:

• Sampling distribution of proportions follow z, rather than t (the
difference is usually trivial)

• We denote the population proportion 𝜋 and the sample proportion

𝜋ො

• Standard error of a sample proportion is 𝑠𝜋ෝ =

• Here are results from a recent political poll:

• Say we want to make a confidence interval for the proportion of
registered voters who say that ``things in the country are going in the
right direction''. Denote this population proportion 𝜋.

• 95\% CI for 𝜋:

𝜋ො

± 𝑧𝑐𝑟𝑖𝑡𝑖𝑐𝑎𝑙

∗ 𝑠𝜋ෝ

= 0.39 ± 1.96 ∗

= 0.39 ± 0.022 = (0.368, 0.412)

• In jamovi, enter a column of frequencies for each value of the
response variable, then use Frequencies / N -- Outcomes or Frequencies /
2 Outcomes:

\hypertarget{toy-example-skin-cream-and-rashes}{%
\subsection{Toy example: skin cream and
rashes}\label{toy-example-skin-cream-and-rashes}}

• Here is an example from the 2013 paper ``Motivated Reasoning and
Enlightened Self Government'', by Kahan et. al.:

• Putting the data into jamovi:

• Analyzing the data using Frequencies / Independent Samples: • Rash is
the response variable • Skin cream is the predictor variable • Frequency
tells how often each combination occurred. (note: if you had raw data
where each row was a single response, you would not use Freq)

• Results! There's a lot in here\ldots{}

• Split bar plot: displays each cell in the contingency table as a bar:

• Here we can easily see that the largest number of people were those
who got the skin cream and whose rash got better.

• But, we can also see that rashes got better at a higher rate for those
who did not get the skin cream.

• We'll cover the chi-square results soon. Right now, let's have jamovi
directly compare proportions, using Frequencies / Independent Samples:

• Here, jamovi quantifies what we saw in the split bar plot: that the
proportion of those who got better without the skin cream is greater
than the proportion of those who got better with the skin cream:

− 𝑃 𝐵𝑒𝑡𝑡𝑒𝑟 𝑁𝑜 = −0.0876

• We also see a 95\% CI for this difference, which is fairly wide and
just barely excludes zero

• And we see the p-value testing against:

𝐻0: 𝜋

− 𝜋 = 0

• The two-sided p-value is 0.047, so this result is just barely
significant. Woohoo!

\hypertarget{relative-risk}{%
\section{Relative risk}\label{relative-risk}}

• Instead of knowing the difference in proportions / probabilities, we
may want to know their ratio. This would tell us how many times larger
one is than the other.

• This is quantified by the ``relative risk'', a.k.a. ``risk ratio'' :

𝑅𝑅 =

𝑃𝑟𝑜𝑝𝑜𝑟𝑡𝑖𝑜𝑛 𝐴

𝑃𝑟𝑜𝑝𝑜𝑟𝑡𝑖𝑜𝑛 𝐵

• The phrase ``risk'' is used because this method is popular for
comparing the risk of a negative outcome under two conditions
(e.g.~treatment and

\hypertarget{relative-risk-in-jamovi}{%
\subsection{Relative Risk in jamovi}\label{relative-risk-in-jamovi}}

• ``Relative Risk'' is an option under Comparative Measures. jamovi will
give conditional probabilities based on the order of the data. Here,
rash getting worse is Better is selected:

𝑃 𝐵𝑒𝑡𝑡𝑒𝑟 𝑌𝑒𝑠 /𝑃 𝐵𝑒𝑡𝑡𝑒𝑟 𝑁𝑜

𝑃 𝑌𝑒𝑠 𝐵𝑒𝑡𝑡𝑒𝑟 /𝑃 𝑌𝑒𝑠 𝑊𝑜𝑟𝑠𝑒

20

𝑃 𝐵𝑒𝑡𝑡𝑒𝑟 𝑌𝑒𝑠 /𝑃 𝐵𝑒𝑡𝑡𝑒𝑟 𝑁𝑜 • We see here that 𝑅𝑅 = 0.895.

Notice the 95\% CI is not very wide.

• We could also flip this ratio:

𝑃 𝐵𝑒𝑡𝑡𝑒𝑟 𝑁𝑜 /𝑃 𝐵𝑒𝑡𝑡𝑒𝑟 𝑌𝑒𝑠 • Now everything is reciprocated, e.g.~1

0.809

= 1.236

𝑃 𝐵𝑒𝑡𝑡𝑒𝑟 𝑌𝑒𝑠 /𝑃 𝐵𝑒𝑡𝑡𝑒𝑟 𝑁𝑜

• Notice that, for ``Better'', RR is small but (just barely)
statistically significant, because the CI does not contain 1. • For
``Worse'', RR is large but (just barely) not statistically significant,
because the CI contains 1.

• There is another very popular kind of ratio called an ``odds ratio'',
which we will consider more when we cover logistic regression.

\hypertarget{the-chi-square-test}{%
\section{The chi-square test}\label{the-chi-square-test}}

• The chi-square (𝜒2) test is a popular hypothesis test for comparing
observed frequencies to expected frequencies under a null hypothesis.

• The null is typically that of ``independence'' between two categorical
variables, meaning the probability an observation falls into a category
for one variable does not depend on its category for the other variable

(As a side note, 𝜒2 is a distribution that is used for many purposes,
including modeling distributions of variances. A statistic that is
distributed chi-square is not necessarily being used in the context of
the chi-square test outlined here)

• Staying with the skin cream example, here is the contingency table as
jamovi initially reports it: • There is a lot being shown here. Top rows
are observed frequencies, what jamovi calls ``observed''.

• The two middle rows are column \% and row \%.

• Notice that the column \%'s sum to 100\% down the columns, and the row

• We can have jamovi display the components of a chi-square test.

• The first of these are the expected counts under the null hypothesis
of independence.

• To see how these ``expected'' counts would suggest independence,
consider the relative risk:

𝑅𝑅 =

𝑃(𝑊𝑜𝑟𝑠𝑒\textbar 𝑁𝑜)

𝑃(𝑊𝑜𝑟𝑠𝑒\textbar 𝑌𝑒𝑠)

28.8451 28.8451+99.1549

28.8451 = 128

0.2254 = = 1 0.2254

• The chi-square statistic compares the expected frequencies under the
null (which we denote E) to the observed frequencies in the data (which
we denote O).

𝜒2 = ෍

2

𝐸

• This is a general formula that can be used when you have one variable,
two variable, three variables, etc.

• Most popular use is for two variables, as in this skin cream example.

• Just to verify the math, here's the chi-square calculation for the
upper left cell of the table:

𝑂 − 𝐸 2 ෍ = 𝐸

107 − 99.2 2 + 99.2

223 − 230.8 2 + 230.8

75 − 67.2 2 + 67.2

21 − 28.8 2

28.8

= 3.94

• And here is the jamovi output showing the full chi-square test:

(``Pearson'' chi-square is the classic chi-square test)

• Verifying that this chi-square statistic is indeed the sum of the
chi-square values for each of the four cells:

2 𝜒2 = ෍ 𝐸

= 0.6207 + 2.1336 + 0.2666 + 0.9165 = 3.937

• The p-value is found using the appropriate degrees of freedom for the
chi-square distribution. We won't cover this part.

• In this case, we see that the chi-square test just barely meets the
standard for statistical significance (𝑝 = 0.0472).

• This is in line with the other methods we used to analyze these data.

• Recall that the test for a difference in proportions was barely
significant, and the risk ratios were just on either side of
significance, depending on whether ``rash got worse'' or ``rash got
better'' was used as the outcome variable.

• The chi-square test can be used when we have frequencies for a single
variable. All we have to do is specify expected counts or probabilities.

• Going back to the polling data, we can select Frequencies / N -
Outcomes, and then make Answer ``Variable'' and Frequency ``Counts''.

• Suppose the null hypothesis is that equal numbers of voters feel the
country is on the right vs.~wrong track. Enter 0.5 for hypothesized
probability:

• Here we get a very large chi-square statistic and a very small
p-value.

• No surprise; the frequencies were very different!

My opinion on these

• I personally dislike statistical tests, and I really dislike tests
that don't incorporate an interpretable statistic.

• So, I am not a big fan of this chi-square test. To compare rates for
categorical variables, I prefer a 95\% CI around either a relative risk
or a difference in proportions, whichever seems more meaning for the
question at hand.

• Note that a ``chi-square test'' is quite general; it refers to any
test whose test statistic follows a chi-square distribution. So you may
see ``chi-square tests'' that are not being used to test against a null
of independence for categorical variables.

• The methods we've covered in these notes are useful for analyzing
fairly simple categorical data.

• In the next set of notes, we will look at logistic regression, which
is a way to use regression modeling to predict the outcome of a
categorical variable.

\bookmarksetup{startatroot}

\hypertarget{generalized-linear-models-glms}{%
\chapter{Generalized Linear Models
(GLMs)}\label{generalized-linear-models-glms}}

\hypertarget{part-1-logistic-regression}{%
\section{Part 1: Logistic regression}\label{part-1-logistic-regression}}

• All of the regression methods we've seen have involved models in which
the response variable is normally distributed, given values for the
predictor variables

• In other words, the residuals have been modeled as normal.

• What if we have a different kind of response variable? In particular,
consider a binary response variable. Maybe the outcomes are ``yes'' and
``no'', or ``success'' and ``failure'', or ``present'' and ``absent''.

• Logistic regression is a type of ``generalized linear model'' (GLM)
that works well for modeling binary outcome data.

• Before we get into logistic regression, though, let's see what happens
if we use standard regression (sometimes called ``ordinary least
squares'', or OLS regression) with a binary response.

• We'll use simulated data corresponding to a study of sexual harassment
reporting at a university. (Brooks and Perot ``Reporting Sexual
Harassment: Exploring a Predictive Model'' (1991)).

• Here is data on whether or not sexual harassment at a university was
reported, using the offensiveness of the behavior as a predictor
variable: • Data points are ``jittered'' so that they don't fall right
on top of one another.

• Suppose we want to predict the value of ``Report'', using
``OffensBeh''.

• Here is the linear regression line. In this picture, the response
variable takes on the values 0 and 1, and the data are not jittered. •
The predicted value of ``Report'' can be thought of as the predicted
probability that Report=1 (for reported behavior)

• Note that this line can go below zero and above one. We don't want to
predict probability greater than 1! A straight line is not great here.
Logistic

\hypertarget{the-logistic-regression-model}{%
\subsection{The logistic regression
model}\label{the-logistic-regression-model}}

Another way of writing a linear regression model

• By now we are well familiar with the linear regression model:

𝑦𝑖 = 𝛽0 + 𝛽1𝑋1𝑖 + 𝛽2𝑋2𝑖 + ⋯ + 𝛽𝑝𝑋𝑝𝑖, +𝜀𝑖 𝜀𝑖\textasciitilde 𝑁𝑜𝑟𝑚𝑎𝑙(0, 𝜎)

• Here is an equivalent way of writing it:

𝑦𝑖\textasciitilde 𝑁𝑜𝑟𝑚𝑎𝑙 𝜇𝑖 = 𝛽0 + 𝛽1𝑋1𝑖 + 𝛽2𝑋2𝑖 + ⋯ + 𝛽𝑝𝑋𝑝𝑖

• In other words, the response variable is normally distributed with
some mean 𝜇, and the value of 𝜇 is determined by the predictor (𝑋)
variables.

Writing a logistic regression model

• We will take this approach to writing the logistic regression model.

• What we want is a regression equation that looks like this:

𝑦𝑖 = 𝛽0 + 𝛽1𝑋1𝑖 + 𝛽2𝑋2𝑖 + ⋯ + 𝛽𝑝𝑋𝑝𝑖

But that will work when 𝑦𝑖 does not follow a normal distribution.

• Response variable 𝑦 takes on the values 0 and 1.

• Denote the probability that 𝑦 = 1 as 𝜋.

• This can be written 𝑦\textasciitilde 𝐵𝑒𝑟𝑛𝑜𝑢𝑙𝑙𝑖

(The Bernoulli distribution is a distribution of 1's and 0's, where the
probability of 1 is 𝜋 and the probability of 0 is 1 − 𝜋)

• We will use regression to model 𝜋, the probability that 𝑦 = 1. This is
often thought of as the probability of a ``success''.

• If we wanted, we could use this model:

𝑦𝑖\textasciitilde 𝐵𝑒𝑟𝑛𝑜𝑢𝑙𝑙𝑖 𝜋𝑖 𝜋𝑖 = 𝛽0 + 𝛽1𝑋1𝑖 + 𝛽2𝑋2𝑖 + ⋯ + 𝛽𝑝𝑋𝑝𝑖

• The Bernoulli distribution is a distribution of 1's and 0's, where the
probability of 1 is 𝜋 and the probability of 0 is 1 − 𝜋.

• The standard deviation of a Bernoulli distribution is 𝜋(1 − 𝜋 ). So, 𝜋
is the only parameter for this distribution. This is different from the
normal distribution, which has two parameters 𝜇 and 𝜎.

• But, as we saw in the opening example, a linear model for probability
can have serious deficiencies.

• So, instead of a linear model for probability 𝜋, we'll make a linear
model for a function of 𝜋, so that the variable on the left hand side of
the equation is linearly related to the variable(s) on the right.

• In logistic regression, we use the ``logit'' function, also known as
``log odds''

𝑙𝑜𝑔𝑖𝑡

= ln

= ``𝑙𝑜𝑔 𝑜𝑑𝑑𝑠''

• Applied to our sexual harassment example, we would like to predict the
probability that harassing behavior is reported. This probability is
denoted 𝜋

• Plugging this into the logit formula:

𝑙𝑜𝑔𝑖𝑡

= ln

= ``𝑙𝑜𝑔 𝑜𝑑𝑑𝑠'' 𝑜𝑓 𝑟𝑒𝑝𝑜𝑟𝑡𝑖𝑛𝑔

• This will be our response variable for logistic regression.

• Logit vs.~probability, visually:

\hypertarget{odds}{%
\subsection{Odds}\label{odds}}

• To understand logistic regression, you'll need to understand odds.

• In casual English, ``odds'' and ``probability'' are often used
interchangeably.

• In statistics, they are not the same thing. Odds tell you how likely
one outcome is compared to another.

• For instance, you might hear that a football team has been given ``3
to 2'' odds of winning a game. This means that their probability of
winning is 3Τ2 = 1.5 times as big as their probability of losing. Or,
that they'd be expected to win 3 times for every 2 times they lost.

• Formally, consider some outcome A, where the probability of A
occurring is written as ``𝑃(𝐴)''. In this case,

𝑜𝑑𝑑𝑠

𝑃(𝐴)

1 − 𝑃(𝐴)

𝑝𝑟𝑜𝑏𝑎𝑏𝑖𝑙𝑖𝑡𝑦 𝐴 𝑜𝑐𝑐𝑢𝑟𝑠

𝑝𝑟𝑜𝑏𝑎𝑏𝑖𝑙𝑖𝑡𝑦 𝐴 𝑑𝑜𝑒𝑠 𝑛𝑜𝑡 𝑜𝑐𝑐𝑢𝑟

• This is the ratio of the probability A occurs to the probability A
does not occur.

• Some probabilities and their associated odds:

• Think of odds(A) as ``how many times will A occur for every time A
does not occur?''

• Sometimes we add ``to 1'' to an odds statement, e.g.~``odds of 4 to
1'' means ``this outcomes occurs 4 times for every 1 time

Back to the logistic regression model

• The response variable for logistic regression, again, is:

𝑙𝑜𝑔𝑖𝑡

= ln

= ``𝑙𝑜𝑔 𝑜𝑑𝑑𝑠''

• So, the full logistic regression model is :

𝑦𝑖\textasciitilde 𝐵𝑒𝑟𝑛𝑜𝑢𝑙𝑙𝑖

• We are not actually interested in log odds; we only make this
conversion

for mathematical convenience. So, once we have 𝑙𝑜𝑔𝑖𝑡 𝜋ො𝑖 back via the
inverse logit function:

, we can get

𝑙𝑜𝑔𝑖𝑡−1

• In the context of the logistic regression model:

\hypertarget{jamovi-example}{%
\subsection{jamovi example}\label{jamovi-example}}

• Applying this to the harassment data, we use Linear Models /
Generalized Linear Models in jamovi and select Logistic under
Categorical dependent variable.

• Note that ``Target Level'' defaults to zero. Changing it to 1 makes
sense in this case; we want to predict 𝑃(𝑅𝑒𝑝𝑜𝑟𝑡𝑒𝑑).

• jamovi also produces a Loglikelihood ratio test, but we will just
focus on ``Parameter Estimates'':

• Here is our estimated model:

𝑙𝑜𝑔 𝑜𝑑𝑑𝑠 = −1.7976 + 0.4869 ∗ 𝑂𝑓𝑓𝑒𝑛𝑠𝐵𝑒ℎ

• Plugging in large and small values for OffensBeh:

𝑙𝑜𝑔 𝑜𝑑𝑑𝑠 𝑅𝑒𝑝𝑜𝑟𝑡𝑒𝑑 = −1.7976 + 0.4869 ∗ 1 = −1.3107 𝑙𝑜𝑔 𝑜𝑑𝑑𝑠 𝑅𝑒𝑝𝑜𝑟𝑡𝑒𝑑 =
−1.7976 + 0.4869 ∗ 8 = 2.0976

𝜋ො\textbar 𝑂𝑓𝑓𝑒𝑛𝐵𝑒ℎ = 1:

𝑒−1.3107 1 + 𝑒−1.3107 =

0.2696 = 0.21 1.2696

𝜋ො\textbar 𝑂𝑓𝑓𝑒𝑛𝐵𝑒ℎ = 1:

𝑒2.0976 1 + 𝑒2.0976 =

8.1466 = 0.89 9.1466

• The slope coefficient is directly interpreted as change in log odds
for a one unit increase in the predictor. ``Log odds'' are not of direct
interest.

• Exponentiating both sides of the equation gives straight odds

𝑜𝑑𝑑𝑠 =

= 𝑒𝛽0+𝛽1𝑋1𝑖+𝛽2𝑋2𝑖+⋯+𝛽𝑝𝑋𝑝𝑖

• This means that, for a one unit increase in 𝑋1, odds are multiplied by
𝑒𝛽1

• For the harassment data:

𝑙𝑜𝑔 𝑜𝑑𝑑𝑠 = −1.7976 + 0.4869 ∗ 𝑂𝑓𝑓𝑒𝑛𝑠𝐵𝑒ℎ

• 𝛽መ = 0.4869. So, for a one unit increase in OffensBeh, predicted odds
of reporting are multiplied by 𝑒0.4869 = 1.627

• In other words, there is about a 63\% increase in odds of reporting
when OffensBeh increases by one. NOTE: odds are not probabilities!

• Comparing probabilities and odds from this model:

OffensBeh P(Report) Odds(Report) 1 0.212 0.27 2 0.305 0.44 3 0.417 0.71
4 0.537 1.16 5 0.654 1.89 6 0.755 3.08 7 0.834 5.01 8 0.891 8.15 9 0.930
13.26 10 0.956 21.57

\hypertarget{part-2-poisson-and-negative-binomial-regression}{%
\section{Part 2: Poisson and negative binomial
regression}\label{part-2-poisson-and-negative-binomial-regression}}

We'll now look at two other popular GLMs: Poisson (``pwa-sawn'' roughly)
and negative binomial.

• These are used for modeling count data, which can be extended to how
often a categorical variable takes on some value. Thus Poisson
regression can be used to model contingency table data.

\hypertarget{the-poisson-distribution}{%
\subsection{The Poisson distribution}\label{the-poisson-distribution}}

• The Poisson distribution is a discrete probability distribution. A
Poisson distributed variable takes on only positive integer values. The
integer is referred to as ``count'' or ``\# of events''.

• The Poisson distribution has a single parameter, 𝜆 (``lambda''), which
is sometimes called the ``rate'' parameter.

• 𝜆 is both the mean and the variance of a Poisson distribution

• The probability function for the Poisson is: 𝜆𝑘

𝑃

= 𝑒𝜆𝑘! 4

Visualizing the Poisson distribution

𝜆 = 1

𝜆 = 2

𝜆 = 5

𝜆 = 10

\hypertarget{the-structure-of-a-glm}{%
\subsection{The structure of a GLM}\label{the-structure-of-a-glm}}

• In logistic regression, the response variable was 𝑙𝑜𝑔𝑖𝑡(𝜋) = ln

• In Poisson regression, the response variable is ln(𝜆)

• The reasoning will be that the natural log allows the estimated rate
to be modeled as a linear function of some predictor variables.

• This is how GLMs work: they allow us to use non-normal response
variables by expressing a function of their mean as a linear function of
the predictors.

• A GLM has three parts:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  A response variable with some distribution
\item
  A ``link function'', 𝑔(∙), that is applied to the mean of the response
  variable.
\item
  A linear expression of the predictor variables: 𝛽0 + 𝛽1𝑋2 + 𝛽2𝑋2 + ⋯
\end{enumerate}

GLM examples

• Logistic regression uses 𝑦\textasciitilde 𝐵𝑒𝑟𝑛𝑜𝑢𝑙𝑙𝑖(𝜋) as the response
variable and

𝑔

= ln

as the link function.

• Poisson regression uses 𝑦\textasciitilde 𝑃𝑜𝑖𝑠𝑠𝑜𝑛(𝜆) as the response
variable and 𝑔 = ln(𝜆) as the link function.

• Ordinary least squares (OLS) regression can also be considered a
special case of a GLM. It uses 𝑦\textasciitilde 𝑁𝑜𝑟𝑚𝑎𝑙(𝜇, 𝜎) and the
response variable and the identity function, 𝑔 = 𝜇 as the link.

• It's worth briefly noting that the mathematical method used to come up
with parameter estimates for GLMs is not ``least squares''. So, we are
not getting our 𝛽መ𝑠 by minimizing sums of squared residuals.

• Instead, the estimation procedure we use is called ``maximum
likelihood''. This method finds the values of the parameter estimates
that maximize (i.e.~make as large as possible for a given set of data)
something called ``the likelihood function''.

• The likelihood function takes a fixed set of data and an assumed
distribution (e.g.~normal), and gives the ``probability of the data'',
given

• So, if we have:

𝑌𝑖\textasciitilde 𝑁𝑜𝑟𝑚𝑎𝑙

• Then the likelihood function is:

• Values of 𝛽መ , 𝛽መ , 𝜎ො are found that maximize this function,
i.e.~that maximize the probability of the data, given the parameter
estimates.

\hypertarget{poisson-regression-example}{%
\subsection{Poisson regression
example}\label{poisson-regression-example}}

• We'll use some General Social Survey data for this example.

• Poisson is good for modeling count data, so we'll use a response
variable that takes the form of counts.

• For this example, the goal will be to look at the relationship (if
any) between the number of sibling a person has, and the number of
children that person has.

• Our question will be: do people with more siblings tend to have more
children? And if so, can we quantify the relationship?

• It would be wise to collect data on covariates that we expect will
also be related to the number of children someone has.

• An obvious one is age. Older people will have more children than
younger people.

• We might also want to control for ``culture''. If people from
different cultural backgrounds tend to have more or fewer children, then
this would definitely induce a relationship between \# of siblings and
\# of children.

• There are lots of possible ways to try account for cultural
background.

• So, the variables will be:

• \# of children • \# of siblings • Age • Frequency of attending
religious services

• We'll just look at 2018 data. The GSS lets us choose any years we
want, going back to 1972.

• This data set is on Canvas, as GSS\_Children\_Siblings.jmp

• First thing to do is plot our variables.

• Yikes! There's some cleaning to do. The instances of 98 siblings are
not real data points.

• GSS data explorer website lets us look in detail at each variable.
Here is part of the coding for ``SIBS'':

• And here's the coding for the variable CHILDS.

• So, CHILDS = 9 is a value for missing data. These should also be
excluded.

• This should feel familiar -- remember how messy the NLSY data was in
the heights analysis?

• Keep in mind that data in public databases often have idiosyncrasies
like this.

• Here are the row selection options that will select all rows with
invalid responses.

• Once selected, they can be excluded.

• Here is the distribution of CHILDS. • This looks a lot like a Poisson
distribution. Hooray!

• To run the regression, use Linear Models / Generalized Linear Models
and choose Poisson(overdispersion) for Frequencies.

• Here are results for a simple model, where \# of siblings is the sole
predictor of \# of children. We'll just look at the parameter estimates
and the overdispersion statistic:

• Letting

𝜆መ

represent the predicted mean \# of children, we have:

ln = 0.375 + 0.0631(𝑆𝐼𝐵𝑆)

• You might not be surprised to learn that, due to the log link, the
exponentiated slope is interpreted as the multiplicative change in the
estimated value of the response variable, given a one unit increase in
the predictor variable. 𝑒0.0627 = 1.065\\
• So, increasing \# of siblings by one is associated with an 6.5\%
increase in \# of children.

• We can see that this is statistically significant, but it is also
small.

• It is also not obvious that \% change is the best way to quantify
this. Maybe an OLS model would have been more interpretable.

• It might be more desirable to relate an additive change in siblings to
an additive change in children.

• Downside is that \# of children is not normally distributed.

• As is often the case, we are trading some interpretability for a
better

• The Poisson distribution makes a strong assumption: the mean should be
equal to the variance.

• Often, we observe real data in which the variance is greater than the
mean.

• This is referred to as ``overdispersion''.

• If mean = variance, this should be equal to 1. But it rarely is.

• If there is strong overdispersion, a negative binomial model will fit
better.

• The overdispersion statistic is the ratio of the Pearson chi-square
statistic to its degrees of freedom..

• For these data, it turns out that \# of siblings, age, frequency of
attending religious services, and the interactions between age and the
other two variables are all statistically significant and all improve
model fit. Here are the parameter estimate results for this model:

• The other predictor variables and the interactions can be interpreted
in the usual ways.

• One thing to notice is that the estimate for SIBS has not changed
much. So, while the other covariates and interactions matter, they don't
substantially change our interpretation of the SIBS predictor.

\hypertarget{negative-binomial-regression}{%
\subsection{Negative binomial
regression}\label{negative-binomial-regression}}

• There is also still some overdispersion, though less than there was
before:

• Remember that the Poisson distribution only has one parameter. This
limits its flexibility.

• The negative binomial distribution is similar to the Poisson
distribution, but it is more flexible, and may be a better choice in the
presence of overdispersion.

• The negative binomial distribution is also a distribution for count
data. It is interpreted as giving the number of ``success'' before a
certain number of ``failures'' occur.

• There are two parameters: 𝑝, the probability of success, and 𝑟, the
number of failures at which counting stops.

• The mean of the negative binomial distribution is 𝑟

= 𝑟 ∗ 𝑜𝑑𝑑𝑠(𝑠𝑢𝑐𝑐𝑒𝑠𝑠)

• Example: suppose 𝑝 = 0.8 and 𝑟 = 2. We expect 2∗0.8

= 8 success before

• If a variable 𝑦 is distributed negative binomial, we denote it:

𝑦\textasciitilde 𝑁𝐵(𝑟, 𝑝)

• The mean of the negative binomial distribution is 𝑟

= 𝑟 ∗ 𝑜𝑑𝑑𝑠(𝑠𝑢𝑐𝑐𝑒𝑠𝑠)

• Example: suppose 𝑝 = 0.8 and 𝑟 = 2. We expect 2∗0.8

= 8 success before

we observe two failures.

• Or suppose 𝑝 = 0.5 and 𝑟 = 1. We expect 1∗0.5

= 1 success before we

observe 1 failure.

• For practical purposes, negative binomial regression will show better
fit than Poisson regression in the presence of overdispersion.

• The tradeoff is that the interpretation is less generally applicable.
It might not make sense to think of your count variable as \# of
successes for a certain \# of failures.

• As with Poisson regression, negative binomial regression uses a GLM
with a log link.

Negative binomial example

• Here is where you select negative binomial regression in jamovi:

• Under ``Generalized Linear Models'', under ``Frequencies'' choose
``Negative Binomial''.

Negative binomial results

• These results are awfully similar to the Poisson results.

• It is often the case that different statistical methods designed for
the same purpose will with similar results.

\bookmarksetup{startatroot}

\hypertarget{chapter-7-mixed-effects-models}{%
\chapter{Chapter 7: Mixed-effects
models}\label{chapter-7-mixed-effects-models}}

STAT 331

\hypertarget{test-for-screen-reader}{%
\section{Test for screen reader:}\label{test-for-screen-reader}}

\begin{longtable}[]{@{}llrlrlrlrl@{}}
\toprule\noalign{}
\multicolumn{10}{@{}>{\raggedright\arraybackslash}p{(\columnwidth - 18\tabcolsep) * \real{0.0000} + 18\tabcolsep}@{}}{%
{Fixed Effect Omnibus tests}} \\
& & & & & & & & & \\
\multicolumn{2}{@{}>{\centering\arraybackslash}p{(\columnwidth - 18\tabcolsep) * \real{0.0000} + 2\tabcolsep}}{%
} &
\multicolumn{2}{>{\centering\arraybackslash}p{(\columnwidth - 18\tabcolsep) * \real{0.0000} + 2\tabcolsep}}{%
F} &
\multicolumn{2}{>{\centering\arraybackslash}p{(\columnwidth - 18\tabcolsep) * \real{0.0000} + 2\tabcolsep}}{%
Num df} &
\multicolumn{2}{>{\centering\arraybackslash}p{(\columnwidth - 18\tabcolsep) * \real{0.0000} + 2\tabcolsep}}{%
Den df} &
\multicolumn{2}{>{\centering\arraybackslash}p{(\columnwidth - 18\tabcolsep) * \real{0.0000} + 2\tabcolsep}@{}}{%
p} \\
\midrule\noalign{}
\endhead
\midrule\noalign{}
\multicolumn{10}{@{}>{\raggedright\arraybackslash}p{(\columnwidth - 18\tabcolsep) * \real{0.0000} + 18\tabcolsep}@{}}{%
{Note.} Satterthwaite method for degrees of freedom} \\
\multicolumn{10}{@{}>{\raggedright\arraybackslash}p{(\columnwidth - 18\tabcolsep) * \real{0.0000} + 18\tabcolsep}@{}}{%
} \\
\bottomrule\noalign{}
\endlastfoot
Soil & & 1.769 & & 1 & & 8.000 & & 0.220 & \\
\end{longtable}

\hypertarget{part-1-repeated-measures}{%
\section{Part 1: Repeated measures}\label{part-1-repeated-measures}}

• This module covers data analysis methods for ``repeated measures''
data.

• ``Repeated measures'' refers to measuring the same subjects (people,
trees, dogs, cities, cells, widgets, etc) more than once.

• Studies that use repeated measures are often referred to as ``within
subjects'' studies. The idea is that multiple measurements within the
same subject will be compared.

• Mixed models are popular tools for analyzing repeated measures data.

• We will see these models formally in the next notes.

• These notes introduce the problems and opportunities that arise from
the use of repeated measures data.

\hypertarget{the-independence-assumption}{%
\subsection{The independence
assumption}\label{the-independence-assumption}}

• We have looked at model assumptions in this class, e.g.

𝜀𝑖\textasciitilde 𝑁𝑜𝑟𝑚𝑎𝑙(0, 𝜎)

• There is one that has been left out. The complete way to write this
statement is:

iid 𝜀𝑖 \textasciitilde{}

𝑁𝑜𝑟𝑚𝑎𝑙(0, 𝜎)

• The ``iid'' means ``independent and identically distributed.

iid 𝜀𝑖 \textasciitilde{}

𝑁𝑜𝑟𝑚𝑎𝑙(0, 𝜎) says that 𝜀𝑖 takes on values that are independently

sampled from the same normal distribution.

• ``independently sampled'' means that the value of the next data point
is not dependent upon the value of the previous data point.

• Every method we have used assumes independence of the response
variable, conditional on the predictor(s).

• But, if the same subjects are measured multiple times, then the
multiple observations on each subject will not be independent.

• Example: suppose we take a random sample of 10 people, have each solve
a maze, and record how much time it takes each person to solve it. Call
this time 𝑇.

• We might model the times as i.i.d. normal:

𝑖𝑖𝑑 𝑇𝑖 \textasciitilde{}

𝑁𝑜𝑟𝑚𝑎𝑙(𝜇, 𝜎)

• This means that, given a mean and standard deviation, each observed
time 𝑇𝑖 is uncorrelated with any previous times. This should make sense:
there is no reason that the 3rd person's maze time should depend upon
the 2nd person's maze time.

• But, if we take repeated measures on each subject, then our data will
not be independent.

• For instance, suppose we have 10 observations on maze completion from
2 people, where the first 5 come from one person and the last 5 come
from the other.

• Then I would expect correlation (non-independence) between the first 5
maze times, and between the last 5 times. Observation 5 should be closer
to observations 1-4 than it is to observations 6-10.

\hypertarget{cheating-with-repeated-measures}{%
\subsection{Cheating with repeated
measures}\label{cheating-with-repeated-measures}}

• We will look at a fake data simulation of repeated measures data.

• In this simulation, suppose we have cows infected with the Staph
bacteria.

• We are comparing two treatments for Staph. We have 6 cows, and each
treatment is randomly assigned to 3 cows. We will do a t-test to compare
mean infection levels (quantified on some arbitrary scale).

• We will also assume that the two treatments have an identical effect.
Therefore the null hypothesis of no difference in means is true.

• The file cows\_ttest.jmp has some fake data, simulated as
𝑦𝑖\textasciitilde 𝑁𝑜𝑟𝑚𝑎𝑙(12,4) for both groups:

• Not surprisingly, the t-test gives no significant difference in means:

95\% 𝐶𝐼 𝑓𝑜𝑟 𝜇1 − 𝜇2 =

, p−value = 0.321

• Now suppose we measure infection level for each cow seven times after
receiving treatment.

• The file cows\_repeated\_ttest.csv contains data simulating this
scenario.

• For each cow, there are 7 observations. These all average out to the
one observation per cow from the last data set.

• But check out these t-test results!

• Notice where it says degrees of freedom = 29.2. If your degrees of
freedom exceeds your sample size, something is wrong.

• Comparing the two t-tests:

• The mean difference is nearly the same for both.

• The standard error for the repeated measures data is much smaller.

• Remember that the t-test is just a simple regression model:

𝑖𝑖𝑑 𝑦𝑖 = 𝛽0 + 𝛽1𝐺𝑟𝑜𝑢𝑝𝑖 + 𝜀𝑖, 𝜀𝑖 \textasciitilde{}

𝑁𝑜𝑟𝑚𝑎𝑙

• In this case, 𝛽መ is the estimated mean difference in infection levels
between the two groups.

• There's nothing wrong with this estimate on its own. However its
standard error is computing using an incorrect assumption: that all of
the observations within each group are independent.

Effective sample size

• In this case, the apparent sample size is 𝑛 = 42, or 𝑛 = 21 per group.

• But the ``effective'' sample size is 𝑛 = 6, or 𝑛 = 3 per group.

• Taking lots of observations from each animal and doing a t-test on all
the data amounts to falsely inflating the sample size without having to
actually collect more data.

• Main lesson: observations that are correlated should not be treated as
independent!

\hypertarget{cheating-ourselves-with-repeated-measures}{%
\subsection{Cheating ourselves with repeated
measures}\label{cheating-ourselves-with-repeated-measures}}

• Treating repeated measures as independent can fool us into thinking
we've discovered an association when really there is none.

• It can also do the opposite: it can fool us into thinking we see no
association when really there is one.

• This will be illustrated with another toy example. In this case, we
will again consider collecting data on infection level in cows under two
treatments. This time, we'll imagine collecting repeated observations
over the course of a week.

• Here are the data in ``wide form''. Treatment group B is highlighted
to distinguish it from treatment group A.

• Two things to note:

o There is a lot of variability across cows. o Every cow's infection
rating goes down over time

• Here are the data in ``long form''.

• Long form is needed for most analyses.

• We can use jamovi's Rj-code editor to wide form to long form. (Though
using Excel might be easier) • The following code will convert the data
from wide to long form. Note you will need to open the data in a new
session of jamovi and rename the Day and Infection\_level columns. 23

Converting from wide to long form

• We must now transform the Day column of the longform data. We want the
variable Day to take on nominal integer values

• Double click the Day column and in the Data menu select Transform /
Using Transform / Create New Transformation.

• Here is a basic plot of infection rates across time.

• There appears to be a small downward trend, and lots of noise in the
data.

• But we can account for this noise! It is due to different cows having
different overall infection rates.

• Here is a similar plot, with the Y axis grouped by treatment.

• This shows a small decrease for treatment B.

• But, we are still treating these observations as though they are
independent.

• They are not independent; they are

• Here is the same plot, but with ``cow\_ID'' as the overlay variable.

• Each cow now gets its own line. The downward trend for B is clear.

• Also, it is now clear that most of the variation in infection rate was
due to differences between cows. Once this is accounted for, variability
is low.

Analyzing the data

• These plots illustrate the idea behind accounting for repeated
measures.

• In this module, we will learn how to incorporate repeated measures in
statistical models.

• For now, let's look at some different way of analyzing the data that
we just plotted.

• We can run regression models where infection rate is the response
variable and some combination of treatment and day are the predictors.

• We know both variables matter, and that they interact -- the
``effect'' of day on infection level depends on treatment, and the
``effect'' of treatment on infection level depends on day.

• The next slide shows output for fitting this model:

𝐼𝑛𝑓𝑒𝑐𝑡𝑖𝑜𝑛 𝑙𝑒𝑣𝑒𝑙𝑖 = 𝛽0 + 𝛽1𝑇𝑟𝑒𝑎𝑡𝑚𝑒𝑛𝑡 + 𝛽2𝐷𝑎𝑦 + 𝛽3𝑇𝑟𝑒𝑎𝑡𝑚𝑒𝑛𝑡 ∗ 𝐷𝑎𝑦 + 𝜀𝑖

𝑖𝑖𝑑 𝜀𝑖 \textasciitilde{}

𝑁𝑜𝑟𝑚𝑎𝑙 29

• 𝑅2 is small, suggesting this model does not explain much variability
in infection rates.

• Remember, we know that a lot of variability is due to differences
between cows. This is all getting ``absorbed'' by the error term.

• Large error variance gives small 𝑅2 and large standard errors for
slopes

Analyzing the data, accounting for cows

• Here are results from analyzing these data using a mixed model. This
mixed model accounts for differences between cows, and the fact that
repeated measures taken on each cow are correlated.

• We will cover the details of how mixed models work in the next set of
notes. For now, note the much larger 𝑅2 = 0.83

Side by side comparison

• The mixed model (on the bottom) gives smaller standard errors for Day
and the interaction.

• Note what didn't change: the parameter estimates!

Challenges from repeated measures data

• Repeated measures data can be challenging to analyze, if the design
gets complicated. We will see some study designs in which it isn't
immediately obvious how to set up an appropriate model.

• Repeated measures data can also be analyzed incorrectly (assuming
independence) to artificially increase apparent sample size and get
strong looking results that are not valid.

Opportunities from repeated measures data

• Repeated measures data can also be very useful!

• ``Within subject'' designs, in which subjects are measured repeatedly
across time and / or conditions, can greatly enhance the precision and
power of our inferences.

• This is because variability that would normally be accounted for by
the error term can instead be attributed to overall differences between
the subjects from whom repeated measurements were taken.

• We will get into the details of mixed modeling for repeated measures
data

\hypertarget{part-2-random-and-fixed-effects}{%
\section{Part 2: Random and fixed
effects}\label{part-2-random-and-fixed-effects}}

• The ``mixed'' in ``mixed models'' refers to a mix of random effects
and fixed effects.

• All of the predictor variables we've seen all semester have been
``fixed effects''. What this means will only make sense in comparison to
a new kind of predictor: a ``random effect''.

STAT 331

\hypertarget{random-effects}{%
\subsection{Random effects}\label{random-effects}}

• Every predictor variable we've seen this semester has been one for
which we estimate and interpret a slope coefficient.

• Sometimes, though, we want our model to account for a variable that is
important for explaining variability in the response variable, but for
which we are not interested in estimating coefficients.

• A random effect (or random factor) will accomplish this. Random
factors take on values that are treated as having been drawn at random
from a larger population of possible values that might be different if
we take a new sample. These random factors have coefficients (aka
slopes)

• In the last set of notes, we imagined measuring the same cows over and
over again. ``Cow'' should probably be treated as a random factor in
these cases. The cows themselves were drawn from a larger population; we
would not get the same cows again in a new study.

• Also, we wanted to account for differences in the mean infection
levels for each individual cow, so that we could estimate standard
errors appropriate to our study designs. And these mean infection levels
should also be treated as random, in that they came from some population
of possible mean infection levels.

• There are many interpretations of a ``random effect'', and they aren't
always helpful. From a 2005 paper by Andrew Gelman: 1. Fixed effects are
constant across individuals, and random effects vary.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\item
  Effects are fixed if they are interesting in themselves or random if
  there is interest in the underlying population.
\item
  When a sample exhausts the population, the corresponding variable is
  fixed; when the sample is a small (i.e., negligible) part of the
  population the corresponding variable is random''.
\item
  ``If an effect is assumed to be a realized value of a random variable,
  it is called
\end{enumerate}

• So, interpretations of what random vs.~fixed effects ``really mean''
will vary.

• But, formally, it isn't so ambiguous. ``Fixed'' effects are treated as
having ``fixed'' coefficients whose values we estimate and draw
inference on (e.g.~with confidence intervals and hypothesis tests).

• ``Random'' effects are treated as having ``random'' coefficients drawn
from some distribution. We won't estimate individual coefficients, but
we will estimate the variance of the distribution from which they came.

Subjects and Nesting

• Our main motivation right now is to have a method of accounting for
repeated measurements on the same subjects.

• So, in this class, we will look at mixed models for which the random
effect is ``subject''. In other words, we will make models that account
for differences between subjects measured multiple times.

• In a study in which subjects are assigned to groups, each subject is
assigned to one group.

• From a modeling perspective, subject is ``nested'' within group.

• In general, one variable is nested within the other if values of the
nested variable only occur in certain categories of the variable it is
nested within.

• In this case, cows are nesting within treatments because each cow is
measures only in one treatment.

• A non-nested design would have each cow measured under each value of
the other predictors.

• In other words, each cow would be measured under both treatment.

• Another example: say we are doing an education research study and we
randomly sample 4 districts in a state, then 3 schools in each district,
then 7 classrooms in each school

• In this case, classroom is nested within school, and school is nested
within district.

• This is because each classroom exists in only one school, and each
school exists in only one district.

• The notation for ``subject nested within group'' is 𝑠𝑢𝑏𝑗𝑒𝑐𝑡(𝑔𝑟𝑜𝑢𝑝)

\hypertarget{the-mixed-effects-model}{%
\subsection{The mixed effects model}\label{the-mixed-effects-model}}

• A mixed effects model contains both random and fixed effects.

• Applying this to the cows example, in which cows are assigned to one
of two treatment groups and measured daily for seven days:

𝐼𝑛𝑓𝑒𝑐𝑡𝑖𝑜𝑛 𝑟𝑎𝑡𝑒𝑖 = 𝛽0 + 𝛽1𝑇𝑟𝑒𝑎𝑡𝑚𝑒𝑛𝑡𝑖 + 𝛽2𝐷𝑎𝑦𝑖 + 𝛽3𝑇𝑟𝑒𝑎𝑡𝑚𝑒𝑛𝑡𝑖 ∗ 𝐷𝑎𝑦𝑖

\begin{itemize}
\tightlist
\item
  𝛼𝑗𝐶𝑜𝑤𝑗
\end{itemize}

𝛼𝑗\textasciitilde 𝑁𝑜𝑟𝑚𝑎𝑙

\begin{itemize}
\tightlist
\item
  𝜀𝑖
\end{itemize}

, 𝜀𝑖

\textasciitilde 𝑁𝑜𝑟𝑚𝑎𝑙(0, 𝜎2)

• Don't worry too much about the notation details. The most important
part is that ``Cow'' is random, while ``Treatment'' and ``Day'' are
fixed.

• We will not estimate coefficients for the different cows. This model
treats each cow as having its own ``random intercept'', meaning that the
intercept, 𝛽0, gets adjusted by some amount for each individual cow.

• As we saw in the last set of notes, the purpose of this is to let the
model estimate the effects of the ``Treatment'' and ``Day'' variables,
while taking accounts of the fact that different cows will have
different overall mean infection rates.

\hypertarget{the-mixed-effects-model-in-jamovi}{%
\subsection{The mixed effects model in
jamovi}\label{the-mixed-effects-model-in-jamovi}}

• To fit this model in jamovi, use ``Linear Models'', select ``Mixed
Model'', and then add the fixed predictors as factors and covariates and
random predictors as cluster variables:

• Note that Cow\_ID is the random effect. Treatment, Day, and their
interaction are fixed effects.

• Note also jamovi will also require you to specify the random effects.

• jamovi gives the usual parameter estimates output, as well as a
residual plot:

• Notice that ``Cow\_ID'' is not listed under parameter estimates; only
fixed coefficients are estimated. • Each cow has its own random
coefficient, modeled as having been drawn from a normal distribution. 17

Slope coding in ``Mixed Model''

• Notice the categorical predictor ``Treatment'' lists ``Effect'' as ``A
-- (B, A)''. jamovi is coding this slope as the difference between
Treatment A and the ``mean'' of Treatments A and B.

• So, if we were looking at Treatment B, we'd apply a slope of -0.513.

\hypertarget{applied-example-the-liking-gap}{%
\subsection{Applied example: ``The liking
gap''}\label{applied-example-the-liking-gap}}

• The journal Psychological Science published many studies in which data
are publicly available.

• The remaining slides reproduce results from a recent study published
in Psychological Science on the difference between how much individuals
``like'' other people and how much they perceive others ``like'' them.

• The paper is available at: https://doi.org/10.1177\%2F0956797618783714

• The basic setup is that volunteers were paired up (each pair is called
a ``dyad'') and directed to have a conversation for five minutes

• After this, participants rated their partners on some survey questions
that the authors take as a measure of liking the other person.

• Participants also rated how much they thought they were liked.

• The study is looking for a ``gap'' (i.e.~difference) between
volunteers' self-perception of how much their partners liked them, and
how much their partners actually liked them.

The liking gap example

• From the results section of the paper:

𝐿𝑖𝑘𝑖𝑛𝑔 𝐼𝑛𝑑𝑒𝑥𝑖 = 𝛽0 + 𝛽1𝑠𝑒𝑙𝑓 𝑜𝑡ℎ𝑒𝑟 + 𝛽2𝐷𝑎𝑦 + 𝛼𝑗𝑝𝑖𝑑𝑗 + 𝛾𝑘𝑑𝑖𝑑𝑘 + 𝜀𝑖

𝛼𝑗(𝑘)\textasciitilde 𝑁𝑜𝑟𝑚𝑎𝑙

, 𝛾𝑘

\textasciitilde 𝑁𝑜𝑟𝑚𝑎𝑙

, 𝜀𝑖

\textasciitilde 𝑁𝑜𝑟𝑚𝑎𝑙(0, 𝜎2)

• The model statement starts looking complicated. We have two different
random effects, one nested in the other.

• The 𝛼's and 𝛾's are modeled as random values.

• We have two fixed effects: self\_other and Day. Their coefficients
will

𝐿𝑖𝑘𝑖𝑛𝑔 𝐼𝑛𝑑𝑒𝑥𝑖 = 𝛽0 + 𝛽1𝑠𝑒𝑙𝑓 𝑜𝑡ℎ𝑒𝑟 + 𝛼𝑗𝑝𝑖𝑑𝑗 + 𝛾𝑘𝑑𝑖𝑑𝑘 + 𝜀𝑖

𝛼𝑗(𝑘)\textasciitilde 𝑁𝑜𝑟𝑚𝑎𝑙

, 𝛾𝑘

\textasciitilde 𝑁𝑜𝑟𝑚𝑎𝑙

, 𝜀𝑖

\textasciitilde 𝑁𝑜𝑟𝑚𝑎𝑙(0, 𝜎2)

• The random effect variances, 𝜎2 and 𝜎2, are estimated in the mixed 𝛼 𝛾
model. These variances could be interpreted, but we will stick to
interpreted the fixed effects in this class.

Adding an interaction\ldots{}

• The next section of the paper looks at personality variables as
predictors:

In jamovi

• We'll stop here. The paper goes on through many additional studies,
and the data are all available via the supplemental materials link.

\hypertarget{part-3-worked-example}{%
\section{Part 3: Worked example}\label{part-3-worked-example}}

• This set of notes on mixed models covers the analysis from a 2017
Psychological Science paper by Goudeau and Croizet, titled

``Hidden Advantages and Disadvantages of Social Class: How Classroom
Settings Reproduce Social Inequality by Staging Unfair Comparison''

• This paper presents three studies on one topic. These notes cover
studies 1 and 3; study 2 is reserved for a homework assignment.

• This study was performed in France. The data are available at
https://osf.io/rkj7y/ and the data file has a codebook sheet that
defines the variables:

• In each study, 6th grade students are given a challenging reading
comprehension assignment.

• Performance on the assignment is the dependent variable.

• The researchers investigate whether students' performance is
associated with their awareness of their classmates' performance, and
whether this association can be moderated by students' awareness of the
different levels of preparation given to different students.

Paper abstract

\hypertarget{study-1}{%
\subsection{Study 1}\label{study-1}}

The variables used in this study are:

• Performance (response) • Visibility condition (fixed effect) • Social
class (fixed effect) • Visibility X Social class interaction (fixed
effect) • School (random effect) • Classroom, nested within School
(random effect)

Study 1 model

𝑃𝐸𝑅𝐹𝑖 = 𝛽0 + 𝛽1𝑋1 + 𝛽2𝑃𝐶𝑆 + 𝛽3𝑋1 ∗ 𝑃𝐶𝑆 + 𝛼𝑗 𝑘 𝐶𝐿𝐴𝑆𝑆𝐸𝑗 𝑘 +
𝛾𝑖𝐸𝑇𝐴𝐵𝐿𝐼𝑆𝑆𝐸𝑀𝐸𝑁𝑇𝑘 + 𝜀𝑖

𝛼𝑗\textasciitilde 𝑁𝑜𝑟𝑚𝑎𝑙

, 𝛾𝑖

\textasciitilde 𝑁𝑜𝑟𝑚𝑎𝑙

, 𝜀𝑖

\textasciitilde 𝑁𝑜𝑟𝑚𝑎𝑙(0, 𝜎2)

• Refer to the codebook for variable definitions

• Note that in this study, subjects (students) are only measured once.
The repeated measurements are on classrooms and schools.

• Also note that PCS compares working class (PCS=1) to upper class
(PCS=3.)

Study 1 in JMP

• Note that the response variable, PERF, is score on a reading
comprehension test, on a scale of 0 to 20 points.

Notes on Study 1 random effects output

• ``The random effects covariance parameter estimates'' table shows that
there is much more residual (error) variance than there is variance
across classrooms or across schools.

• One very odd result: the classroom variance estimate is negative! But
variances cannot be negative.

• If we had deselected ``unbounded variance estimates'' under Fit Model,
this would be zeroed out. It is a strange quirk of maximum likelihood
variance estimation that negative estimates are possible. We can ignore
this, and treat the classroom variance as just very small.

• We see strong ``main effects'' for visibility condition (X1), social
class (PCS), and their interaction.

• A general principle for models with strong interactions is that our
interpretation should focus on the interactions.

• Here, the interaction coefficient is -2.69, and the coefficient for X1
is 1.12. • For the Effect of X1''-1 - 1'' refers to the ``differences
not visible'' condition, in which students do not raise their hands when
they have the answer. • For the Effect of PCS ``3 - 1'' refers to change
in score for upper class students vs.~working class students. • So, the
``effect'' of visibility seems to apply to working class students but
not to upper class students, since --2.69 and 1.12 nearly cancel each
other out.

• Here is the plot reported in the paper, and the same plot in JMP:

• The practice of using a bar plot to show means is common, but flawed.
The bars don't mean anything; all they do is go up to the means. • Here
is what this looks like using boxplots instead.

• Notice that this plot shows variability in the data, where the bar
plot does not.

\hypertarget{study-3}{%
\subsection{Study 3}\label{study-3}}

• In Study 2, social class is not used. Rather, some students are given
better preparation for the reading comprehension test than others.

• Similar results are found: those with worse preparation perform more
poorly when students are told to raise their hands after determining the
answer.

• In Study 3, the authors attempt to ``undo'' this effect by informing
the class that some students were given better preparation than others.
Half the classrooms are made aware of this; the other half are not.

• The variables in this study are almost the same as in study 1, with
these changes:

• X1 is ``context'': -1 = awareness of the disadvantage 1 = unawareness
of the disadvantage

• X2 is ``level of familiarity'' with the reading material, based on
intentional preparation given by the researchers: -1 = high level 1 =
low level

Study 3 model

𝑃𝐸𝑅𝐹𝑖 = 𝛽0 + 𝛽1𝑋1 + 𝛽2𝑋2 + 𝛽3𝑋1 ∗ 𝑋2 + 𝛼𝑗𝐶𝐿𝐴𝑆𝑆𝐸𝑗 + 𝜀𝑖

𝛼𝑗\textasciitilde 𝑁𝑜𝑟𝑚𝑎𝑙

, 𝜀𝑖

\textasciitilde 𝑁𝑜𝑟𝑚𝑎𝑙(0, 𝜎2)

• In this data set, there is no variable for ``school'', only one for
``classroom''. This isn't explained in the paper.

• As with before, the variance across classrooms is very small relative
to the error variance.

• The largest overall effect is X2: level of familiarity with the
material. Students who are better prepared score higher.

• The negative interaction shows that the difference in performance
between those with higher vs.~low familiarity is lower when students are
aware of the difference in familiarity.

• The interaction of -5.81 just about cancels out the estimated slope
for X1{[}-1{]} of 4.24.

• This shows that there is very little difference in scores between
students with high preparation who are and are not aware of the
advantage.

• It helps to look at the data.

• The generic ``X1'' and X2'' have been replaced with more meaningful
titles:

• And here are the bar plots shows in the paper.

• Again, the boxplots show more information.

• In this case, the boxplots reveal a potentially concerned ``ceiling
effect'': many students earned the maximum possible score. This can't be
seen from the bar plot. 24

\bookmarksetup{startatroot}

\hypertarget{chapter-8-minding-the-gap-between-science-and-statistics}{%
\chapter{Chapter 8: Minding the gap between science and
statistics}\label{chapter-8-minding-the-gap-between-science-and-statistics}}

• These notes cover some general problems that come up in applied data
analysis. Many are issues that have come to light recently.

\hypertarget{the-replication-crisis}{%
\section{The ``replication crisis''}\label{the-replication-crisis}}

• ``The replication crisis'' refers to a recent realization in many
areas of science that previously published results often fail to
replicate.

• Arguably, this started with Daryl Bem's 2011 paper ``Feeling the
Future'', published in the Journal of Personality and Social Psychology.

• This paper used standard statistical tools to show strong evidence for
pre-cognition, a.k.a. ESP. Many scientists were bothered by this,
because they did not believe in ESP but they did believe in the
statistical methods used in this paper!

• 2015's ``Reproducibility Project: Psychology'' by the Center for Open
Science found that a large number of published experimental results in
top Psychology journals failed to replicate.

• Similar studies in Cancer Biology, Medicine, Economics, Marketing, and
Sports Science have found high rates of non-replication.

• Why do so many studies fail to replicate? There are many reasons, and
statistical analysis plays a prominent role. These notes cover the role
of statistics in the replication crisis.

• Many of the statistical issues surrounding the replication crisis
concern ``power''.

• Statistical power is the probability of rejecting a null hypothesis
(??0), in the case that ??0 is false.

• In terms of real-world effects, if power is high, then there is a good
chance of ``detecting'' an effect -- i.e.~of declaring statistical
significance.

• If power is low, then even if a real-world effect exists, the result
of a hypothesis test will likely be ``fail to reject'' ??0;
i.e.~non-significance.

• A ``Type I'' error occurs when a true ??0 is rejected. In other words,
if we declare a result ``statistically significant'' even though no
real-world effect exists, we are committing a Type I error. This is
sometimes called a ``false positive'' outcome.

• A ``Type II'' error occurs when a false ??0 is not rejected. In other
words, if we declare a result ``not significant'' even though a
real-world effect does exist, we are committing a Type II error. This is
sometimes called a ``false negative'' outcome.

• Low statistical power implies a high chance of a false negative.

• The ``true'' power of a statistical test is almost never known. To
calculate power, one must assume the ``true'' size of the effect being
studied.

• For instance, power to reject the null hypothesis that a new drug is
equally effective as a previous drug depends in part upon how different
the two drugs are in effectiveness. But if we knew that, we wouldn't
need to conduct a study!

• There has been research estimating average statistical power in
various research fields, and reason to believe that low power studies
are not uncommon. Which means that a lot of these replication failures
might be ``false negatives'', rather than the original studies being
``false positives''.

\hypertarget{some-history-on-nhst}{%
\section{Some history on ``NHST''}\label{some-history-on-nhst}}

• The form of hypothesis testing used today is sometimes called Null
Hypothesis Significance Testing (NHST). It combines what used to be two
different methods created by two ``camps''. The camps disagreed with one
another, and did not get along personally.

Ronald Fisher Jerzy Neyman and

• Introduced the null hypothesis and p-value

• On interpreting small p-values: ``Either an exceptionally rare chance
has occurred, or the theory of random distribution is not true''

• On the use of p \textless{} 0.05 as a standard of evidence: ``In order
to assert that a natural phenomenon is experimentally demonstrable we
need, not an isolated record, but a reliable method of procedure. In
relation to the test of significance, we may say that a phenomenon is
experimentally demonstrable when we know how to conduct an experiment
which will rarely fail to give us a statistically significant result''

• Introduced the alternative hypothesis, Type I and II errors, and
power.

• Reported only Type I and Type II error probabilities in testing;
e.g.~p-values of 0.001 and 0.04 would both be reported as falling under
a pre-specified ?? = 0.05 Type I error rate. A Type II error rate should
also be reported.

``We are inclined to think that as far as a particular hypothesis is
concerned, no test based upon a theory of probability can by itself
provide any valuable evidence of the truth or falsehood of a
hypothesis''

• NHST is a mix between the two approaches.

• Hypothesis testing comes from Neyman and Pearson. They did not believe
p-values should be interpreted directly as quantifying evidence. They
saw their procedure as simply a method for making a decision.

• The direct interpretation of p-values as quantifying the probability
of obtaining results at least as extreme, assuming the null hypothesis
is true, comes from Fisher. He did not use an alternative hypothesis,
and he did not accept formal power analysis.

\hypertarget{when-p-0.05-isnt-that-meaningful}{%
\section{When ``p \textless{} 0.05'' isn't that
meaningful}\label{when-p-0.05-isnt-that-meaningful}}

• We say results are ``statistically significant'' when we calculate a
p-value less than the ?? level of significance, which is commonly 0.05.

• So, ``significant'' results are the kind that would be unlikely to
occur by chance, if the null hypothesis were true.

• There are some issues here\ldots{}

• The reason small p-values are considered ``evidence'' for a research
hypothesis is that they are supposed to be unlikely to occur by chance
alone.

• If there is flexibility in how to conduct an analysis, then it becomes
more likely that small p-values will occur by chance.

• This has a derogatory name: ``P-hacking''.

• Less derogatory alternatives: ``Researcher degrees of freedom'', ``The
garden of forking paths''

• Examples of flexibility:

• Trying out different combinations of independent and dependent
variables • Trying out different models and testing methods • Redefining
variables (e.g.~averaging over different combinations of survey
responses, choosing different cut points for placing responses into
categories)

• Discarding / retaining outliers • Transforming variables • Collecting
more data than originally planned • Ceasing data collection earlier than
originally planned

Capitalizing upon flexibility, so that ??(?? \textless{} 0.05)
\textgreater{} 0.05

• Flexible practices are perfectly justifiable in many contexts

• But -- to interpret a p-value as ``the probability of obtaining
results at least this extreme, assuming the null is true'', there can be
no flexibility, unless accounted for using a correction
(e.g.~Bonferroni). Flexibility renders the classical p-value
interpretation invalid.

• Another way of putting it: if you report a p-value as ``the
probability of obtaining results this extreme, assuming the null is
true'', you are implicitly claiming you would have conducted the
identical analysis, even if the data had been different in any way.

• Perhaps ``??0 is false'' does not imply that the version of ``not
??0'' you have in mind is true. • In observational studies, we must
always think about possible confounders. Maybe there is a ``significant
association'' between variables for some reason that isn't being
considered. • In experimental studies, we must consider all the possible
effects of our interventions. Did the experiment produce ``significant''
results for a reason we didn't consider, e.g.~poor control, biased
question wording?

• Example: CSU has found that students who complete their Math and
Composition AUCC requirements during their first year earn higher grades
and have higher graduation rates than those who do not.

• Does this suggest that completing these requirements in the first year
results in higher grades and a higher chance of graduation?

• Could this ``statistically significant'' effect be due to confounding
factors? Is it even reasonable to suspect that students who complete
these classes during their first year should have identical grades and
graduation rates as those who don't? This is what the rejected ??0
states. Are we impressed that it is rejected?

• ``All we know about the world teaches us that the effects of A and B
are always different -- in some decimal place -- for any A and B. Thus
asking `Are the effects different?' is foolish'' - John Tukey

• The null can be used as a ``Straw Man'' that no one really believes.
Overturning a straw man null may not be that impressive.

• A good question to ask: ``would we expect this null to be true?''

• If we are performing an experiment, the null is that the treatment
does literally nothing. Is this common?

• If we are analyzing observational data and testing for ``significant
correlation'', the null is that there is precisely zero correlation at
the population level. Do we think this is possible?

• Psychology/Philosopher/Statistician Paul Meehl called this the ``crud
factor'': the extent to which everything is correlated with everything,
at least at some small level.

• But, there are times when the null is plausible. • In manufacturing
quality control analysis, the null is that ``everything is being
produced the normal way''. Defects show up as large deviations from this
observed null distribution. • In Ronald Fisher's ``The Lady Tasting
Tea'', the subject of the story claimed she could tell whether milk or
tea had been poured into a cup first, simply by tasting the result. The
null is that she couldn't tell. Seems plausible. • My personal rule is
that I am only interested in p-values when I think the null is
plausible.

• A significance test returns a binary outcome: the results are
significant, or they are non-significant.

• Binary outcomes can produce binary thinking: the temptation to think
``significant'' means ``real'' and ``non-significant'' means ``due to
chance''.

• Is coming to a binary choice even necessary? Why not just report a
point estimate and standard error or 95\% CI?

• If there is an actionable outcome, a binary choice might be necessary.

• Example: deciding whether to continue investing money into a research
program.

• If the analysis is being done for the purpose of decision making, then
a decision rule must be established, and ``significance'' can be such a
rule. If the analysis is being done for the purpose of conveying
information in data, I personally see no reason to add in a declaration
of ``significant'' or ``not significant''.

• Consider the t-test statistic:

?? =

???1 ? ??2?

??????. ?????????? ????

???1?

???2

• As sample size increases, standard error decreases, the t-test
statistic increases, and the p-value decreases.

• So, the bigger the sample size, the smaller the value of to achieve
statistical significance.

???1?

???2 that is needed

• Upshot: if ?? is larger, very small effects will be ``significant''.

``Women had a slightly higher percentage of transactions for which
positive feedback had been given in the year preceding the current
transaction (99.60\% for women and 99.58\% for men, P \textless{}
0.05)''

(http://advances.sciencemag.org/content/2/2/e1500599.full)

• Going back to Type I errors, here is the ``Type I error rate'':

?? ??0 ???? ????????)

• What if we're actually interested in the reverse?

?? ???????????? ??0)

• Classical hypothesis testing controls the probability of rejecting
??0, given ??0 is false, typically at 5\%.

• It says nothing about the probability ??0 is false, given that ??0 has
been rejected. ??(???????????? ??0 \textbar{} ??0 ??????????) ? ??(??0
?????????? \textbar{} ???????????? ??0)

• Upshot: don't imagine that a Type I error rate of 0.05 implies that
only 5\% of significant results you see should be Type I errors!

• Jacob Cohen:

What's wrong with NHST? Well, among many other things, it does not tell
us what we want to know, and we so much want to know what we want to
know that, out of desperation, we nevertheless believe that it does!
What we want to know is ``Given these data, what is the probability that
H0 is true?'' But as most of us know, what it tells us is ``Given that
H0 is true, what is the probability of these (or more extreme) data?'' -
The Earth is Round (p\textless0.05) (1994)

• ``Statistical tests, P values, confidence intervals, and power: a
guide to misinterpretations'' (2016) lists a large number of popular
misinterpretations. Some highlights\ldots{}

• ``The p-value is the probability that the test hypothesis is true; for
example, if a test of the null hypothesis gave P = 0.01, the null
hypothesis has only a 1 \% chance of being true; if instead it gave P =
0.40, the null hypothesis has a 40 \% chance of being true.''

• ``The p-value for the null hypothesis is the probability that chance
alone produced the observed association''

• ``A null-hypothesis p-value greater than 0.05 means that no effect was
observed, or that absence of an effect was shown or demonstrated.''

• ``Statistical significance is a property of the phenomenon being
studied, and thus statistical tests detect significance.''

• ``When the same hypothesis is tested in two different populations and
the resulting p-values are on opposite sides of 0.05, the results are
conflicting.''

(note: all of these are wrong)

\hypertarget{the-sampling-distribution-of-the-p-value}{%
\section{The sampling distribution of the
p-value}\label{the-sampling-distribution-of-the-p-value}}

• It is intuitive to think that larger p-values are more likely to occur
than smaller p-values when the null is true. • This is intuitive, but it
is false. In most testing scenarios, all p-values are equally likely
when the null is true. The distribution of p under ??0 is uniform:

• Simulation: https://csu-statistics.shinyapps.io/visualize\_power/

• When the null is false, the distribution of p is right skewed. The
greater the statistical power, the greater the skew.

• Here is the distribution of p for Cohen's d = 0.5 and n = 30, implying
power ? 75\%

• When null is false, smaller p-values are more likely than larger ones.

• Some people have the intuition that for weaker effects or lower power,
significant p-values should be close to 0.05.

• This is false; even for low power, very small p-values are more likely
than ``marginally significant'' p-values. Here is the distribution of p
for Cohen's d = 0.2 and n = 30, impying power ? 19\%

• Upshot: we should never see lots of p-values just below 0.05, even
under low power.

• But -- there are papers in which many studies are performed, all of
which produce p-values just below 0.05. There are bodies of published
research in which p- values just below 0.05 occur too frequently.

• This suggests some combination of ``publication bias'' and
``p-hacking''

• Formal test: ``P-curve'' (www.p-curve.com)

• ``P-curve'' takes a collection of p-values less than 0.05 and compares
them to the uniform distribution expected under ??0

• If p-values close to 0.05 occur too frequently, the p-curve is
consistent with a true ??0, despite all p-values being less than 0.05.

obvious effects? • Simmons, Simonsohn, and Leif conducted an Amazon
M-Turk study (n = 697) to estimate required sample size to detect a
variety of ``effects'' at ?? = 0.05. Examples:

• Men are taller than women (n = 12; i.e.~n = 6 per group) • People who
like spicy food eat more Indian food than people who don't like spicy
food (n = 52) • People who like eggs eat more egg salad than people who
don't like eggs (n = 96) • Smokers think smoking is less likely to kill
someone than do non-smokers (n = 288) • Men weigh more than women (n =
92) (!!!!!!!!!!!)

• ``Publication bias'' is the phenomenon by which statistically
significant results are more likely to be published in a journal than
results that are not statistically significant.

• There are many tools for trying to diagnose this (including P-curve)

• Test of Insufficient Variance: convert p-values to z-statistics.
Expected variance of z-statistics is 1. Variance less than 1 could
suggest ``missing'' studies.

• Plot effect size vs.~standard error. Variance in effect sizes should
decrease as standard error decreases, but effect size and standard error
should not be correlated.

• Correlation could suggest ``missing'' studies.

``Do the results hit you between the eyes?''

(z = 1.96 is the threshold for statistical significance)

• Despite the fact that large sample sizes are needed to detect what
seem like ``medium'' effects, we see lots of studies will small sample
sizes reporting significant effects.

• Also, significant effects from small sample sizes are always large.

• A likely culprit: the combination of publication bias and low power.

• Low power: a typical consequence of\ldots{}

• Small sample sizes • Small effects • Noisy or imprecise measurements •
Noisy or imprecise manipulations

• A nasty consequence of publication bias combined with low power:
published effect sizes are biased upward. The lower the power, the
greater the bias.

• A diagram of low power:

• Notice: ``the truth'' is NOT in the ``reject the null'' region. So
when power is low, ``the truth'' is not statistically significant!

• Low power =\textgreater{} wide CIs • Here, wide CI centered on true
effect =\textgreater{} not significant • For CI to exclude zero, sample
effect must greatly overestimate true effect

• Left histogram: sampling distribution of the Cohen's d statistic
(standardized diff. in means). Simulated so that power = 0.27 and
population d = 0.5. • Right histogram: same thing, but after removing
all d statistics that do not achieve statistical significance. • Upshot:
conditioning an unbiased estimator on p \textless{} 0.05 creates a
biased estimator. The lower the power, the greater the bias.

From Andrew Gelman:

• ``Post-hoc'' power analysis describes performing a power analysis on a
data set after having conducted a significance test.

• Sometimes researchers will get a non-significant result, and suspect
low power is the reason. So, they do a power analysis using the effect
size and standard error from the data, and find low power.

• THIS IS INVALID! If p \textgreater{} 0.05, then post-hoc power must be
lower than 50\%.

• So, ``non-significant'' results will always be identified as ``low
power'', post-hoc.

• R-Index: calculate observed power for each study. Compare average
power to proportion of studies showing significance. Lower observed
power could suggest missing studies.

e.g.~observed power = 0.6, 10 / 10 results significant; 6 / 10 expected
if power = 0.6.

• If a collection of studies all show significant results, then average
observed power must be greater than 50\%. But, it might not be much
greater.

• There are statistical problems that arise when only reporting
significant results.

• There is also a scientific problem: are non-significant results really
uninteresting?

• If the question is worth asking (``do I have evidence for this
substantive hypothesis?''), isn't the answer worth knowing?

• The practice of throwing away non-significant results goes hand in
hand with a false interpretation of NHST results: that non-significance
implies ``no effect''.

• p-value \textless{} 0.05 is commonly interpreted as evidence for an
effect.

• But, p-value \textgreater{} 0.05 should not necessarily be interpreted
as evidence for no effect.

• Example: suppose three studies (A, B, and C) all aimed to estimate a
standardized difference in means in terms of Cohen's d:

?? =

???1 ? ????

???2

• Suppose that for each study, a p-value is also computed, testing
against the null of ??0: ??1 ? ??2 = 0

• A 95\% CI for d is also constructed.

• In this example, all CIs contain zero, so all p-values exceed 0.05.
All three tests are consistent with ``no effect''.

• However, the first CI is also consistent with a very large effect,
which could be positive or negative. The third CI is consistent with no
effect or a very small effect . • But - the p-values are the same! In
all three cases, p = 0.51.

• The article ``Arthroscopic partial meniscectomy versus placebo surgery
for a degenerative meniscus tear: a 2-year follow-up of the randomized
controlled trial'' assesses the effectiveness of a surgical procedure
for treating a degenerative knee tear relative to a sham ``placebo''
surgery (!!!). https://ard.bmj.com/content/77/2/188

• The article finds a non-significant difference between treatment and
placebo, and interprets this as the treatment being ``no better'' than
placebo. Author conclude there is ``no evidence'' in favor of the
treatment.

• However, later in the paper the authors make a much stronger argument:
that the 95\% CI for the difference in means is fully below the minimum
clinically meaningful difference, which they established a priori:

• Note the sharp difference between these arguments:

o ``Surgery was not effective because the difference between surgery and
placebo was not statistically significant.''

o ``Surgery was not effective because the 95\% CI for the difference
between surgery and placebo fell entirely below the minimum clinically
significant difference.

• The first argument says ``the estimated difference is no bigger than
what would be expected by chance.'' The second says ``the largest
plausible value for the difference is still too small to be
interesting.''

\hypertarget{debates-over-statistical-signficance}{%
\section{Debates over statistical
signficance}\label{debates-over-statistical-signficance}}

• The use of statistical significance has always been controversial.

• Three recent high profile papers have argued for some different
viewpoints on statistical significance.

\hypertarget{redefine-statistical-significance}{%
\subsection{``Redefine statistical
significance''}\label{redefine-statistical-significance}}

• ``Redefine Statistical Significance'' (2017) calls for lowering the
standard threshold for significance to p \textless{} 0.005

• The argument: p \textless{} 0.05 is too easy to obtain from noise.

• This paper proposes labeling p-values between 0.005 and 0.05 as
``suggestive'', and p-values less than 0.005 as ``significant''.

• An exception: if the procedure is pre-registered, p \textless{} 0.05
can be labeled ``significant''. So a distinction is drawn between
exploratory and confirmatory data analyses.

\hypertarget{justify-your-alpha}{%
\subsection{``Justify your alpha''}\label{justify-your-alpha}}

• ``Justify Your Alpha'' (2017), written in response, calls for allowing
flexibility in alpha levels rather than defaulting to p \textless{}
0.05.

• The argument: alpha (a.k.a. the significance level) sets a trade-off
between Type I errors and Type II errors.

• Smaller values of alpha lower Type I error rates but increase Type II
error rates, and vice versa.

• The optimal trade-off will be different for different research fields.
FDA drug trials should not use the same trade-off as exploratory
research in brand new research fields.

\hypertarget{abandon-statistical-significance}{%
\subsection{``Abandon statistical
significance''}\label{abandon-statistical-significance}}

• ``Abandon Statistical Significance'' (2017) calls for the elimination
of thresholds entirely.

• The argument: ``significance'' is just a way of taking continuous
phenomena (e.g.~differences in means, probabilities, correlations) and
forcing them into one of two categories.

• Instead, why not report the evidence on its own terms? No need to
force it into an artificial and simplistic ``either / or'' distinction.

\hypertarget{proposed-reforms}{%
\section{Proposed reforms}\label{proposed-reforms}}

• Pre-registration and registered reports: data analysis plans are
stated ahead of time. This removes flexibility in analysis.

• With registered reports, data analysis plans are peer reviewed, and
papers can be accepted for publication before results are known. This
removes publication bias.

• Rewarding ``open'' practices.

• The Association for Psychological Science now does this using badges:

• ``Equivalence testing'': an alternative to ``accepting'' a null that
has not been rejected.

• Idea: establish a minimum effect size of interest (e.g.~``we're not
interested in this drug if it doesn't reduce blood pressure by at
least\ldots{}'')

• Make the null of the equivalence test be that the true effect is
smaller than the minimum effect size of interest.

• If the null is rejected, then observed results are ``equivalent'' to
the null insofar as they are too small to be interesting.

• Visualization of equivalence testing, using confidence intervals:

• Note that results can be both ``not significantly different'' and
``not significantly equivalent''.

• They can also be both ``significantly equivalent'' and ``significantly
different''!

• ``The New Statistics'' proposes that we emphasize confidence intervals
over p-values, as they are easier to understand and less noisy
(i.e.~they don't change as much across repeated samples)

• The Peer Reviewers' Openness Initiative calls on reviewers to require
open data, open methods, and code that will reproduce analyses, so that
reviewers can double check the analyses and results.

• The GRIM test, SPRITE test, and Statcheck are algorithms that check
for internal consistency of reported results. They provide a ``sanity
check'' that can detect potentially p-hacked data analyses.

• As stated at the outset, there is great controversy over the
appropriate use of statistical methods!

• Some wise words from participants in this controversy:

\begin{quote}
``We often hear it's too easy to obtain small p-values, yet replication
attempts find it difficult to get small p-values with preregistered
results. This shows the problem isn't p-values but failing to adjust
them for cherry picking, multiple testing, post- data subgroups and
other biasing selection effects.''
\end{quote}

-Deborah Mayo, ``Don't throw out the error control baby with the bad
statistics bathwater''

\begin{quote}
``It seems to me that statistics is often sold as a sort of alchemy that
transmutes randomness into certainty, an''uncertainty laundering'' that
begins with data and concludes with success as measured by statistical
significance \ldots{} the solution is not to reform p-values or to
replace them with some other statistical summary or threshold, but
rather to move toward a greater acceptance of uncertainty and embracing
of variation.
\end{quote}

Andrew Gelman, ``The problems with p-values are not just p-values''

\bookmarksetup{startatroot}

\hypertarget{brief-looks-at-major-topics-we-didnt-cover}{%
\chapter{Brief looks at major topics we didn't
cover}\label{brief-looks-at-major-topics-we-didnt-cover}}

These are topics that we didn't have time to cover in STAT 331, but
which come up often in applied statistical research. Brief descriptions
are provided, along with links if you want to learn more.

\hypertarget{formal-model-selection-tools}{%
\section{Formal model selection
tools}\label{formal-model-selection-tools}}

\hypertarget{aic-bic}{%
\subsection{AIC / BIC}\label{aic-bic}}

\hypertarget{backwards-and-forwards-selection}{%
\subsection{Backwards and forwards
selection}\label{backwards-and-forwards-selection}}

\hypertarget{penalized-regression-lasso-and-ridge}{%
\subsection{Penalized regression (LASSO and
Ridge)}\label{penalized-regression-lasso-and-ridge}}

\hypertarget{bayesian-statistics}{%
\section{Bayesian statistics}\label{bayesian-statistics}}

\hypertarget{probability-as-rational-degree-of-belief}{%
\subsection{Probability as rational degree of
belief}\label{probability-as-rational-degree-of-belief}}

\hypertarget{priors}{%
\subsection{Priors}\label{priors}}

\hypertarget{things-bayes-permits-that-classical-methods-dont}{%
\subsection{Things Bayes permits that classical methods
don't}\label{things-bayes-permits-that-classical-methods-dont}}

\hypertarget{non-parametric-methods}{%
\section{Non-parametric methods}\label{non-parametric-methods}}

\hypertarget{alternatives-for-t-tests-and-anova}{%
\subsection{Alternatives for t-tests and
ANOVA}\label{alternatives-for-t-tests-and-anova}}

\hypertarget{alternatives-for-regression}{%
\subsection{Alternatives for
regression}\label{alternatives-for-regression}}

\hypertarget{meta-analysis}{%
\section{Meta-analysis}\label{meta-analysis}}

\hypertarget{combining-lots-of-studies}{%
\subsection{Combining lots of studies}\label{combining-lots-of-studies}}

\hypertarget{heterogeneity}{%
\subsection{Heterogeneity}\label{heterogeneity}}

\hypertarget{interpretability}{%
\subsection{Interpretability}\label{interpretability}}

\hypertarget{regression-to-the-mean}{%
\section{Regression to the mean}\label{regression-to-the-mean}}

\bookmarksetup{startatroot}

\hypertarget{references}{%
\chapter*{References}\label{references}}
\addcontentsline{toc}{chapter}{References}

\markboth{References}{References}

\hypertarget{refs}{}
\begin{CSLReferences}{0}{0}
\end{CSLReferences}



\end{document}

<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.361">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>STAT 331 - 3&nbsp; Chapter 3: Assessing and improving model fit</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./Ch4_ANOVA.html" rel="next">
<link href="./Ch2_Model_Building.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

</head>

<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
      <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./Ch3_Model_Fit.html"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Chapter 3: Assessing and improving model fit</span></a></li></ol></nav>
      <a class="flex-grow-1" role="button" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
      </a>
      <button type="button" class="btn quarto-search-button" aria-label="" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">STAT 331</a> 
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Introduction to STAT 331: Intermediate Applied Statistical Methods</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Ch1_Review.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Chapter 1: Review of classical inference</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Ch2_Model_Building.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Chapter 2: Model building with linear regression</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Ch3_Model_Fit.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Chapter 3: Assessing and improving model fit</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Ch4_ANOVA.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Chapter 4: ANOVA-based methods</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Ch5_Categorical.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Chapter 5: Analyzing categorical data</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Ch6_GLMs.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Generalized Linear Models (GLMs)</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Ch7_Mixed.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Chapter 7: Mixed-effects models</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Ch8_Mind_the_Gap.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Chapter 8: Minding the gap between science and statistics</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">References</span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active" data-toc-expanded="99">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#part-1-assumptions-and-assumption-violations" id="toc-part-1-assumptions-and-assumption-violations" class="nav-link active" data-scroll-target="#part-1-assumptions-and-assumption-violations">Part 1: assumptions and assumption violations</a></li>
  <li><a href="#outline-of-notes" id="toc-outline-of-notes" class="nav-link" data-scroll-target="#outline-of-notes">Outline of notes:</a></li>
  <li><a href="#violating-model-assumptions" id="toc-violating-model-assumptions" class="nav-link" data-scroll-target="#violating-model-assumptions"><span class="header-section-number">3.1</span> Violating model assumptions</a>
  <ul class="collapse">
  <li><a href="#the-regression-model-and-what-it-assumes" id="toc-the-regression-model-and-what-it-assumes" class="nav-link" data-scroll-target="#the-regression-model-and-what-it-assumes"><span class="header-section-number">3.1.1</span> The regression model and what it assumes</a></li>
  </ul></li>
  <li><a href="#linearity" id="toc-linearity" class="nav-link" data-scroll-target="#linearity"><span class="header-section-number">3.2</span> Linearity</a>
  <ul class="collapse">
  <li><a href="#diagnosing-non-linearity" id="toc-diagnosing-non-linearity" class="nav-link" data-scroll-target="#diagnosing-non-linearity"><span class="header-section-number">3.2.1</span> Diagnosing non-linearity</a></li>
  <li><a href="#what-to-look-for-in-a-residual-plot" id="toc-what-to-look-for-in-a-residual-plot" class="nav-link" data-scroll-target="#what-to-look-for-in-a-residual-plot"><span class="header-section-number">3.2.2</span> What to look for in a residual plot</a></li>
  </ul></li>
  <li><a href="#normality-of-residuals" id="toc-normality-of-residuals" class="nav-link" data-scroll-target="#normality-of-residuals"><span class="header-section-number">3.3</span> Normality of residuals</a>
  <ul class="collapse">
  <li><a href="#the-qq-plot" id="toc-the-qq-plot" class="nav-link" data-scroll-target="#the-qq-plot"><span class="header-section-number">3.3.1</span> The QQ plot</a></li>
  <li><a href="#assessing-normality-with-a-qq-plot" id="toc-assessing-normality-with-a-qq-plot" class="nav-link" data-scroll-target="#assessing-normality-with-a-qq-plot"><span class="header-section-number">3.3.2</span> Assessing normality with a QQ plot</a></li>
  <li><a href="#limitations-of-qq-plots" id="toc-limitations-of-qq-plots" class="nav-link" data-scroll-target="#limitations-of-qq-plots"><span class="header-section-number">3.3.3</span> Limitations of QQ plots</a></li>
  <li><a href="#tests-for-normality-not-recommended" id="toc-tests-for-normality-not-recommended" class="nav-link" data-scroll-target="#tests-for-normality-not-recommended"><span class="header-section-number">3.3.4</span> Tests for normality (not recommended)</a></li>
  </ul></li>
  <li><a href="#the-homogeneity-of-variance-assumption" id="toc-the-homogeneity-of-variance-assumption" class="nav-link" data-scroll-target="#the-homogeneity-of-variance-assumption"><span class="header-section-number">3.4</span> The homogeneity of variance assumption</a>
  <ul class="collapse">
  <li><a href="#heterogeneity-of-variance" id="toc-heterogeneity-of-variance" class="nav-link" data-scroll-target="#heterogeneity-of-variance"><span class="header-section-number">3.4.1</span> Heterogeneity of variance</a></li>
  <li><a href="#the-residuals-vs-fitted-plot" id="toc-the-residuals-vs-fitted-plot" class="nav-link" data-scroll-target="#the-residuals-vs-fitted-plot"><span class="header-section-number">3.4.2</span> The residuals vs fitted plot</a></li>
  </ul></li>
  <li><a href="#influential-observations-outliers" id="toc-influential-observations-outliers" class="nav-link" data-scroll-target="#influential-observations-outliers"><span class="header-section-number">3.5</span> Influential observations (outliers)</a>
  <ul class="collapse">
  <li><a href="#cooks-distances" id="toc-cooks-distances" class="nav-link" data-scroll-target="#cooks-distances"><span class="header-section-number">3.5.1</span> Cook‚Äôs Distances</a></li>
  </ul></li>
  <li><a href="#multicollinearity" id="toc-multicollinearity" class="nav-link" data-scroll-target="#multicollinearity"><span class="header-section-number">3.6</span> (Multi)collinearity</a>
  <ul class="collapse">
  <li><a href="#why-dont-we-want-correlated-predictors" id="toc-why-dont-we-want-correlated-predictors" class="nav-link" data-scroll-target="#why-dont-we-want-correlated-predictors"><span class="header-section-number">3.6.1</span> Why don‚Äôt we want correlated predictors?</a></li>
  <li><a href="#collinearity-example-florida-election-data" id="toc-collinearity-example-florida-election-data" class="nav-link" data-scroll-target="#collinearity-example-florida-election-data"><span class="header-section-number">3.6.2</span> Collinearity example: Florida election data</a></li>
  <li><a href="#variance-inflation-factor-vif" id="toc-variance-inflation-factor-vif" class="nav-link" data-scroll-target="#variance-inflation-factor-vif"><span class="header-section-number">3.6.3</span> Variance inflation factor (VIF)</a></li>
  <li><a href="#when-should-we-worry-about-multicollinearity" id="toc-when-should-we-worry-about-multicollinearity" class="nav-link" data-scroll-target="#when-should-we-worry-about-multicollinearity"><span class="header-section-number">3.6.4</span> When should we worry about (multi)collinearity?</a></li>
  </ul></li>
  <li><a href="#part-2-improving-models" id="toc-part-2-improving-models" class="nav-link" data-scroll-target="#part-2-improving-models">Part 2: Improving models</a></li>
  <li><a href="#log-transformation" id="toc-log-transformation" class="nav-link" data-scroll-target="#log-transformation"><span class="header-section-number">3.7</span> Log transformation</a>
  <ul class="collapse">
  <li><a href="#why-log-transform" id="toc-why-log-transform" class="nav-link" data-scroll-target="#why-log-transform"><span class="header-section-number">3.7.1</span> Why log transform?</a></li>
  <li><a href="#same-again-with-log-base-2" id="toc-same-again-with-log-base-2" class="nav-link" data-scroll-target="#same-again-with-log-base-2"><span class="header-section-number">3.7.2</span> Same again, with log base 2</a></li>
  <li><a href="#log-transforming-right-skewed-data" id="toc-log-transforming-right-skewed-data" class="nav-link" data-scroll-target="#log-transforming-right-skewed-data"><span class="header-section-number">3.7.3</span> Log transforming right-skewed data</a></li>
  <li><a href="#interpreting-slope-as-a-change-in-outcome" id="toc-interpreting-slope-as-a-change-in-outcome" class="nav-link" data-scroll-target="#interpreting-slope-as-a-change-in-outcome"><span class="header-section-number">3.7.4</span> Interpreting slope as a % change in outcome</a></li>
  <li><a href="#log-transformation-applied-example" id="toc-log-transformation-applied-example" class="nav-link" data-scroll-target="#log-transformation-applied-example"><span class="header-section-number">3.7.5</span> Log transformation applied example</a></li>
  <li><a href="#log-transformation-in-y-vs.-in-x" id="toc-log-transformation-in-y-vs.-in-x" class="nav-link" data-scroll-target="#log-transformation-in-y-vs.-in-x"><span class="header-section-number">3.7.6</span> Log transformation in <span class="math inline">\(Y\)</span> vs.&nbsp;in <span class="math inline">\(X\)</span></a></li>
  </ul></li>
  <li><a href="#non-linearity" id="toc-non-linearity" class="nav-link" data-scroll-target="#non-linearity"><span class="header-section-number">3.8</span> Non-linearity</a>
  <ul class="collapse">
  <li><a href="#polynomial-review" id="toc-polynomial-review" class="nav-link" data-scroll-target="#polynomial-review"><span class="header-section-number">3.8.1</span> ‚ÄúPolynomial‚Äù review</a></li>
  <li><a href="#nd-degree-and-3rd-degree-polynomials" id="toc-nd-degree-and-3rd-degree-polynomials" class="nav-link" data-scroll-target="#nd-degree-and-3rd-degree-polynomials"><span class="header-section-number">3.8.2</span> <span class="math inline">\(2^{nd}\)</span> degree and <span class="math inline">\(3^{rd}\)</span> degree polynomials</a></li>
  <li><a href="#curvature-in-residuals" id="toc-curvature-in-residuals" class="nav-link" data-scroll-target="#curvature-in-residuals"><span class="header-section-number">3.8.3</span> Curvature in residuals</a></li>
  <li><a href="#example-florida-election-data" id="toc-example-florida-election-data" class="nav-link" data-scroll-target="#example-florida-election-data"><span class="header-section-number">3.8.4</span> Example: Florida election data</a></li>
  <li><a href="#example-florida-election-data-check-note" id="toc-example-florida-election-data-check-note" class="nav-link" data-scroll-target="#example-florida-election-data-check-note"><span class="header-section-number">3.8.5</span> Example: Florida election data: check note</a></li>
  </ul></li>
  <li><a href="#over-fitting" id="toc-over-fitting" class="nav-link" data-scroll-target="#over-fitting"><span class="header-section-number">3.9</span> Over-fitting</a></li>
  <li><a href="#back-to-basics-is-the-model-sensible" id="toc-back-to-basics-is-the-model-sensible" class="nav-link" data-scroll-target="#back-to-basics-is-the-model-sensible"><span class="header-section-number">3.10</span> Back to basics: is the model sensible?</a>
  <ul class="collapse">
  <li><a href="#if-you-just-want-to-make-predictions" id="toc-if-you-just-want-to-make-predictions" class="nav-link" data-scroll-target="#if-you-just-want-to-make-predictions"><span class="header-section-number">3.10.1</span> If you just want to make predictions</a></li>
  <li><a href="#if-you-want-to-interpret-slopes" id="toc-if-you-want-to-interpret-slopes" class="nav-link" data-scroll-target="#if-you-want-to-interpret-slopes"><span class="header-section-number">3.10.2</span> If you want to interpret slopes</a></li>
  <li><a href="#is-the-model-missing-something-important" id="toc-is-the-model-missing-something-important" class="nav-link" data-scroll-target="#is-the-model-missing-something-important"><span class="header-section-number">3.10.3</span> Is the model missing something important?</a></li>
  <li><a href="#what-would-you-like-to-control-for" id="toc-what-would-you-like-to-control-for" class="nav-link" data-scroll-target="#what-would-you-like-to-control-for"><span class="header-section-number">3.10.4</span> What would you like to ‚Äúcontrol‚Äù for?</a></li>
  <li><a href="#beware-the-kitchen-sink-approach" id="toc-beware-the-kitchen-sink-approach" class="nav-link" data-scroll-target="#beware-the-kitchen-sink-approach"><span class="header-section-number">3.10.5</span> Beware the ‚Äúkitchen sink‚Äù approach</a></li>
  <li><a href="#the-model-is-simpler-than-whats-being-modeled" id="toc-the-model-is-simpler-than-whats-being-modeled" class="nav-link" data-scroll-target="#the-model-is-simpler-than-whats-being-modeled"><span class="header-section-number">3.10.6</span> The model is simpler than what‚Äôs being modeled</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Chapter 3: Assessing and improving model fit</span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  

</header>

<section id="part-1-assumptions-and-assumption-violations" class="level2 unnumbered">
<h2 class="unnumbered anchored" data-anchor-id="part-1-assumptions-and-assumption-violations">Part 1: assumptions and assumption violations</h2>
</section>
<section id="outline-of-notes" class="level2 unnumbered">
<h2 class="unnumbered anchored" data-anchor-id="outline-of-notes">Outline of notes:</h2>
<ol type="1">
<li>Regression assumptions</li>
<li>Linearity</li>
<li>Normality of residuals</li>
<li>Homogeneity of variance</li>
<li>Influential observations</li>
<li>(Multi)collinearity</li>
<li>Log transformation</li>
<li>Non-linearity</li>
<li>Over-fitting</li>
<li>Back to basics: is the model sensible?</li>
</ol>
</section>
<section id="violating-model-assumptions" class="level2" data-number="3.1">
<h2 data-number="3.1" class="anchored" data-anchor-id="violating-model-assumptions"><span class="header-section-number">3.1</span> Violating model assumptions</h2>
<p>The previous notes outlined the fundamentals of specifying, fitting, and interpreting a linear regression model.</p>
<p>These notes focus on the assumptions that our models are based upon, and how we check to see if they are being violated.</p>
<p>If model assumptions are violated, DON‚ÄôT PANIC! There is nearly always a remedy. In these notes we will look at how to spot violations of assumptions. In the next set of notes we will look at ways to remedy these violations, and how to decide if they pose a serious problem to the usefulness of the model.</p>
<section id="the-regression-model-and-what-it-assumes" class="level3" data-number="3.1.1">
<h3 data-number="3.1.1" class="anchored" data-anchor-id="the-regression-model-and-what-it-assumes"><span class="header-section-number">3.1.1</span> The regression model and what it assumes</h3>
<p>Once again, here is the regression model:</p>
<p><span class="math display">\[
Y_i = \beta_0 + \beta_1x_{1i} + \beta_2x_{2i} + \dots + \beta_px_{pi} + \epsilon_i, \text{ where } \epsilon_i \sim Normal(0,\sigma)
\]</span></p>
<ul>
<li><p>This assumes:</p>
<ul>
<li><p>That the response variable is a linear (straight line) function of the predictor variables</p></li>
<li><p>That the residuals will be normally distributed</p></li>
<li><p>That the standard deviation of the residuals does not vary</p></li>
<li><p>That the residuals are independent</p></li>
</ul></li>
</ul>
</section>
</section>
<section id="linearity" class="level2" data-number="3.2">
<h2 data-number="3.2" class="anchored" data-anchor-id="linearity"><span class="header-section-number">3.2</span> Linearity</h2>
<p>Remember the ‚Äúsimple‚Äù (i.e.&nbsp;single predictor) regression model</p>
<p><span class="math display">\[
Y_i = \beta_0 + \beta_1x_{i} + \epsilon_i, \text{ where } \epsilon_i \sim Normal(0,\sigma)
\]</span></p>
<p>This is linear in that it fits a straight line to the two-dimensional data.</p>
<p>A two-predictor model would fit a flat plane to the three-dimensional data, and so on</p>
<p>Here‚Äôs a bad idea: fitting a linear model to non-linear data!</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/mod4_pt1_1.png" class="img-fluid figure-img" width="400"></p>
</figure>
</div>
<section id="diagnosing-non-linearity" class="level3" data-number="3.2.1">
<h3 data-number="3.2.1" class="anchored" data-anchor-id="diagnosing-non-linearity"><span class="header-section-number">3.2.1</span> Diagnosing non-linearity</h3>
<p>When running ‚ÄúLinear Regression‚Äù in jamovi, a ‚Äúresiduals by predicted‚Äù plot can be created by selecting ‚ÄúResidual plots‚Äù under ‚ÄúAssumption Checks‚Äù</p>
<p>The residuals are the differences between each observed values of the response variable and the value that the model predicts:</p>
<p><span class="math display">\[
residual_i = y_i - \hat{y}_i = y_i - (\hat{\beta}_0 + \hat{\beta}_1x_1 + \hat\beta_2x_2 + \dots)
\]</span></p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/mod4_pt1_2.png" class="img-fluid figure-img" width="383"></p>
</figure>
</div>
<p>For simple regression, this plot just looks like the regression plot with the line turned horizontally.</p>
<p>For multiple regression, there is no (two dimensional) ‚Äúregression plot‚Äù, so the residual plot will be very useful!</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/mod4_pt1_3.png" class="img-fluid figure-img"></p>
</figure>
</div>
<p>In this example, there is clear curvature in the data. A straight line model is not appropriate.</p>
<p>Here‚Äôs an example of what a linear relationship might look like:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/mod4_pt1_4.png" class="img-fluid figure-img"></p>
</figure>
</div>
<p>When there is non-linearity, you will see the residuals mostly on one side of zero, then on the other size of zero, then back again, and so on.</p>
<p>When there is linearity, the residuals should randomly fall on either side of zero.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/mod4_pt1_5.png" class="img-fluid figure-img"></p>
</figure>
</div>
</section>
<section id="what-to-look-for-in-a-residual-plot" class="level3" data-number="3.2.2">
<h3 data-number="3.2.2" class="anchored" data-anchor-id="what-to-look-for-in-a-residual-plot"><span class="header-section-number">3.2.2</span> What to look for in a residual plot</h3>
<p>We will look at many more examples of residual plots in these notes.</p>
<ul>
<li><p>We want a residual plot that appears to agree with the model assumptions:</p>
<ul>
<li><p>Straight line relationship between the predictors and response</p></li>
<li><p>Normally distributed random residuals around this line</p></li>
<li><p>Equal variance in residuals across line</p></li>
</ul></li>
</ul>
</section>
</section>
<section id="normality-of-residuals" class="level2" data-number="3.3">
<h2 data-number="3.3" class="anchored" data-anchor-id="normality-of-residuals"><span class="header-section-number">3.3</span> Normality of residuals</h2>
<p>The ‚Äúerror term‚Äù in a regression model is that <span class="math inline">\(+ \epsilon_i\)</span> on the end</p>
<p>When we write <span class="math inline">\(\epsilon_i \sim Normal(0, \sigma)\)</span>, we are saying that the errors (aka residuals) are normally distributed, with mean zero and some standard deviation <span class="math inline">\(\sigma\)</span>.</p>
<p>This can be assessed visually: run the regression plot the residuals to see if they appear roughly normally distributed.</p>
<section id="the-qq-plot" class="level3" data-number="3.3.1">
<h3 data-number="3.3.1" class="anchored" data-anchor-id="the-qq-plot"><span class="header-section-number">3.3.1</span> The QQ plot</h3>
<p>When fitting a model using ‚ÄúLinear Regression‚Äù in jamovi, there is an option to save residuals. This will create a new column with a residual for each row.</p>
<p>The option is under the last drop-down menu, under ‚ÄúSave‚Äù.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/mod4_pt1_6.png" class="img-fluid figure-img" width="278"></p>
</figure>
</div>
<p>To create a plot of the residuals, select ‚ÄúQ-Q plot of residuals‚Äù under ‚ÄúAssumption Checks‚Äù in ‚ÄúLinear Regression‚Äù</p>
<p>The Normal Quantile plot is also known as the QQ plot, for ‚Äúquantile quantile‚Äù.</p>
<p>It is easier to assess normality with a QQ plot than with a histogram.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/mod4_pt1_7.png" class="img-fluid figure-img" width="274"></p>
</figure>
</div>
</section>
<section id="assessing-normality-with-a-qq-plot" class="level3" data-number="3.3.2">
<h3 data-number="3.3.2" class="anchored" data-anchor-id="assessing-normality-with-a-qq-plot"><span class="header-section-number">3.3.2</span> Assessing normality with a QQ plot</h3>
<p>On Canvas under Simulations there is a ‚ÄúQQ plot generator‚Äù app.</p>
<p>This app allows you to manipulate a distribution, sample from it, and then view a histogram next to a QQ plot. It is meant to give you an idea of how QQ plots work.</p>
<p>By default, it draws data from a normal distribution. But, you can add ‚Äúskewedness‚Äù or ‚Äúpeakedness‚Äù (aka kurtosis) to make the distribution non-normal. You can also adjust sample size.</p>
<p><img src="images/mod4_pt1_8.png" class="img-fluid"></p>
<p>A QQ plot shows you how much the distribution of your data ‚Äúagree‚Äù with a normal distribution.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/mod4_pt1_9.png" class="img-fluid figure-img" width="323"></p>
</figure>
</div>
<p>The horizonal axis gives the distribution data would follow if it were perfectly normal.</p>
<p>The vertical axis gives the distribution your data actually follows.</p>
<p>The diagonal line shows perfect agreement between the two.</p>
<div class="quarto-figure quarto-figure-left">
<figure class="figure">
<p><img src="images/mod4_pt1_10.png" class="img-fluid figure-img" width="312"></p>
</figure>
</div>
<div class="quarto-figure quarto-figure-right">
<figure class="figure">
<p><img src="images/mod4_pt1_11.png" class="img-fluid figure-img" width="356"></p>
</figure>
</div>
<p>The big advantage of the QQ plot vs.&nbsp;the histogram is that very often data that come from a normal distribution don‚Äôt look normal, especially if the sample size is small.</p>
<p>In this case, the histogram isn‚Äôt clearly normal. But, on the QQ plot the data are close to the line.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/mod4_pt1_12.png" class="img-fluid figure-img"></p>
</figure>
</div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/mod4_pt1_13.png" class="img-fluid figure-img"></p>
</figure>
</div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/mod4_pt1_14.png" class="img-fluid figure-img"></p>
</figure>
</div>
<p>Notice that the data still veer from the diagonal line to some extent, even though we know for a fact they came from a normal distribution.</p>
</section>
<section id="limitations-of-qq-plots" class="level3" data-number="3.3.3">
<h3 data-number="3.3.3" class="anchored" data-anchor-id="limitations-of-qq-plots"><span class="header-section-number">3.3.3</span> Limitations of QQ plots</h3>
<p>As you can see from the app, sometimes data that come from a normal distribution don‚Äôt sit right on the line.</p>
<p>Sometimes data that come from a skewed distribution look similar to data that come from a normal distribution</p>
<p>It‚Äôs easier to assess normality when sample sizes are larger.</p>
<p>As it turns out, the assumption of normality is not vital to the validity of a regression model. If the QQ plot is vague, you‚Äôre probably fine. We only worry when we see extreme non-normality.</p>
</section>
<section id="tests-for-normality-not-recommended" class="level3" data-number="3.3.4">
<h3 data-number="3.3.4" class="anchored" data-anchor-id="tests-for-normality-not-recommended"><span class="header-section-number">3.3.4</span> Tests for normality (not recommended)</h3>
<p>There are statistical tests, such as ‚ÄúShapiro-Wilks‚Äù or ‚ÄúKolmgorov-Smirnov‚Äù, for which the null hypothesis is that the data come from some specified distribution, like the normal.</p>
<p>Rejecting this null means that the data ‚Äúsignificantly‚Äù disagree with the assumption of normality.</p>
<p>I do not recommend using this test. The problem is that, when the sample size is large enough, even small deviations from normality will be statistically significant. But small deviations from normality are OK. Only major deviations are concerning.</p>
</section>
</section>
<section id="the-homogeneity-of-variance-assumption" class="level2" data-number="3.4">
<h2 data-number="3.4" class="anchored" data-anchor-id="the-homogeneity-of-variance-assumption"><span class="header-section-number">3.4</span> The homogeneity of variance assumption</h2>
<p>Back to the error term:</p>
<p><span class="math inline">\(\epsilon_i \sim Normal(0, \sigma)\)</span></p>
<p>Notice that <span class="math inline">\(\sigma\)</span> is just one number. This suggests that the standard deviation of the residuals should be the same across all values of predictor variables. In other words, there is homogeneity of variance.</p>
<p>Another name for this is ‚Äúhomoscedasticity‚Äù. If this assumption is violated, then we have ‚Äúheteroscedasticity‚Äù.</p>
<section id="heterogeneity-of-variance" class="level3" data-number="3.4.1">
<h3 data-number="3.4.1" class="anchored" data-anchor-id="heterogeneity-of-variance"><span class="header-section-number">3.4.1</span> Heterogeneity of variance</h3>
<p>To show heterogeneity of variance, I‚Äôll simulate data that is a function of an X variable, plus random values from a normal distribution with standard deviation equal to X.</p>
<p>Thus, the standard deviation of residuals will get bigger as X gets bigger:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/mod4_pt1_15.png" class="img-fluid figure-img" width="256"></p>
</figure>
</div>
</section>
<section id="the-residuals-vs-fitted-plot" class="level3" data-number="3.4.2">
<h3 data-number="3.4.2" class="anchored" data-anchor-id="the-residuals-vs-fitted-plot"><span class="header-section-number">3.4.2</span> The residuals vs fitted plot</h3>
<p>Here is the regression plot and residual plot when this simulated variable (called ‚ÄúW‚Äù here) is the response and X is the predictor:</p>
<p><img src="images/mod4_pt1_16.png" class="img-fluid"></p>
<p>Notice that the residuals are more spread out for larger X</p>
<p>We also see ‚Äúheavy tails‚Äù when plotting the residuals with a histogram and QQ plot:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/mod4_pt1_17.png" class="img-fluid figure-img" width="332"></p>
</figure>
</div>
<p>Heavy tails refers to a distribution with outliers on both ends.</p>
<p>This shows up on the QQ plot as the residuals being too flat in the middle and then curving out on both ends.</p>
</section>
</section>
<section id="influential-observations-outliers" class="level2" data-number="3.5">
<h2 data-number="3.5" class="anchored" data-anchor-id="influential-observations-outliers"><span class="header-section-number">3.5</span> Influential observations (outliers)</h2>
<p>Outliers in regression can be seen on a residual plot, or on a QQ plot, or on just a regular plot of the data.</p>
<p>Example: the Florida election data.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/mod4_pt1_18.png" class="img-fluid figure-img" width="411"></p>
</figure>
</div>
<p>Outliers can ‚Äúpull‚Äù on the regression line, especially if they are far away from the mean of the predictor(s).</p>
<p>There are many statistics that assess influence. jamovi will calculate one of the most popular: a Cook‚Äôs Distance</p>
<section id="cooks-distances" class="level3" data-number="3.5.1">
<h3 data-number="3.5.1" class="anchored" data-anchor-id="cooks-distances"><span class="header-section-number">3.5.1</span> Cook‚Äôs Distances</h3>
<p>As we saw in the Florida election example, removing the outlier (Palm Beach County) had a substantial effect on the regression results.</p>
<p>The logic behind Cook‚Äôs Distance is to quantify what happens to the regression model when a single observation is removed. This is sometimes referred to as a ‚Äúleave one out‚Äù method.</p>
<p>Cook‚Äôs Distances quantify how much the predicted values of the response variable change when an observation is removed.</p>
<p>Recall that, in simple regression, the predicted values are the values on the regression line.</p>
<p>It is hard to interpret the actual values for Cook‚Äôs Distances. Values greater than 1 are often considered ‚Äúinfluential‚Äù.</p>
<p>The formula shows that it is based on the sum of the differences in predicted values between a model with the data point included and a model with it removed:</p>
<p><span class="math display">\[
\text{Cook's Distance for data point } "i" = D_i = \frac{\sum^n_{j=1}(\hat{y}_i - \hat{y}_{j(i)})^2}{MSE * p}
\]</span></p>
<p>Where <span class="math inline">\(\hat{y}_{j(i)}\)</span> is the predicted value of the response variable when the model is re- fit with the <span class="math inline">\(i^{th}\)</span> data point removed, and <span class="math inline">\(p\)</span> is the number of predictor variables in</p>
<p>A Cook‚Äôs Distance is calculated for every data point. The option to do this in jamovi is under ‚ÄúSave‚Äù in ‚ÄúLinear Regression‚Äù. This creates a new column with a Cook‚Äôs distance for each row.</p>
<p>The saved Cook‚Äôs Distances can then be plotted. Any possibly influential points will usually stand out clearly from the rest.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/mod4_pt1_19.png" class="img-fluid figure-img" width="393"></p>
</figure>
</div>
<p>To quickly narrow in on the influential counties, we can filter out all the small Cook‚Äôs distances.</p>
<p>After implementing the filter, we can see that all rows which do not meet the criteria of the filter are excluded.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/mod4_pt1_20.png" class="img-fluid figure-img" width="363"></p>
</figure>
</div>
<p>Only rows 13 and 50 should be highlighted for Dade and Palmbeach county which have cook‚Äôs distances of 1.983 and 3.786.</p>
</section>
</section>
<section id="multicollinearity" class="level2" data-number="3.6">
<h2 data-number="3.6" class="anchored" data-anchor-id="multicollinearity"><span class="header-section-number">3.6</span> (Multi)collinearity</h2>
<p>In regression analysis, we want our predictor variables to be correlated with the response variable.</p>
<p>But we don‚Äôt want our predictor variables to be (highly) correlated with one another!</p>
<p>When two predictor variables are highly correlated, we say our model has ‚Äúcollinearity.‚Äù</p>
<p>When more than two predictor variables are mutually highly correlated, we say our model as ‚Äúmulticollinearity‚Äù.</p>
<section id="why-dont-we-want-correlated-predictors" class="level3" data-number="3.6.1">
<h3 data-number="3.6.1" class="anchored" data-anchor-id="why-dont-we-want-correlated-predictors"><span class="header-section-number">3.6.1</span> Why don‚Äôt we want correlated predictors?</h3>
<p>To understand why we don‚Äôt want correlated predictors, recall that multiple regression models estimate the association between each individual predictor variable and the response, <span class="math inline">\(\textit{while holding all other predictor variables constant}\)</span>.</p>
<p>This can be thought of as asking ‚Äúwhat is the difference in the response variable when we observe data points that have different values of one predictor but the same values of the other predictors?‚Äù</p>
<p>If <span class="math inline">\(Y\)</span> is the response and <span class="math inline">\(x_1\)</span> and <span class="math inline">\(x_2\)</span> are predictors, we want to know how different <span class="math inline">\(Y\)</span> is when <span class="math inline">\(x_1\)</span> values differ but <span class="math inline">\(x_2\)</span> values are the same, or vice versa.</p>
<p>But, if <span class="math inline">\(x_1\)</span> and <span class="math inline">\(x_2\)</span> are highly correlated, then we don‚Äôt get to observe cases where values of one variable differ substantially while values of the other are the same! Consider instances of no correlation vs.&nbsp;heavy correlation:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/mod4_pt1_21.png" class="img-fluid figure-img"></p>
</figure>
</div>
<p>When <span class="math inline">\(x_1\)</span> and <span class="math inline">\(x_2\)</span> are uncorrelated, we see lots of instances of <span class="math inline">\(x_1\)</span> values differing a lot when <span class="math inline">\(x_2\)</span> values are equal.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/mod4_pt1_22.0.png" class="img-fluid figure-img" width="369"></p>
</figure>
</div>
<p>When <span class="math inline">\(x_1\)</span> and <span class="math inline">\(x_2\)</span> are highly correlated, we never see instances where <span class="math inline">\(x_2\)</span> values are equal but <span class="math inline">\(x_1\)</span> values are highly correlated.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/mod4_pt1_22.5.png" class="img-fluid figure-img" width="337"></p>
</figure>
</div>
<p>The upshot is that, when <span class="math inline">\(x_1\)</span> and <span class="math inline">\(x_2\)</span> are highly correlated, the regression procedure has a difficult time distinguishing between the ‚Äúeffect‚Äù of <span class="math inline">\(x_1\)</span> on <span class="math inline">\(Y\)</span> and the ‚Äúeffect‚Äù of <span class="math inline">\(x_2\)</span> on <span class="math inline">\(Y\)</span>.</p>
<p>Extreme example: if we had degrees Celsius and degrees Fahrenheit as predictors in the same model, it would be impossible to tell their effects apart. You can‚Äôt change Celsius while holding Fahrenheit constant!</p>
<p>The practical consequence is that the standard errors for the slopes of (multi)collinear predictors are much larger than they would be if it were not for the (multi)collinearity.</p>
<p>If two or more predictors are perfectly correlated (<span class="math inline">\(r=1\)</span>), then the model cannot be fit and jamovi produces an error:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/mod4_pt1_23.png" class="img-fluid figure-img" width="314"></p>
</figure>
</div>
<p>Here, <span class="math inline">\(X1\)</span> and <span class="math inline">\(X2\)</span> are perfectly correlated. jamovi cannot estimate a slope for <span class="math inline">\(X2\)</span>.</p>
</section>
<section id="collinearity-example-florida-election-data" class="level3" data-number="3.6.2">
<h3 data-number="3.6.2" class="anchored" data-anchor-id="collinearity-example-florida-election-data"><span class="header-section-number">3.6.2</span> Collinearity example: Florida election data</h3>
<p>In the Florida election data, we used total votes for each county as our predictor variable.</p>
<p>There is another variable called ‚ÄúTotal_Reg‚Äù. This is the total number of registered voters in each county.</p>
<p>Unsurprisingly, Total_Votes and Total_Reg are highly correlated:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/mod4_pt1_24.png" class="img-fluid figure-img" width="355"></p>
</figure>
</div>
<p>If we run two separate simple regression models, we get very similar results:</p>
<p><span class="math display">\[
Buchanin_i = \beta_0 + \beta_1Total\_Votes_i + \epsilon_i
\]</span></p>
<div class="quarto-figure quarto-figure-left">
<figure class="figure">
<p><img src="images/mod4_pt1_25.png" class="img-fluid figure-img" width="328"></p>
</figure>
</div>
<p><span class="math display">\[
Buchanin_i = \beta_0 + \beta_1Total\_Reg_i + \epsilon_i
\]</span></p>
<div class="quarto-figure quarto-figure-left">
<figure class="figure">
<p><img src="images/mod4_pt1_27.png" class="img-fluid figure-img" width="315"></p>
</figure>
</div>
<p>But look what happens if we use Total_Votes and Total_Reg as predictors in the same model:</p>
<p><span class="math display">\[
Buchanin_i = \beta_0 + \beta_1Total\_Votes_i + \beta_2Total\_Reg_i + \epsilon_i
\]</span></p>
<div class="quarto-figure quarto-figure-left">
<figure class="figure">
<p><img src="images/mod4_pt1_27-01.png" class="img-fluid figure-img" width="312"></p>
</figure>
</div>
<p>Two important things to note:</p>
<p>P-values on slopes are much larger than for the individual models</p>
<p><span class="math inline">\(R^2\)</span> is larger than on either individual model!</p>
<p>Looking at the estimates and standard errors in all three models, we see that the standard errors are much larger in the multiple regression model. These estimates are ‚Äúunstable‚Äù ‚Äì their values will change a lot if the data change a little.</p>
<div class="quarto-figure quarto-figure-left">
<figure class="figure">
<p><img src="images/mod4_pt1_28.png" class="img-fluid figure-img" width="341"></p>
</figure>
</div>
<p>We know the association between Total_Reg are Buchanan is actually positive. But with so high a standard error, the slope for Total_Reg turned out negative!</p>
</section>
<section id="variance-inflation-factor-vif" class="level3" data-number="3.6.3">
<h3 data-number="3.6.3" class="anchored" data-anchor-id="variance-inflation-factor-vif"><span class="header-section-number">3.6.3</span> Variance inflation factor (VIF)</h3>
<p>(Multi)collinearity can be assessed using a ‚ÄúVariance Inflation Factor‚Äù, or VIF. A VIF is calculated for the <span class="math inline">\(j^{th}\)</span> predictor variable as:</p>
<p><span class="math display">\[
VIF_j = \frac{1}{1-R^2_j}
\]</span></p>
<p>Where <span class="math inline">\(R^2_j\)</span> is the <span class="math inline">\(R^2\)</span> from a regression model with predictor <span class="math inline">\(j\)</span> as the response variable and all other predictors still as predictors.</p>
<p>In the Florida election example, the VIF for Total_Votes can be found using the <span class="math inline">\(R^2\)</span> for the model:</p>
<p><span class="math display">\[
Total\_Votes_i = \beta_0 + \beta_1Total\_Reg_i + \epsilon_i
\]</span></p>
<div class="quarto-figure quarto-figure-left">
<figure class="figure">
<p><img src="images/mod4_pt1_29.png" class="img-fluid figure-img" width="268"></p>
</figure>
</div>
<p><br>
This <span class="math inline">\(R^2\)</span> is huge! Plugging it into the formula:</p>
<p><span class="math display">\[
VIF_{Total\_Votes}=\frac{1}{1-0.997} = 333.33
\]</span></p>
<p>Thankfully we don‚Äôt have to do this by hand. In jamovi, under ‚ÄúLinear Regression‚Äù select ‚ÄúCollinearity statistics‚Äù:</p>
<div class="quarto-figure quarto-figure-left">
<figure class="figure">
<p><img src="images/mod4_pt1_30.png" class="img-fluid figure-img" width="330"></p>
</figure>
</div>
<p>VIF &gt; 10 typically is considered large (note that this would imply <span class="math inline">\(R^2 = 0.9\)</span> between predictor variables).</p>
<p>The most obvious thing to do in the presence of (multi)collinearity is to remove one or more correlated predictor variable. From a scientific standpoint, you also may not want highly correlated predictors in the same model.</p>
</section>
<section id="when-should-we-worry-about-multicollinearity" class="level3" data-number="3.6.4">
<h3 data-number="3.6.4" class="anchored" data-anchor-id="when-should-we-worry-about-multicollinearity"><span class="header-section-number">3.6.4</span> When should we worry about (multi)collinearity?</h3>
<p>(Multi)collinearity is a potentially huge problem if the goal of the regression model is to interpret the estimated slopes.</p>
<p>This is because it increases the standard error of these slopes, making their values less reliable. Some people say it makes slopes ‚Äúunstable‚Äù.</p>
<p>It may also complicate the interpretation of slopes: you are trying to statistically ‚Äúhold constant‚Äù a predictor variable that doesn‚Äôt naturally stay constant when the other predictor varies. This isn‚Äôt necessarily a problem, but it is something to be aware of.</p>
<p>However, (multi)collinearity does not negatively impact the predicted values themselves. Remember that it didn‚Äôt hurt the <span class="math inline">\(R^2\)</span> value in the Florida election example. <span class="math inline">\(R^2\)</span> tells you how good your predictions are.</p>
<p>So, if the model is only for predicting, you probably don‚Äôt need to worry about using correlated predictor variables. Just beware when interpreting the slopes.</p>
</section>
</section>
<section id="part-2-improving-models" class="level2 unnumbered">
<h2 class="unnumbered anchored" data-anchor-id="part-2-improving-models">Part 2: Improving models</h2>
<p>In the the first part of chapter 3, we looked at some things that can go wrong in regression modeling, including:</p>
<pre><code>-    Non-linear relationships between predictor(s) and response

-   Non-normality of residuals

-   Non-constant (heterogeneous) variance of residuals

-   Influential outliers

-   Multicollinearity</code></pre>
<p>Now we‚Äôll look at some tools available for dealing with these problems.</p>
</section>
<section id="log-transformation" class="level2" data-number="3.7">
<h2 data-number="3.7" class="anchored" data-anchor-id="log-transformation"><span class="header-section-number">3.7</span> Log transformation</h2>
<p>Recall the regression model:</p>
<p><span class="math display">\[
Y_i = \beta_0 + \beta_1x_{1i} + \beta_2x_{2i} + \dots + \beta_px_{pi} + \epsilon_i, \text{ where } \epsilon_i \sim Normal(0,\sigma)
\]</span></p>
<p>Sometimes we can correct violations of model assumptions by applying a mathematical transformation to the response or predictor variables.</p>
<p>The most common transformation in Statistics is the log transformation:</p>
<p><span class="math display">\[
ln(x) = log_e(x)
\]</span></p>
<p><span class="math inline">\(ln(ùë•)\)</span> is the inverse function of <span class="math inline">\(e^ùë•\)</span>, where <span class="math inline">\(ùëí = 2.718 \dots\)</span></p>
<p>In other words, <span class="math inline">\(ln(e^x) = x\)</span></p>
<p>Example: <span class="math inline">\(ùëí^3 = 20.086; ln(20.086) = 3\)</span></p>
<p>So, the natural log of <span class="math inline">\(x\)</span> is the number you would have to raise <span class="math inline">\(e\)</span> to so that you‚Äôd get <span class="math inline">\(x\)</span>.</p>
<p>Note: in statistics, when we say ‚Äúlog‚Äù, we usually mean ‚Äúnatural log‚Äù. It turns out that the distinction is not very important. I‚Äôll say ‚Äúlog transform‚Äù</p>
<section id="why-log-transform" class="level3" data-number="3.7.1">
<h3 data-number="3.7.1" class="anchored" data-anchor-id="why-log-transform"><span class="header-section-number">3.7.1</span> Why log transform?</h3>
<ul>
<li><p>There are two main reasons for log transforming a variable:</p>
<ul>
<li><p>To correct for skew in data or residuals</p></li>
<li><p>To interpret increases in a variable as multiplicative rather than additive.</p></li>
</ul></li>
</ul>
<p>Both can be understood by recognizing an important property of logarithms; they ‚Äúturn addition into multiplication‚Äù</p>
<p><span class="math display">\[
log(ùê¥) + log(ùêµ) = log(ùê¥ùêµ)
\]</span></p>
<p>In this sense, logarithms turn addition into multiplication.</p>
<p>Example: suppose we have data for a skewed variable <span class="math inline">\(X_1\)</span>:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/mod4_pt2_1.png" class="img-fluid figure-img"></p>
</figure>
</div>
<p>Now we define <span class="math inline">\(x_2 = ln(x_1)\)</span>:</p>
<div class="quarto-figure quarto-figure-left">
<figure class="figure">
<p><img src="images/mod4_pt2_2.png" class="img-fluid figure-img" width="216"></p>
</figure>
</div>
<p>This is a toy ‚Äúdata set‚Äù. I chose <span class="math inline">\(x_1\)</span> so that <span class="math inline">\(x_2 = ln(x_1)\)</span> would just be the integers <span class="math inline">\(1\)</span> through <span class="math inline">\(10\)</span>.</p>
<p>Note: there is no more skew.</p>
<p>Also note: increasing <span class="math inline">\(x_2\)</span> by one unit results in multiplying <span class="math inline">\(x_1\)</span> by <span class="math inline">\(e\)</span>. Addition in <span class="math inline">\(x_2 = ln(x_1)\)</span> is the same thing as multiplication in <span class="math inline">\(x_1\)</span>.</p>
</section>
<section id="same-again-with-log-base-2" class="level3" data-number="3.7.2">
<h3 data-number="3.7.2" class="anchored" data-anchor-id="same-again-with-log-base-2"><span class="header-section-number">3.7.2</span> Same again, with log base 2</h3>
<p>Even simpler: define <span class="math inline">\(x_2\)</span> as log base <span class="math inline">\(2\)</span> of <span class="math inline">\(x_1\)</span>, i.e.&nbsp;<span class="math inline">\(log_2(x_1)\)</span></p>
<div class="quarto-figure quarto-figure-left">
<figure class="figure">
<p><img src="images/mod4_pt2_3.png" class="img-fluid figure-img" width="162"></p>
</figure>
</div>
<p>Now increasing <span class="math inline">\(x_2\)</span> by one unit is equivalent to multiplying <span class="math inline">\(x_1\)</span> by <span class="math inline">\(2\)</span>. Addition in <span class="math inline">\(x_2 = log_2(x_1)\)</span> is the same thing as multiplication in <span class="math inline">\(x_1\)</span>.</p>
</section>
<section id="log-transforming-right-skewed-data" class="level3" data-number="3.7.3">
<h3 data-number="3.7.3" class="anchored" data-anchor-id="log-transforming-right-skewed-data"><span class="header-section-number">3.7.3</span> Log transforming right-skewed data</h3>
<ul>
<li>Skewed data can be bad for regression, in that it can lead to:</li>
</ul>
<pre><code>-   Non-linear relationship between X and Y

-   Influential outliers

-   Non-normal residuals

-   Non-constant variance in residuals</code></pre>
<p>So a simple log transformation can sometimes go a long way toward making the regression model fit the better!</p>
<p>It is most common to log transform a response variable, because assumptions about residuals apply to <span class="math inline">\(Y\)</span>, not <span class="math inline">\(X\)</span>.</p>
<p>But if <span class="math inline">\(X\)</span> is skewed, the model can benefit from a log transformation of <span class="math inline">\(X\)</span>.</p>
<p>Bear in mind that log transformation will affect the interpretation of slope coefficients!</p>
<p>If <span class="math inline">\(X\)</span> is log transformed, then a one unit increase in <span class="math inline">\(ln(ùëã)\)</span> corresponds to multiplying <span class="math inline">\(X\)</span> by <span class="math inline">\(e \approx 2.72\)</span>. So the slope for <span class="math inline">\(ln(ùëã)\)</span> tells you how much <span class="math inline">\(Y\)</span> increases when <span class="math inline">\(X\)</span> is multiplied by <span class="math inline">\(2.72\)</span>. Or, even better, use log base <span class="math inline">\(2\)</span> and the slope will give how much <span class="math inline">\(Y\)</span> changes when <span class="math inline">\(X\)</span> is doubled.</p>
<p>If <span class="math inline">\(Y\)</span> is log transformed, then the interpretations of slopes get more complicated. Here‚Äôs the math, with the error term omitted for convenience:</p>
<p><span class="math display">\[
ln(y_i) = \beta_0 + \beta_1X_i
\]</span></p>
<p><span class="math display">\[
\therefore y_i = e^{\beta_0 + \beta_1X_i}
\]</span></p>
<p>Increase <span class="math inline">\(X\)</span> by <span class="math inline">\(1 \dots\)</span></p>
<p><span class="math display">\[
y_i^* = e^{\beta_0 + \beta_1(X_i + 1)} = e^{\beta_0 + \beta_1X_i} \cdot e^{\beta_1}
\]</span></p>
<p>So, when <span class="math inline">\(Y\)</span> is log transformed, a one unit increase in <span class="math inline">\(X\)</span> multiplies predicted <span class="math inline">\(Y\)</span> by <span class="math inline">\(e^{\beta_1}\)</span></p>
</section>
<section id="interpreting-slope-as-a-change-in-outcome" class="level3" data-number="3.7.4">
<h3 data-number="3.7.4" class="anchored" data-anchor-id="interpreting-slope-as-a-change-in-outcome"><span class="header-section-number">3.7.4</span> Interpreting slope as a % change in outcome</h3>
<p>Recall the heights vs.&nbsp;wages data from group project 1. The paper reported this estimated model:</p>
<p><span class="math display">\[
ln(\text{wage}) = \hat{\beta}_0 + 0.002\text{(Adult Height)} + 0.027\text{(Youth Height)} + 0.024\text{(Age)}
\]</span></p>
<p>So, when comparing two adults <span class="math inline">\(1\)</span> inch apart in height but with the same youth height and age predicted wage is multiplied by <span class="math inline">\(e^{0.027} = 1.027\)</span> for the taller adult.</p>
<p>Multiplying by <span class="math inline">\(1.027\)</span> can be thought of as increasing by <span class="math inline">\(2.7\%\)</span></p>
</section>
<section id="log-transformation-applied-example" class="level3" data-number="3.7.5">
<h3 data-number="3.7.5" class="anchored" data-anchor-id="log-transformation-applied-example"><span class="header-section-number">3.7.5</span> Log transformation applied example</h3>
<p>Here is the percent change formula:</p>
<p><span class="math display">\[
\%\text{ change (from A to B)} = \frac{B-A}{A}*100\%
\]</span></p>
<p>If B is <span class="math inline">\(1.027*\)</span>A, then</p>
<p><span class="math display">\[
\%\text{ change} = \frac{1.027*A - A}{A}*100 = \frac{0.027A}{A}*100 = 2.7\%
\]</span></p>
<p>So, when comparing two adults 1 inch apart in height but with the same youth height and age, predicted wage is <span class="math inline">\(2.7\%\)</span> higher for the taller adult.</p>
</section>
<section id="log-transformation-in-y-vs.-in-x" class="level3" data-number="3.7.6">
<h3 data-number="3.7.6" class="anchored" data-anchor-id="log-transformation-in-y-vs.-in-x"><span class="header-section-number">3.7.6</span> Log transformation in <span class="math inline">\(Y\)</span> vs.&nbsp;in <span class="math inline">\(X\)</span></h3>
<p>Remember that log transformation ‚Äúturns addition into multiplication‚Äù. So, to keep track of how log transforming <span class="math inline">\(Y\)</span> vs.&nbsp;log transforming <span class="math inline">\(X\)</span> affects your model:</p>
<p><span class="math display">\[
\text{log}(Y_i) = \beta_0 + \beta_1X_i + \epsilon_i
\]</span></p>
<p><span class="math display">\[
\text{vs.}
\]</span></p>
<p><span class="math display">\[
Y_i = \beta_0 + \beta_1\text{log}(X_i) + \epsilon_i
\]</span></p>
<p>If you log transform <span class="math inline">\(Y\)</span> but not <span class="math inline">\(X\)</span>, your model estimates the multiplicative change in predicted <span class="math inline">\(Y\)</span> for an additive change in <span class="math inline">\(X\)</span>.</p>
<p>If you log transform <span class="math inline">\(X\)</span> but not <span class="math inline">\(Y\)</span>, your model estimates the additive change in predicted <span class="math inline">\(Y\)</span> for a multiplicative change in <span class="math inline">\(X\)</span>.</p>
</section>
</section>
<section id="non-linearity" class="level2" data-number="3.8">
<h2 data-number="3.8" class="anchored" data-anchor-id="non-linearity"><span class="header-section-number">3.8</span> Non-linearity</h2>
<p>Sometimes data show obvious curvature, in the sense that <span class="math inline">\(Y\)</span> is clearly not a straight line function of <span class="math inline">\(X\)</span>.</p>
<p>This will be visible on a plot of <span class="math inline">\(Y\)</span> vs.&nbsp;<span class="math inline">\(X\)</span>. It will also be visible on a residuals vs.&nbsp;predicted values plot after running a regression.</p>
<p>If there is curvature in the relationship between <span class="math inline">\(Y\)</span> and <span class="math inline">\(X\)</span>, then it might be sensible to add a polynomial <span class="math inline">\(X\)</span> term:</p>
<p><span class="math display">\[
Y_i = \beta_0 + \beta_1x_i + \beta_2x_i^2 + \epsilon_i
\]</span></p>
<section id="polynomial-review" class="level3" data-number="3.8.1">
<h3 data-number="3.8.1" class="anchored" data-anchor-id="polynomial-review"><span class="header-section-number">3.8.1</span> ‚ÄúPolynomial‚Äù review</h3>
<p>A ‚Äúpolynomial‚Äù expression is typically one in which variables are included at different powers. For example, a generic third degree polynomial equation might look like:</p>
<p><span class="math display">\[
y = a + bx + cx^2 + dx^3
\]</span></p>
<p>A ‚Äúsecond degree‚Äù polynomial is one in which an <span class="math inline">\(x\)</span> and <span class="math inline">\(x^2\)</span> term are both included. This is by far the most common type of polynomial seen in regression models.</p>
</section>
<section id="nd-degree-and-3rd-degree-polynomials" class="level3" data-number="3.8.2">
<h3 data-number="3.8.2" class="anchored" data-anchor-id="nd-degree-and-3rd-degree-polynomials"><span class="header-section-number">3.8.2</span> <span class="math inline">\(2^{nd}\)</span> degree and <span class="math inline">\(3^{rd}\)</span> degree polynomials</h3>
<p><span class="math inline">\(2^{nd}\)</span> degree polynomials are often called ‚Äúquadratic‚Äù. <span class="math inline">\(3^{rd}\)</span> degree polynomials are often called ‚Äúcubic‚Äù. Here are visual examples of simulated quadratic and cubic relationships between <span class="math inline">\(Y\)</span> and <span class="math inline">\(X\)</span>:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/mod4_pt2_4.png" class="img-fluid figure-img"></p>
</figure>
</div>
</section>
<section id="curvature-in-residuals" class="level3" data-number="3.8.3">
<h3 data-number="3.8.3" class="anchored" data-anchor-id="curvature-in-residuals"><span class="header-section-number">3.8.3</span> Curvature in residuals</h3>
<p>Here is regression output comparing a linear model to a quadratic model when the relationship between <span class="math inline">\(Y\)</span> and <span class="math inline">\(X\)</span> is quadratic:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/mod4_pt2_5.png" class="img-fluid figure-img"></p>
</figure>
</div>
</section>
<section id="example-florida-election-data" class="level3" data-number="3.8.4">
<h3 data-number="3.8.4" class="anchored" data-anchor-id="example-florida-election-data"><span class="header-section-number">3.8.4</span> Example: Florida election data</h3>
<p>Here is what the Florida election data look like with the Palm Beach County outlier removed, along with regression results for the simple linear regression model:</p>
<p><span class="math display">\[
Buchanan_i = \beta_0 + \beta_1Total\_Votes_i + \epsilon_i
\]</span></p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/mod4_pt2_6.png" class="img-fluid figure-img"></p>
</figure>
</div>
<p>Now we will fit a quadratic polynomial model to the same data:</p>
<p><span class="math display">\[
Buchanan_i = \beta_0 + \beta_1Total\_Votes_i + \beta_2Total\_Votes^2_i + \epsilon_i
\]</span></p>
<p>To create this polynomial predictor in jamovi we first need to install the GAMLj module. Then, select ‚ÄúGeneralized Linear Models‚Äù and last our ‚ÄúDependent Variable‚Äù and ‚ÄúCovariates‚Äù.</p>
<p>Under the ‚ÄúModel‚Äù drop down menu, click on Total_Votes in the ‚ÄúComponents‚Äù table.</p>
<p>An up and down arrow will appear with the degree of Total_Votes, click the up arrow so the 1 changes to 2. Then click the right arrow to add Total_Votes2 to ‚ÄúModel Terms‚Äù</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/mod4_pt2_7.png" class="img-fluid figure-img" width="463"></p>
</figure>
</div>
<p><span class="math display">\[
Buchanan_i = \beta_0 + \beta_1Total\_Votes_i + \beta_2Total\_Votes^2_i + \epsilon_i
\]</span></p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/mod4_pt2_8.png" class="img-fluid figure-img"></p>
</figure>
</div>
<p>This is better, but we still see curvature in the residual plot.</p>
<p>Let‚Äôs try a cubic model:</p>
<p><span class="math display">\[
Buchanan_i = \beta_0 + \beta_1Total\_Votes_i + \beta_2Total\_Votes^2_i +\beta_3Total\_Votes_i^3 + \epsilon_i
\]</span></p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/mod4_pt2_9.png" class="img-fluid figure-img" width="417"></p>
</figure>
</div>
<p><span class="math display">\[
Buchanan_i = \beta_0 + \beta_1Total\_Votes_i + \beta_2Total\_Votes^2_i +\beta_3Total\_Votes_i^3 + \epsilon_i
\]</span></p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/mod4_pt2_10.png" class="img-fluid figure-img"></p>
</figure>
</div>
</section>
<section id="example-florida-election-data-check-note" class="level3" data-number="3.8.5">
<h3 data-number="3.8.5" class="anchored" data-anchor-id="example-florida-election-data-check-note"><span class="header-section-number">3.8.5</span> Example: Florida election data: check note</h3>
<p>It‚Äôs debatable whether this is much better. For one, the <span class="math inline">\(Total\_Votes^2\)</span> term is non-significant.</p>
<p>But think back to multicollinearity. Each polynomial term will be correlated with the other terms ‚Äì after all, <span class="math inline">\(Total\_Votes\)</span>, <span class="math inline">\(Total\_Votes^2\)</span>, and <span class="math inline">\(Total\_Votes^3\)</span> must all be correlated.</p>
<p>Note that jamovi will not produce VIFs in GLM. Intuitively we know these will be highly correlated with each other.</p>
<p>It turns out that centering helps in polynomial models:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/mod4_pt2_11.png" class="img-fluid figure-img"></p>
</figure>
</div>
<p>By default jamovi centers polynomials which makes their standard errors smaller than if they were not centered. But, these estimates are not interpretable; you cannot hold <span class="math inline">\(Total\_Votes^2\)</span> constant while increasing <span class="math inline">\(Total\_Votes\)</span>.</p>
<p>In this example, the two counties with the highest total votes are heavily pulling on the regression line.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/mod4_pt2_12.png" class="img-fluid figure-img" width="382"></p>
</figure>
</div>
</section>
</section>
<section id="over-fitting" class="level2" data-number="3.9">
<h2 data-number="3.9" class="anchored" data-anchor-id="over-fitting"><span class="header-section-number">3.9</span> Over-fitting</h2>
<p>This model might be ‚Äúover-fit‚Äù.</p>
<p>Over-fitting is when a model fits the data so well that it ends up fitting random variation that is not of interest.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/mod4_pt2_13.png" class="img-fluid figure-img" width="343"></p>
</figure>
</div>
<p>Imagine if the two data points on the right had slightly higher vote counts for Buchanan. Or if the next two to their left had slightly lower vote counts. The curve would look very different.</p>
<p>At this point, we might just be modelling noise.</p>
<p>Here is an extreme example of over-fitting: fitting a ‚Äúsmoother‚Äù curve to data and giving it permission to move dramatically up and down through the data.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/mod4_pt2_14.png" class="img-fluid figure-img" width="357"></p>
</figure>
</div>
<p>This line fits the data very well, but does it represent the general trend between total votes and votes for Buchanan? Definitely not!</p>
<p>(Side note: ‚Äúsmoothers‚Äù are great tools for visualizing and summarizing data, but they can be extremely sensitive to degree of smoothing. We won‚Äôt use them in STAT 331)</p>
<p>Compare the over-fit model to the linear model.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/mod4_pt2_15.png" class="img-fluid figure-img" width="354"></p>
</figure>
</div>
<p>The linear model may be missing out on some curvature. But it might also make better predictions.</p>
<p>If we were to observe a new county with <span class="math inline">\(450,000\)</span> total votes, would we be better guessing that votes for Buchanan fall on the highly curved line or on the straight line?</p>
</section>
<section id="back-to-basics-is-the-model-sensible" class="level2" data-number="3.10">
<h2 data-number="3.10" class="anchored" data-anchor-id="back-to-basics-is-the-model-sensible"><span class="header-section-number">3.10</span> Back to basics: is the model sensible?</h2>
<ul>
<li><p>Back to basics: regression models are typically used for two purposes:</p>
<ul>
<li><p>Predicting values of the response variable, using the predictor variables. This is done by plugging values for the predictor variables into the estimated model.</p></li>
<li><p>Estimating the association between each individual predictor variable and the outcome while statistically holding other predictor variables constant. This is done by interpreted estimated slope coefficients.</p></li>
</ul></li>
</ul>
<section id="if-you-just-want-to-make-predictions" class="level3" data-number="3.10.1">
<h3 data-number="3.10.1" class="anchored" data-anchor-id="if-you-just-want-to-make-predictions"><span class="header-section-number">3.10.1</span> If you just want to make predictions</h3>
<p><span class="math inline">\(R^2\)</span> is the easiest to understand statistic for assessing how well your model makes predictions. The closer to <span class="math inline">\(1\)</span>, the better.</p>
<p>Multicollinearity isn‚Äôt an issue. It doesn‚Äôt affect predicted values.</p>
<p>BUT ‚Äì beware of overfitting! The more complex your model, the more risk you take of modeling noise instead of signal.</p>
<p>Also, be aware that <span class="math inline">\(R^2\)</span> can never go down when adding predictors. You can add complete nonsense predictor variables, and the worst that will happen to <span class="math inline">\(R^2\)</span> is that it stays the same.</p>
</section>
<section id="if-you-want-to-interpret-slopes" class="level3" data-number="3.10.2">
<h3 data-number="3.10.2" class="anchored" data-anchor-id="if-you-want-to-interpret-slopes"><span class="header-section-number">3.10.2</span> If you want to interpret slopes</h3>
<p>Always remember that each slope is interpreted under the assumption that all other predictor variables are being held constant, i.e.&nbsp;‚Äúcontrolled for‚Äù.</p>
<p>The more predictor variables in the model, the less sense this will make.</p>
<p>Example: wage vs.&nbsp;height study:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/mod4_pt2_16.png" class="img-fluid figure-img" width="502"></p>
</figure>
</div>
<p>In model 4, the estimated slope for youth height can be interpreted as:</p>
<p>‚ÄúThe predicted difference in ln(wage) for two people one inch apart in youth height, but equal in adult height, whose mothers have the same # of years of schooling and are both in either skilled fields or work or not, whose fathers have the same # of years of schooling and are both in either skilled fields or work or not, and who have the same number of siblings.‚Äù</p>
<div class="quarto-figure quarto-figure-left">
<figure class="figure">
<p><img src="images/mod4_pt2_17.png" class="img-fluid figure-img" width="216"></p>
</figure>
</div>
<p>Maybe this is the best way to think about the association between youth height and wages. But it is fairly complicated.</p>
<p>If you‚Äôre going to try to make ‚Äúreal world‚Äù sense out of regression results, your model should be informed by theory.</p>
<p>This is necessarily subjective! You have to choose which variables you think are important. You have to think about what makes sense.</p>
<ul>
<li><p>This might require:</p>
<ul>
<li><p>Log transforming a variable solely because you like the multiplicative interpretation better than the additive interpretation.</p></li>
<li><p>Keeping a variable in a model even though it isn‚Äôt statistically significant.</p></li>
<li><p>Removing a variable you are interested in, because it doesn‚Äôt make sense to ‚Äúhold it constant‚Äù when estimating slopes for other variables.</p></li>
</ul></li>
</ul>
</section>
<section id="is-the-model-missing-something-important" class="level3" data-number="3.10.3">
<h3 data-number="3.10.3" class="anchored" data-anchor-id="is-the-model-missing-something-important"><span class="header-section-number">3.10.3</span> Is the model missing something important?</h3>
<p>There is another variable in the Florida election data set that could be worth including: ‚ÄúReg_Reform‚Äù: the total number of voters registered with the Reform Party. Pat Buchanan was the Reform Party candidate. Let‚Äôs add it to the model:</p>
<p><span class="math display">\[
Buchanan_i = \beta_0 + \beta_1Total\_Votes_i + \beta_2Reg\_Reform_i + \epsilon_i
\]</span></p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/mod4_pt2_18.png" class="img-fluid figure-img"></p>
</figure>
</div>
<p>This residual plot looks great!</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/mod4_pt2_19.png" class="img-fluid figure-img" width="275"></p>
</figure>
</div>
<p>It turns out that the curvature in the previous residual plots went away when adding an important variable to the model. No need to mess with polynomials after all!</p>
<p>Also, the <span class="math inline">\(R^2\)</span> is roughly the same as in the cubic model using only total votes as a predictor.</p>
<p>So we have roughly equal fit, without having to worry about overfitting, and without having to give up interpretability of the slopes.</p>
<p>One downside: there is some collinearity. Look at the VIFs.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/mod4_pt2_20.png" class="img-fluid figure-img"></p>
</figure>
</div>
<p>VIF of about <span class="math inline">\(5\)</span> implies <span class="math inline">\(\frac{1}{1-R^2}\approx 5\)</span> when using the <span class="math inline">\(R^2\)</span> from:</p>
<p><span class="math display">\[
Total\_Votes_i = \beta_0 + \beta_1Reg\_Reform_i + \epsilon_i
\]</span></p>
<p>So, this <span class="math inline">\(R^2\)</span> is about <span class="math inline">\(1 ‚àí \frac{1}{5} = 0.8\)</span>. And so <span class="math inline">\(r = \sqrt{0.8} =0.89\)</span> . These predictors are strongly correlated.</p>
<p><img src="images/mod4_pt2_21.png" class="img-fluid"></p>
<p>Note also that total votes is not significant.</p>
<p>But: the slope for <span class="math inline">\(Reg\_Reform\)</span> has a nice interpretation:</p>
<p>When comparing two counties with the same number of votes cast in the election, a county with an additional registered Reform Party member is estimated to have <span class="math inline">\(2.24\)</span> additional votes, on average, for Pat Buchanan.</p>
<p>Should total votes be taken out of the model? This is a subjective decision.</p>
</section>
<section id="what-would-you-like-to-control-for" class="level3" data-number="3.10.4">
<h3 data-number="3.10.4" class="anchored" data-anchor-id="what-would-you-like-to-control-for"><span class="header-section-number">3.10.4</span> What would you like to ‚Äúcontrol‚Äù for?</h3>
<p>In regression analysis, we usually emphasize (correctly) that correlation does not imply causation.</p>
<p>However, if you have knowledge or beliefs about causal direction, you should take these into account when choosing your variables!</p>
<p>Example: in the rheumatoid arthritis study, we were looking at the effect of drugs on inflammation level. In particular, we were comparing how effective they were at reducing inflammation. Suppose we also asked patients to rate their mobility level (RA tends to reduce mobility).</p>
<p>Our model might be:</p>
<p><span class="math display">\[
Difference_i = \beta_0 + \beta_1age_i + \beta_2drug_i + \beta_3(age*drug)_i + \beta_4mobility_i + \epsilon_i
\]</span></p>
<p>Now, when interpreting the previous slopes, I am comparing average reduction in inflammation (‚Äúdifference‚Äù, the response variable), between two people who have the same mobility level. But if they have the same mobility level, then they will have more similar inflammation levels than if we allowed mobility level to differ.</p>
<p>In other words, because the drug reduces inflammation <em>and</em> improves mobility, ‚Äúcontrolling‚Äù for mobility will make it look like the drugs are less effective than they really are.</p>
</section>
<section id="beware-the-kitchen-sink-approach" class="level3" data-number="3.10.5">
<h3 data-number="3.10.5" class="anchored" data-anchor-id="beware-the-kitchen-sink-approach"><span class="header-section-number">3.10.5</span> Beware the ‚Äúkitchen sink‚Äù approach</h3>
<p>There‚Äôs an old saying: ‚Äútaking everything but the kitchen sink‚Äù.</p>
<p>It can be tempting to toss everything but the kitchen sink into a regression model, especially when you have loads of variables that all seem like they‚Äôd be associated with the response.</p>
<p>But beware! Adding in one predictor can have a dramatic effect on the slopes of other predictors, as well as on their standard errors.</p>
<p>It really really <em>really</em> matters that each slope is estimated as though all other predictors are held constant. This can reveal otherwise unseen effects, but it can also obscure otherwise obvious effects or induce apparent effects that aren‚Äôt real. There is no substitute for scientific reasoning when choosing a model.</p>
</section>
<section id="the-model-is-simpler-than-whats-being-modeled" class="level3" data-number="3.10.6">
<h3 data-number="3.10.6" class="anchored" data-anchor-id="the-model-is-simpler-than-whats-being-modeled"><span class="header-section-number">3.10.6</span> The model is simpler than what‚Äôs being modeled</h3>
<p>Let‚Äôs take a step back and ask: why are we fitting data to models?</p>
<p>Well, we are interested in the real world. And the real world in incredibly complicated. Maybe incomprehensibly complicated.</p>
<p>So, we simplify things using models. We hope that the model captures the essence of what we care about in the real world. But we know it is a simplification; perhaps an extreme simplification.</p>
<p>‚ÄúAll models are wrong; some are useful‚Äù ‚Äì George Box</p>
<p>Consider how the regression model describes where data comes from:</p>
<p><span class="math display">\[
Y_i = \beta_0 + \beta_1x_{i} + \epsilon_i, \text{ where } \epsilon_i \sim Normal(0,\sigma)
\]</span></p>
<p>This says that to get data, we pick a value for X, go to a line, then randomly draw a value from a normal distribution and add this to the value on the line. And that‚Äôs where data comes from!</p>
<p>Except, that‚Äôs not where data comes from. This is a model. It is a simplification of reality. We use these models because we think they will help us answer questions we care about (e.g.&nbsp;make predictions, identify associations between variables). Don‚Äôt forget that the model is not the thing itself.</p>


</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "Óßã";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./Ch2_Model_Building.html" class="pagination-link">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Chapter 2: Model building with linear regression</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./Ch4_ANOVA.html" class="pagination-link">
        <span class="nav-page-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Chapter 4: ANOVA-based methods</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->



</body></html>
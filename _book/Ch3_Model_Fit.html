<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.361">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>STAT 331 - 3&nbsp; Chapter 3: Assessing and improving model fit</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./Ch4_ANOVA.html" rel="next">
<link href="./Ch2_Model_Building.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>


</head>

<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
      <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./Ch3_Model_Fit.html"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Chapter 3: Assessing and improving model fit</span></a></li></ol></nav>
      <a class="flex-grow-1" role="button" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
      </a>
      <button type="button" class="btn quarto-search-button" aria-label="" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">STAT 331</a> 
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Introduction to STAT 331: Intermediate Applied Statistical Methods</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Ch1_Review.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Chapter 1: Review of classical inference</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Ch2_Model_Building.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Chapter 2: Model building with linear regression</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Ch3_Model_Fit.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Chapter 3: Assessing and improving model fit</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Ch4_ANOVA.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Chapter 4: ANOVA-based methods</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Ch5_Categorical.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Chapter 5: Analyzing categorical data</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Ch6_GLMs.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Generalized Linear Models (GLMs)</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Ch7_Mixed.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Chapter 7: Mixed-effects models</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Ch8_Mind_the_Gap.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Chapter 8: Minding the gap between science and statistics</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Ch9_Other_Topics.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Brief looks at major topics we didnâ€™t cover</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">References</span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#part-1-assumptions-and-assumption-violations" id="toc-part-1-assumptions-and-assumption-violations" class="nav-link active" data-scroll-target="#part-1-assumptions-and-assumption-violations"><span class="header-section-number">3.1</span> Part 1: assumptions and assumption violations</a>
  <ul class="collapse">
  <li><a href="#violating-model-assumptions" id="toc-violating-model-assumptions" class="nav-link" data-scroll-target="#violating-model-assumptions"><span class="header-section-number">3.1.1</span> Violating model assumptions</a></li>
  <li><a href="#the-regression-model-and-what-it-assumes" id="toc-the-regression-model-and-what-it-assumes" class="nav-link" data-scroll-target="#the-regression-model-and-what-it-assumes"><span class="header-section-number">3.1.2</span> The regression model and what it assumes</a></li>
  <li><a href="#what-to-look-for-in-a-residual-plot" id="toc-what-to-look-for-in-a-residual-plot" class="nav-link" data-scroll-target="#what-to-look-for-in-a-residual-plot"><span class="header-section-number">3.1.3</span> What to look for in a residual plot</a></li>
  <li><a href="#the-normality-assumption" id="toc-the-normality-assumption" class="nav-link" data-scroll-target="#the-normality-assumption"><span class="header-section-number">3.1.4</span> The normality assumption</a></li>
  <li><a href="#the-homogeneity-of-variance-assumption" id="toc-the-homogeneity-of-variance-assumption" class="nav-link" data-scroll-target="#the-homogeneity-of-variance-assumption"><span class="header-section-number">3.1.5</span> The homogeneity of variance assumption</a></li>
  <li><a href="#influential-observations" id="toc-influential-observations" class="nav-link" data-scroll-target="#influential-observations"><span class="header-section-number">3.1.6</span> Influential observations</a></li>
  <li><a href="#multicollinearity" id="toc-multicollinearity" class="nav-link" data-scroll-target="#multicollinearity"><span class="header-section-number">3.1.7</span> (Multi)collinearity</a></li>
  </ul></li>
  <li><a href="#part-2-improving-models" id="toc-part-2-improving-models" class="nav-link" data-scroll-target="#part-2-improving-models"><span class="header-section-number">3.2</span> Part 2: Improving models</a>
  <ul class="collapse">
  <li><a href="#summary-of-part-1" id="toc-summary-of-part-1" class="nav-link" data-scroll-target="#summary-of-part-1"><span class="header-section-number">3.2.1</span> Summary of part 1</a></li>
  <li><a href="#transforming-variables" id="toc-transforming-variables" class="nav-link" data-scroll-target="#transforming-variables"><span class="header-section-number">3.2.2</span> Transforming variables</a></li>
  <li><a href="#log-transformation" id="toc-log-transformation" class="nav-link" data-scroll-target="#log-transformation"><span class="header-section-number">3.2.3</span> Log transformation</a></li>
  <li><a href="#non-linearity" id="toc-non-linearity" class="nav-link" data-scroll-target="#non-linearity"><span class="header-section-number">3.2.4</span> Non-linearity</a></li>
  <li><a href="#over-fitting" id="toc-over-fitting" class="nav-link" data-scroll-target="#over-fitting"><span class="header-section-number">3.2.5</span> Over-fitting</a></li>
  <li><a href="#back-to-basics-is-the-model-sensible" id="toc-back-to-basics-is-the-model-sensible" class="nav-link" data-scroll-target="#back-to-basics-is-the-model-sensible"><span class="header-section-number">3.2.6</span> Back to basics: is the model sensible?</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Chapter 3: Assessing and improving model fit</span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  

</header>

<section id="part-1-assumptions-and-assumption-violations" class="level2" data-number="3.1">
<h2 data-number="3.1" class="anchored" data-anchor-id="part-1-assumptions-and-assumption-violations"><span class="header-section-number">3.1</span> Part 1: assumptions and assumption violations</h2>
<p>Outline of notes</p>
<p>â€¢ Regression assumptions â€¢ Linearity â€¢ Normality of residuals â€¢ Homogeneity of variance â€¢ Influential observations â€¢ (Multi)collinearity</p>
<section id="violating-model-assumptions" class="level3" data-number="3.1.1">
<h3 data-number="3.1.1" class="anchored" data-anchor-id="violating-model-assumptions"><span class="header-section-number">3.1.1</span> Violating model assumptions</h3>
<p>â€¢ The previous notes outlined the fundamentals of specifying, fitting, and interpreting a linear regression model.</p>
<p>â€¢ These notes focus on the assumptions that our models are based upon, and how we check to see if they are being violated.</p>
<p>â€¢ If model assumptions are violated, DONâ€™T PANIC! There is nearly always a remedy. In these notes we will look at how to spot violations of assumptions. In the next set of notes we will look at ways to remedy these violations, and how to decide if they pose a serious problem to the usefulness of the model.</p>
</section>
<section id="the-regression-model-and-what-it-assumes" class="level3" data-number="3.1.2">
<h3 data-number="3.1.2" class="anchored" data-anchor-id="the-regression-model-and-what-it-assumes"><span class="header-section-number">3.1.2</span> The regression model and what it assumes</h3>
<p>â€¢ Once again, here is the regression model:</p>
<p>ğ‘Œğ‘– = ğ›½0 + ğ›½1ğ‘¥1ğ‘– + ğ›½2ğ‘¥2ğ‘– + â‹¯ + ğ›½ğ‘ğ‘¥ğ‘ğ‘– + ğœ€ğ‘– ğœ€ğ‘–~ğ‘ğ‘œğ‘Ÿğ‘šğ‘ğ‘™</p>
<p>â€¢ This assumes:</p>
<p>â€¢ That the response variable is a linear (straight line) function of the predictor variables â€¢ That the residuals will be normally distributed â€¢ That the standard deviation of the residuals does not vary â€¢ That the residuals are independent</p>
<p>â€¢ Remember the â€œsimpleâ€ (i.e.&nbsp;single predictor) regression model:</p>
<p>ğ‘Œğ‘– = ğ›½0 + ğ›½1ğ‘¥ğ‘– + ğœ€ğ‘– ğœ€ğ‘–~ğ‘ğ‘œğ‘Ÿğ‘šğ‘ğ‘™</p>
<p>â€¢ This is linear in that it fits a straight line to the two-dimensional data.</p>
<p>â€¢ A two-predictor model would fit a flat plane to the three-dimensional data, and so on</p>
<p>â€¢ Hereâ€™s a bad idea: fitting a linear model to non-linear data!</p>
<p>â€¢ When running â€œLinear Regressionâ€ in jamovi, a â€œresiduals by predictedâ€ plot can be created by selecting â€œResidual plotsâ€ under â€œAssumption Checksâ€</p>
<p>â€¢ The residuals are the differences between each observed values of the response variable and the value that the model predicts:</p>
<p>ğ‘Ÿğ‘’ğ‘ ğ‘–ğ‘‘ğ‘¢ğ‘ğ‘™ğ‘– = ğ‘¦ğ‘– âˆ’</p>
<p>ğ‘¦à·œğ‘– = ğ‘¦ğ‘– âˆ’ (ğ›½áˆ˜</p>
<ul>
<li>ğ›½áˆ˜</li>
</ul>
<p>ğ‘¥1</p>
<ul>
<li>ğ›½áˆ˜</li>
</ul>
<p>ğ‘¥2</p>
<ul>
<li>â‹¯ )</li>
</ul>
<p>â€¢ For simple regression, this plot just looks like the regression plot with the line turned horizontally.</p>
<p>â€¢ For multiple regression, there is no (two dimensional) â€œregression plotâ€, so the residual plot will be very useful!</p>
<p>â€¢ In this example, there is clear curvature in the data. A straight line model is not appropriate.</p>
<p>â€¢ Hereâ€™s an example of what a linear relationship might look like:</p>
<p>â€¢ When there is non-linearity, you will see the residuals mostly on one side of zero, then on the other size of zero, then back again, and so on.</p>
<p>â€¢ When there is linearity, the residuals should randomly fall on either side of zero.</p>
</section>
<section id="what-to-look-for-in-a-residual-plot" class="level3" data-number="3.1.3">
<h3 data-number="3.1.3" class="anchored" data-anchor-id="what-to-look-for-in-a-residual-plot"><span class="header-section-number">3.1.3</span> What to look for in a residual plot</h3>
<p>â€¢ We will look at many more examples of residual plots in these notes.</p>
<p>â€¢ We want a residual plot that appears to agree with the model assumptions:</p>
<p>o Straight line relationship between the predictors and response</p>
<p>o Normally distributed random residuals around this line</p>
<p>o Equal variance in residuals across line</p>
</section>
<section id="the-normality-assumption" class="level3" data-number="3.1.4">
<h3 data-number="3.1.4" class="anchored" data-anchor-id="the-normality-assumption"><span class="header-section-number">3.1.4</span> The normality assumption</h3>
<p>â€¢ The â€œerror termâ€ in a regression model is that + ğœ€ğ‘– on the end</p>
<p>â€¢ When we write ğœ€ğ‘–~ğ‘ğ‘œğ‘Ÿğ‘šğ‘ğ‘™</p>
<p>, we are saying that the errors (aka</p>
<p>residuals) are normally distributed, with mean zero and some standard deviation Ïƒ.</p>
<p>â€¢ This can be assessed visually: run the regression plot the residuals to see if they appear roughly normally distributed.</p>
<section id="the-qq-plot" class="level4" data-number="3.1.4.1">
<h4 data-number="3.1.4.1" class="anchored" data-anchor-id="the-qq-plot"><span class="header-section-number">3.1.4.1</span> The QQ plot</h4>
<p>â€¢ When fitting a model using â€œLinear Regressionâ€ in jamovi, there is an option to save residuals. This will create a new column with a residual for each row. â€¢ The option is under the last drop-down menu, under â€œSaveâ€.</p>
<p>â€¢ To create a plot of the residuals, select â€œQ-Q plot of residualsâ€ under â€œAssumption Checksâ€ in â€œLinear Regressionâ€</p>
<p>â€¢ The Normal Quantile plot is also known as the QQ plot, for â€œquantile quantileâ€.</p>
<p>â€¢ It is easier to assess normality with a QQ plot than with a histogram.</p>
<p>â€¢ On Canvas under Simulations there is a â€œQQ plot generatorâ€ app.</p>
<p>â€¢ This app allows you to manipulate a distribution, sample from it, and then view a histogram next to a QQ plot. It is meant to give you an idea of how QQ plots work.</p>
<p>â€¢ By default, it draws data from a normal distribution. But, you can add â€œskewednessâ€ or â€œpeakednessâ€ (aka kurtosis) to make the distribution non-normal. You can also adjust sample size.</p>
<p>â€¢ A QQ plot shows you how much the distribution of your data â€œagreeâ€ with a normal distribution.</p>
<p>â€¢ The horizonal axis gives the distribution data would follow if it were perfectly normal. â€¢ The vertical axis gives the distribution your data actually follows.</p>
<p>â€¢ The diagonal line shows perfect agreement between the two.</p>
<p>â€¢ The big advantage of the QQ plot vs.&nbsp;the histogram is that very often data that come from a normal distribution donâ€™t look normal, especially if the sample size is small. â€¢ In this case, the histogram isnâ€™t clearly normal. But, on the QQ plot the data are close to the line.</p>
<p>â€¢ Notice that the data still veer from the diagonal line to some extent, even though we know for a fact they came from a normal distribution.</p>
</section>
<section id="limitations-of-qq-plots" class="level4" data-number="3.1.4.2">
<h4 data-number="3.1.4.2" class="anchored" data-anchor-id="limitations-of-qq-plots"><span class="header-section-number">3.1.4.2</span> Limitations of QQ plots</h4>
<p>â€¢ As you can see from the app, sometimes data that come from a normal distribution donâ€™t sit right on the line.</p>
<p>â€¢ Sometimes data that come from a skewed distribution look similar to data that come from a normal distribution</p>
<p>â€¢ Itâ€™s easier to assess normality when sample sizes are larger.</p>
<p>â€¢ As it turns out, the assumption of normality is not vital to the validity of a regression model. If the QQ plot is vague, youâ€™re probably fine. We only worry when we see extreme non-normality.</p>
</section>
<section id="tests-for-normality-not-recommended" class="level4" data-number="3.1.4.3">
<h4 data-number="3.1.4.3" class="anchored" data-anchor-id="tests-for-normality-not-recommended"><span class="header-section-number">3.1.4.3</span> Tests for normality (not recommended)</h4>
<p>â€¢ There are statistical tests, such as â€œShapiro-Wilksâ€ or â€œKolmgorov-Smirnovâ€, for which the null hypothesis is that the data come from some specified distribution, like the normal.</p>
<p>â€¢ Rejecting this null means that the data â€œsignificantlyâ€ disagree with the assumption of normality.</p>
<p>â€¢ I do not recommend using this test. The problem is that, when the sample size is large enough, even small deviations from normality will be statistically significant. But small deviations from normality are OK. Only major deviations are concerning.</p>
</section>
</section>
<section id="the-homogeneity-of-variance-assumption" class="level3" data-number="3.1.5">
<h3 data-number="3.1.5" class="anchored" data-anchor-id="the-homogeneity-of-variance-assumption"><span class="header-section-number">3.1.5</span> The homogeneity of variance assumption</h3>
<p>â€¢ Back to the error term:</p>
<p>ğœ€ğ‘–~ğ‘ğ‘œğ‘Ÿğ‘šğ‘ğ‘™</p>
<p>â€¢ Notice that Ïƒ is just one number. This suggests that the standard deviation of the residuals should be the same across all values of predictor variables. In other words, there is homogeneity of variance.</p>
<p>â€¢ Another name for this is â€œhomoscedasticityâ€. If this assumption is violated, then we have â€œheteroscedasticityâ€.</p>
<section id="heterogeneity-of-variance" class="level4" data-number="3.1.5.1">
<h4 data-number="3.1.5.1" class="anchored" data-anchor-id="heterogeneity-of-variance"><span class="header-section-number">3.1.5.1</span> Heterogeneity of variance</h4>
<p>â€¢ To show heterogeneity of variance, Iâ€™ll simulate data that is a function of an X variable, plus random values from a normal distribution with standard deviation equal to X.</p>
<p>â€¢ Thus, the standard deviation of residuals will get bigger as X gets bigger:</p>
</section>
<section id="the-residuals-vs-fitted-plot" class="level4" data-number="3.1.5.2">
<h4 data-number="3.1.5.2" class="anchored" data-anchor-id="the-residuals-vs-fitted-plot"><span class="header-section-number">3.1.5.2</span> The residuals vs fitted plot</h4>
<p>â€¢ Here is the regression plot and residual plot when this simulated variable (called â€œWâ€ here) is the response and X is the predictor: â€¢ Notice that the residuals are more spread out for larger X</p>
<p>â€¢ We also see â€œheavy tailsâ€ when plotting the residuals with a histogram and QQ plot:</p>
<p>â€¢ Heavy tails refers to a distribution with outliers on both ends.</p>
<p>â€¢ This shows up on the QQ plot as the residuals being too flat in the middle and then curving out on both ends.</p>
</section>
</section>
<section id="influential-observations" class="level3" data-number="3.1.6">
<h3 data-number="3.1.6" class="anchored" data-anchor-id="influential-observations"><span class="header-section-number">3.1.6</span> Influential observations</h3>
<p>â€¢ Outliers in regression can be seen on a residual plot, or on a QQ plot, or on just a regular plot of the data.</p>
<p>â€¢ Example: the Florida election data.</p>
<p>â€¢ Outliers can â€œpullâ€ on the regression line, especially if they are far away from the mean of the predictor(s).</p>
<p>â€¢ There are many statistics that assess influence. jamovi will calculate one of the most popular: a Cookâ€™s Distance</p>
<p>â€¢ As we saw in the Florida election example, removing the outlier (Palm Beach County) had a substantial effect on the regression results.</p>
<p>â€¢ The logic behind Cookâ€™s Distance is to quantify what happens to the regression model when a single observation is removed. This is sometimes referred to as a â€œleave one outâ€ method.</p>
<p>â€¢ Cookâ€™s Distances quantify how much the predicted values of the response variable change when an observation is removed.</p>
<p>â€¢ Recall that, in simple regression, the predicted values are the values on the</p>
<p>â€¢ It is hard to interpret the actual values for Cookâ€™s Distances. Values greater than 1 are often considered â€œinfluentialâ€.</p>
<p>â€¢ The formula shows that it is based on the sum of the differences in predicted values between a model with the data point included and a model with it removed:</p>
<p>ğ¶ğ‘œğ‘œğ‘˜â€²ğ‘  ğ·ğ‘–ğ‘ ğ‘¡ğ‘ğ‘›ğ‘ğ‘’ ğ‘“ğ‘œğ‘Ÿ ğ‘‘ğ‘ğ‘¡ğ‘ ğ‘ğ‘œğ‘–ğ‘›ğ‘¡ â€œğ‘–â€ = ğ·ğ‘– =</p>
<p>ğ‘› 2 ğ‘—=1</p>
<p>ğ‘€ğ‘†ğ¸ âˆ— ğ‘</p>
<p>â€¢ Where ğ‘¦à·œğ‘— ğ‘– is the predicted value of the response variable when the model is re- fit with the ğ‘–ğ‘¡â„ data point removed, and p is the number of predictor variables in</p>
<p>â€¢ A Cookâ€™s Distance is calculated for every data point. The option to do this in jamovi is under â€œSaveâ€ in â€œLinear Regressionâ€. This creates a new column with a Cookâ€™s distance for each row.</p>
<p>â€¢ The saved Cookâ€™s Distances can then be plotted. Any possibly influential points will usually stand out clearly from the rest.</p>
<p>â€¢ To quickly narrow in on the influential counties, we can filter out all the small Cookâ€™s distances.</p>
<p>â€¢ After implementing the filter, we can see that all rows which do not meet the criteria of the filter are excluded.</p>
<p>â€¢ Only rows 13 and 50 should be highlighted for Dade and Palmbeach county which have cookâ€™s distances of 1.983 and 3.786.</p>
</section>
<section id="multicollinearity" class="level3" data-number="3.1.7">
<h3 data-number="3.1.7" class="anchored" data-anchor-id="multicollinearity"><span class="header-section-number">3.1.7</span> (Multi)collinearity</h3>
<p>â€¢ In regression analysis, we want our predictor variables to be correlated with the response variable.</p>
<p>â€¢ But we donâ€™t want our predictor variables to be (highly) correlated with one another!</p>
<p>â€¢ When two predictor variables are highly correlated, we say our model has â€œcollinearityâ€</p>
<p>â€¢ When more than two predictor variables are mutually highly correlated, we say our model as â€œmulticollinearityâ€.</p>
<p>â€¢ To understand why we donâ€™t want correlated predictors, recall that multiple regression models estimate the association between each individual predictor variable and the response, while holding all other predictor variables constant.</p>
<p>â€¢ This can be thought of as asking â€œwhat is the difference in the response variable when we observe data points that have different values of one predictor but the same values of the other predictors?</p>
<p>â€¢ If ğ‘Œ is the response and ğ‘¥1 and ğ‘¥2 are predictors, we want to know how different ğ‘Œ is when ğ‘¥1 values differ but ğ‘¥2 values are the same, or vice</p>
<p>â€¢ But, if ğ‘¥1 and ğ‘¥2 are highly correlated, then we donâ€™t get to observe cases where values of one variable differ substantially while values of the other are the same! Consider instances of no correlation vs.&nbsp;heavy correlation:</p>
<section id="why-dont-we-want-correlated-predictors" class="level4" data-number="3.1.7.1">
<h4 data-number="3.1.7.1" class="anchored" data-anchor-id="why-dont-we-want-correlated-predictors"><span class="header-section-number">3.1.7.1</span> Why donâ€™t we want correlated predictors?</h4>
<p>â€¢ When ğ‘¥1 and ğ‘¥2 are uncorrelated, we see lots of instances of ğ‘¥1 values differing a lot when ğ‘¥2 values are equal.</p>
<p>â€¢ When ğ‘¥1 and ğ‘¥2 are highly correlated, we never see instances where ğ‘¥2 values are equal but ğ‘¥1 values are highly correlated.</p>
<p>â€¢ The upshot is that, when ğ‘¥1 and ğ‘¥2 are highly correlated, the regression procedure has a difficult time distinguishing between the â€œeffectâ€ of ğ‘¥1 on ğ‘Œ and the â€œeffectâ€ of ğ‘¥2 on ğ‘Œ.</p>
<p>â€¢ Extreme example: if we had degrees Celsius and degrees Fahrenheit as predictors in the same model, it would be impossible to tell their effects apart. You canâ€™t change Celsius while holding Fahrenheit constant!</p>
<p>â€¢ The practical consequence is that the standard errors for the slopes of (multi)collinear predictors are much larger than they would be if it were not for the (multi)collinearity.</p>
<p>â€¢ If two or more predictors are perfectly correlated (ğ‘Ÿ = 1), then the model cannot be fit and jamovi produces an error:</p>
<p>â€¢ Here, X1 and X2 are perfectly correlated. jamovi cannot estimate a slope for X2.</p>
<p>â€¢ In the Florida election data, we used total votes for each county as our predictor variable.</p>
<p>â€¢ There is another variable called â€œTotal_Regâ€. This is the total number of registered voters in each county.</p>
<p>â€¢ Unsurprisingly, Total_Votes and Total_Reg are highly correlated:</p>
<p>â€¢ If we run two separate simple regression models, we get very similar results:</p>
<p>ğµğ‘¢ğ‘â„ğ‘ğ‘›ğ‘ğ‘›ğ‘– = ğ›½0 + ğ›½1ğ‘‡ğ‘œğ‘¡ğ‘ğ‘™ _ğ‘‰ğ‘œğ‘¡ğ‘’ğ‘ ğ‘– + ğœ€ğ‘– ğµğ‘¢ğ‘â„ğ‘ğ‘›ğ‘ğ‘›ğ‘– = ğ›½0 + ğ›½1ğ‘‡ğ‘œğ‘¡ğ‘ğ‘™ _ğ‘…ğ‘’ğ‘”ğ‘– + ğœ€ğ‘–</p>
<p>â€¢ But look what happens if we use Total_Votes and Total_Reg as predictors in the same model:</p>
<p>ğµğ‘¢ğ‘â„ğ‘ğ‘›ğ‘ğ‘›ğ‘– = ğ›½0 + ğ›½1ğ‘‡ğ‘œğ‘¡ğ‘ğ‘™_ğ‘‰ğ‘œğ‘¡ğ‘’ğ‘ ğ‘– + ğ‘‡ğ‘œğ‘¡ğ‘ğ‘™_ğ‘…ğ‘’ğ‘”ğ‘– + ğœ€ğ‘– â€¢ Two important things to note:</p>
<p>â€¢ P-values on slopes are much larger than for the individual models</p>
<p>â€¢ ğ‘…2 is larger than on either individual model!</p>
<p>â€¢ Looking at the estimates and standard errors in all three models, we see that the standard errors are much larger in the multiple regression model. These estimates are â€œunstableâ€ â€“ their values will change a lot if the data change a little.</p>
<p>â€¢ We know the association between Total_Reg are Buchanan is actually positive. But with so high a standard error, the slope for Total_Reg turned out negative!</p>
</section>
<section id="variance-inflation-factor-vif" class="level4" data-number="3.1.7.2">
<h4 data-number="3.1.7.2" class="anchored" data-anchor-id="variance-inflation-factor-vif"><span class="header-section-number">3.1.7.2</span> Variance inflation factor (VIF)</h4>
<p>â€¢ (Multi)collinearity can be assessed using a â€œVariance Inflation Factorâ€, or VIF. A VIF is calculated for the ğ‘—ğ‘¡â„ predictor variable as:</p>
<p>ğ‘‰ğ¼ğ¹ğ‘— =</p>
<p>1</p>
<p>1 âˆ’ ğ‘…2</p>
<p>â€¢ Where ğ‘…2 is the ğ‘…2 from a regression model with predictor j as the response variable and all other predictors still as predictors.</p>
<p>â€¢ In the Florida election example, the VIF for Total_Votes can be found using the ğ‘…2 for the model: ğ‘‡ğ‘œğ‘¡ğ‘ğ‘™ _ğ‘‰ğ‘œğ‘¡ğ‘’ğ‘ ğ‘– = ğ›½0 + ğ›½1ğ‘‡ğ‘œğ‘¡ğ‘ğ‘™ _ğ‘…ğ‘’ğ‘”ğ‘– + ğœ€ğ‘–<br>
â€¢ This ğ‘…2 is huge! Plugging it into the formula:</p>
<p>ğ‘‰ğ¼ğ¹ğ‘‡ğ‘œğ‘¡ğ‘ğ‘™_ğ‘‰ğ‘œğ‘¡ğ‘’ğ‘  =</p>
<p>1 = 333.33 1 âˆ’ 0.997</p>
<p>â€¢ Thankfully we donâ€™t have to do this by hand. In jamovi, under â€œLinear Regressionâ€ select â€œCollinearity statisticsâ€:</p>
<p>â€¢ VIF &gt; 10 typically is considered large (note that this would imply ğ‘…2 = 0.9 between predictor variables).</p>
<p>â€¢ The most obvious thing to do in the presence of (multi)collinearity is to remove one or more correlated predictor variable. From a scientific standpoint, you also may not want highly correlated predictors in the same model.</p>
</section>
<section id="when-should-we-worry-about-multicollinearity" class="level4" data-number="3.1.7.3">
<h4 data-number="3.1.7.3" class="anchored" data-anchor-id="when-should-we-worry-about-multicollinearity"><span class="header-section-number">3.1.7.3</span> When should we worry about (multi)collinearity?</h4>
<p>â€¢ (Multi)collinearity is a potentially huge problem if the goal of the regression model is to interpret the estimated slopes.</p>
<p>â€¢ This is because it increases the standard error of these slopes, making their values less reliable. Some people say it makes slopes â€œunstableâ€.</p>
<p>â€¢ It may also complicate the interpretation of slopes: you are trying to statistically â€œhold constantâ€ a predictor variable that doesnâ€™t naturally stay constant when the other predictor varies. This isnâ€™t necessarily a problem, but it is something to be aware of.</p>
<p>â€¢ However, (multi)collinearity does not negatively impact the predicted values themselves. Remember that it didnâ€™t hurt the ğ‘…2 value in the Florida election example. ğ‘…2 tells you how good your predictions are.</p>
<p>â€¢ So, if the model is only for predicting, you probably donâ€™t need to worry about using correlated predictor variables. Just beware when interpreting the slopes.</p>
</section>
</section>
</section>
<section id="part-2-improving-models" class="level2" data-number="3.2">
<h2 data-number="3.2" class="anchored" data-anchor-id="part-2-improving-models"><span class="header-section-number">3.2</span> Part 2: Improving models</h2>
<section id="summary-of-part-1" class="level3" data-number="3.2.1">
<h3 data-number="3.2.1" class="anchored" data-anchor-id="summary-of-part-1"><span class="header-section-number">3.2.1</span> Summary of part 1</h3>
<p>â€¢ In the last set of notes, we looked at some things that can go wrong in regression modeling, including:</p>
<p>â€¢ Non-linear relationships between predictor(s) and response â€¢ Non-normality of residuals â€¢ Non-constant (heterogeneous) variance of residuals â€¢ Influential outliers â€¢ Multicollinearity</p>
<p>â€¢ In these notes, weâ€™ll look at some tools available for dealing with these problems.</p>
</section>
<section id="transforming-variables" class="level3" data-number="3.2.2">
<h3 data-number="3.2.2" class="anchored" data-anchor-id="transforming-variables"><span class="header-section-number">3.2.2</span> Transforming variables</h3>
<p>â€¢ Recall the regression model:</p>
<p>ğ‘Œğ‘– = ğ›½0 + ğ›½1ğ‘¥1ğ‘– + ğ›½2ğ‘¥2ğ‘– + â‹¯ + ğ›½ğ‘ğ‘¥ğ‘ğ‘– + ğœ€ğ‘– ğœ€ğ‘–~ğ‘ğ‘œğ‘Ÿğ‘šğ‘ğ‘™</p>
<p>â€¢ Sometimes we can correct violations of model assumptions by applying a mathematical transformation to the response or predictor variables.</p>
<p>â€¢ The most common transformation in Statistics is the log transformation:</p>
<p>ln(ğ‘¥) = logğ‘’(ğ‘¥)</p>
</section>
<section id="log-transformation" class="level3" data-number="3.2.3">
<h3 data-number="3.2.3" class="anchored" data-anchor-id="log-transformation"><span class="header-section-number">3.2.3</span> Log transformation</h3>
<p>â€¢ ln(ğ‘¥) is the inverse function of ğ‘’ğ‘¥, where ğ‘’ = 2.718 â€¦</p>
<p>â€¢ In other words, ln</p>
<p>= ğ‘¥</p>
<p>â€¢ Example: ğ‘’3 = 20.086; ln(20.086) = 3</p>
<p>â€¢ So, the natural log of ğ‘¥ is the number you would have to raise ğ‘’ to so that youâ€™d get ğ‘¥.</p>
<p>â€¢ Note: in statistics, when we say â€œlogâ€, we usually mean â€œnatural logâ€. It turns out that the distinction is not very important. Iâ€™ll say â€œlog transformâ€</p>
<section id="why-log-transform" class="level4" data-number="3.2.3.1">
<h4 data-number="3.2.3.1" class="anchored" data-anchor-id="why-log-transform"><span class="header-section-number">3.2.3.1</span> Why log transform?</h4>
<p>â€¢ There are two main reasons for log transforming a variable:</p>
<p>â€¢ To correct for skew in data or residuals â€¢ To interpret increases in a variable as multiplicative rather than additive.</p>
<p>â€¢ Both can be understood by recognizing an important property of logarithms; they â€œturn addition into multiplicationâ€</p>
<p>log(ğ´) + log(ğµ) = log(ğ´ğµ)</p>
<p>â€¢ In this sense, logarithms turn addition into multiplication.</p>
<p>â€¢ Example: suppose we have data for a skewed variable ğ‘‹1:</p>
<p>â€¢ Now we define ğ‘¥2 = ln(ğ‘¥1): â€¢ This is a toy â€œdata setâ€. I chose ğ‘¥1 so that ğ‘¥2 = ln</p>
<p>would just be the integers 1 through 10.</p>
<p>â€¢ Note: there is no more skew.</p>
<p>â€¢ Also note: increasing ğ‘¥2 by one unit results in multiplying ğ‘¥1 by ğ‘’.</p>
<p>Addition in ğ‘¥2 = ln</p>
<p>is the same</p>
<p>thing as multiplication in ğ‘¥1.</p>
<p>Same again, with log base 2</p>
<p>â€¢ Even simpler: define ğ‘¥2 as log base 2 of ğ‘¥1, i.e.&nbsp;log2(ğ‘¥1)</p>
<p>â€¢ Now increasing ğ‘¥2 by one unit is equivalent to multiplying ğ‘¥1 by 2.</p>
<p>Addition in ğ‘¥2 = ğ‘™ğ‘œğ‘”2</p>
<p>is the same</p>
<p>thing as multiplication in ğ‘¥1.</p>
</section>
<section id="log-transforming-right-skewed-data" class="level4" data-number="3.2.3.2">
<h4 data-number="3.2.3.2" class="anchored" data-anchor-id="log-transforming-right-skewed-data"><span class="header-section-number">3.2.3.2</span> Log transforming right-skewed data</h4>
<p>â€¢ Skewed data can be bad for regression, in that it can lead to:</p>
<p>â€¢ Non-linear relationship between X and Y â€¢ Influential outliers â€¢ Non-normal residuals â€¢ Non-constant variance in residuals</p>
<p>â€¢ So a simple log transformation can sometimes go a long way toward making the regression model fit the better!</p>
<p>â€¢ It is most common to log transform a response variable, because assumptions about residuals apply to Y, not X.</p>
<p>â€¢ But if X is skewed, the model can benefit from a log transformation of X.</p>
<p>â€¢ Bear in mind that log transformation will affect the interpretation of slope coefficients!</p>
<p>â€¢ If X is log transformed, then a one unit increase in ln(ğ‘‹) corresponds to multiplying X by ğ‘’ â‰ˆ 2.72. So the slope for ln(ğ‘‹) tells you how much Y increases when X is multiplied by 2.72. Or, even better, use log base 2 and the</p>
<p>â€¢ If Y is log transformed, then the interpretations of slopes get more complicated. Hereâ€™s the math, with the error term omitted for convenience:</p>
<p>ln</p>
<p>= ğ›½0 + ğ›½1ğ‘‹ğ‘–</p>
<p>â€¢ Increase X by 1â€¦</p>
<p>âˆ´ ğ‘¦ğ‘– = ğ‘’ğ›½0+ğ›½1ğ‘‹ğ‘–</p>
<p>ğ‘¦âˆ— = ğ‘’ğ›½0+ğ›½1(ğ‘‹ğ‘–+1) = ğ‘’ğ›½0+ğ›½1(ğ‘‹ğ‘–) âˆ™ ğ‘’ğ›½1</p>
<p>â€¢ So, when Y is log transformed, a one unit increase in X multiplies predicted Y by</p>
</section>
<section id="interpreting-slope-as-a-change-in-outcome" class="level4" data-number="3.2.3.3">
<h4 data-number="3.2.3.3" class="anchored" data-anchor-id="interpreting-slope-as-a-change-in-outcome"><span class="header-section-number">3.2.3.3</span> Interpreting slope as a % change in outcome</h4>
<p>â€¢ Recall the heights vs.&nbsp;wages data from group project 1. The paper reported this estimated model:</p>
<p>ln</p>
<p>= ğ›½áˆ˜</p>
<ul>
<li><p>0.002</p></li>
<li><p>0.027</p></li>
<li><p>0.024(Age)</p></li>
</ul>
<p>â€¢ So, when comparing two adults 1 inch apart in height but with the same youth height and age predicted wage is multiplied by ğ‘’0.027 = 1.027 for the taller adult.</p>
<p>â€¢ Multiplying by 1.027 can be thought of as increasing by 2.7%</p>
</section>
<section id="log-transformation-applied-example" class="level4" data-number="3.2.3.4">
<h4 data-number="3.2.3.4" class="anchored" data-anchor-id="log-transformation-applied-example"><span class="header-section-number">3.2.3.4</span> Log transformation applied example</h4>
<p>â€¢ Here is the percent change formula:</p>
<p>% ğ‘â„ğ‘ğ‘›ğ‘”ğ‘’ (ğ‘“ğ‘Ÿğ‘œğ‘š ğ´ ğ‘¡ğ‘œ ğµ) =</p>
<p>â€¢ If B is 1.027*A, then</p>
<p>ğµ âˆ’ ğ´</p>
<p>ğ´</p>
<p>âˆ— 100%</p>
<p>% ğ‘â„ğ‘ğ‘›ğ‘”ğ‘’ =</p>
<p>1.027ğ´ âˆ’</p>
<p>ğ´</p>
<p>âˆ— 100 =</p>
<p>0.027ğ´</p>
<p>ğ´</p>
<p>âˆ— 100 = 2.7%</p>
<p>â€¢ So, when comparing two adults 1 inch apart in height but with the same youth height and age, predicted wage is 2.7% higher for the taller adult.</p>
</section>
<section id="log-transformation-in-y-vs.-in-x" class="level4" data-number="3.2.3.5">
<h4 data-number="3.2.3.5" class="anchored" data-anchor-id="log-transformation-in-y-vs.-in-x"><span class="header-section-number">3.2.3.5</span> Log transformation in Y vs.&nbsp;in X</h4>
<p>â€¢ Remember that log transformation â€œturns addition into multiplicationâ€. So, to keep track of how log transforming Y vs.&nbsp;log transforming X affects your model:</p>
<p>log</p>
<p>= ğ›½0 + ğ›½1ğ‘‹ğ‘– + ğœ€ğ‘– vs.</p>
<p>ğ‘Œğ‘– = ğ›½0 + ğ›½1log(ğ‘‹ğ‘–) + ğœ€ğ‘–</p>
<p>â€¢ If you log transform Y but not X, your model estimates the multiplicative change in predicted Y for an additive change in X.</p>
<p>â€¢ If you log transform X but not Y, your model estimates the additive change</p>
</section>
</section>
<section id="non-linearity" class="level3" data-number="3.2.4">
<h3 data-number="3.2.4" class="anchored" data-anchor-id="non-linearity"><span class="header-section-number">3.2.4</span> Non-linearity</h3>
<p>â€¢ Sometimes data show obvious curvature, in the sense that Y is clearly not a straight line function of X.</p>
<p>â€¢ This will be visible on a plot of Y vs.&nbsp;X. It will also be visible on a residuals vs.&nbsp;predicted values plot after running a regression.</p>
<p>â€¢ If there is curvature in the relationship between Y and X, then it might be sensible to add a polynomial X term:</p>
<p>ğ‘Œğ‘– = ğ›½0 + ğ›½1ğ‘¥ğ‘– + ğ›½2ğ‘¥2 + ğœ€ğ‘–</p>
<section id="polynomial-review" class="level4" data-number="3.2.4.1">
<h4 data-number="3.2.4.1" class="anchored" data-anchor-id="polynomial-review"><span class="header-section-number">3.2.4.1</span> â€œPolynomialâ€ review</h4>
<p>â€¢ A â€œpolynomialâ€ expression is typically one in which variables are included at different powers. For example, a generic third degree polynomial equation might look like:</p>
<p>ğ‘¦ = ğ‘ + ğ‘ğ‘¥ + ğ‘ğ‘¥2 + ğ‘‘ğ‘¥3</p>
<p>â€¢ A â€œsecond degreeâ€ polynomial is one in which an ğ‘¥ and ğ‘¥2 term are both included. This is by far the most common type of polynomial seen in regression models.</p>
<p>2nd degree and 3rd degree polynomials</p>
<p>â€¢ 2nd degree polynomials are often called â€œquadraticâ€. 3rd degree polynomials are often called â€œcubicâ€. Here are visual examples of simulated quadratic and cubic relationships between Y and X:</p>
</section>
<section id="curvature-in-residuals" class="level4" data-number="3.2.4.2">
<h4 data-number="3.2.4.2" class="anchored" data-anchor-id="curvature-in-residuals"><span class="header-section-number">3.2.4.2</span> Curvature in residuals</h4>
<p>â€¢ Here is regression output comparing a linear model to a quadratic model when the relationship between Y and X is quadratic:</p>
<p>â€¢ Here is what the Florida election data look like with the Palm Beach County outlier removed, along with regression results for the simple linear regression model: ğµğ‘¢ğ‘â„ğ‘ğ‘›ğ‘ğ‘›ğ‘– = ğ›½0 + ğ›½1ğ‘‡ğ‘œğ‘¡ğ‘ğ‘™_ğ‘‰ğ‘œğ‘¡ğ‘’ğ‘ ğ‘– + ğœ€ğ‘–</p>
<p>â€¢ Now we will fit a quadratic polynomial model to the same data:</p>
<p>ğµğ‘¢ğ‘â„ğ‘ğ‘›ğ‘ğ‘›ğ‘– = ğ›½0 + ğ›½1ğ‘‡ğ‘œğ‘¡ğ‘ğ‘™_ğ‘‰ğ‘œğ‘¡ğ‘’ğ‘ ğ‘– + ğ›½2ğ‘‡ğ‘œğ‘¡ğ‘ğ‘™_ğ‘‰ğ‘œğ‘¡ğ‘’ğ‘ 2 + ğœ€ğ‘–</p>
<p>â€¢ To create this polynomial predictor in jamovi we first need to install the GAMLj module. Then, select â€œGeneralized Linear Modelsâ€ and last our â€œDependent Variableâ€ and â€œCovariatesâ€.</p>
<p>â€¢ Under the â€œModelâ€ drop down menu, click on Total_Votes in the â€œComponentsâ€ table.</p>
<p>â€¢ An up and down arrow will appear with the degree of Total_Votes, click the up arrow so the 1 changes to 2. Then click the right arrow to add Total_Votes2 to â€œModel Termsâ€</p>
<p>ğµğ‘¢ğ‘â„ğ‘ğ‘›ğ‘ğ‘›ğ‘– = ğ›½0 + ğ›½1ğ‘‡ğ‘œğ‘¡ğ‘ğ‘™_ğ‘‰ğ‘œğ‘¡ğ‘’ğ‘ ğ‘– + ğ›½2ğ‘‡ğ‘œğ‘¡ğ‘ğ‘™_ğ‘‰ğ‘œğ‘¡ğ‘’ğ‘ 2 + ğœ€ğ‘–</p>
<p>â€¢ This is better, but we still see curvature in the residual plot.</p>
<p>â€¢ Letâ€™s try a cubic model:</p>
<p>ğµğ‘¢ğ‘â„ğ‘ğ‘›ğ‘ğ‘›ğ‘– = ğ›½0 + ğ›½1ğ‘‡ğ‘œğ‘¡ğ‘ğ‘™_ğ‘‰ğ‘œğ‘¡ğ‘’ğ‘ ğ‘– + ğ›½2ğ‘‡ğ‘œğ‘¡ğ‘ğ‘™_ğ‘‰ğ‘œğ‘¡ğ‘’ğ‘ 2 + ğ›½3ğ‘‡ğ‘œğ‘¡ğ‘ğ‘™_ğ‘‰ğ‘œğ‘¡ğ‘’ğ‘ 3 + ğœ€ğ‘–</p>
<p>ğµğ‘¢ğ‘â„ğ‘ğ‘›ğ‘ğ‘›ğ‘– = ğ›½0 + ğ›½1ğ‘‡ğ‘œğ‘¡ğ‘ğ‘™_ğ‘‰ğ‘œğ‘¡ğ‘’ğ‘ ğ‘– + ğ›½2ğ‘‡ğ‘œğ‘¡ğ‘ğ‘™_ğ‘‰ğ‘œğ‘¡ğ‘’ğ‘ 2 + ğ›½3ğ‘‡ğ‘œğ‘¡ğ‘ğ‘™_ğ‘‰ğ‘œğ‘¡ğ‘’ğ‘ 3 + ğœ€ğ‘–</p>
<p>â€¢ Itâ€™s debatable whether this is much better. For one, the ğ‘‡ğ‘œğ‘¡ğ‘ğ‘™_ğ‘‰ğ‘œğ‘¡ğ‘’ğ‘ 2 term is non-significant.</p>
<p>â€¢ But think back to multicollinearity. Each polynomial term will be correlated with the other terms â€“ after all, ğ‘‡ğ‘œğ‘¡ğ‘ğ‘™_ğ‘‰ğ‘œğ‘¡ğ‘’ğ‘ , ğ‘‡ğ‘œğ‘¡ğ‘ğ‘™_ğ‘‰ğ‘œğ‘¡ğ‘’ğ‘ 2, and ğ‘‡ğ‘œğ‘¡ğ‘ğ‘™_ğ‘‰ğ‘œğ‘¡ğ‘’ğ‘ 3 must all be correlated.</p>
<p>â€¢ Note that jamovi will not produce VIFs in GLM. Intuitively we know these will be highly correlated with each other.</p>
<p>â€¢ It turns out that centering helps in polynomial models: Compare to:</p>
<p>â€¢ By default jamovi centers polynomials which makes their standard errors smaller than if they were not centered. But, these estimates are not interpretable; you cannot hold Total_Votes2 constant while increasing Total_Votes.</p>
<p>â€¢ In this example, the two counties with the highest total votes are heavily pulling on the regression line.</p>
</section>
</section>
<section id="over-fitting" class="level3" data-number="3.2.5">
<h3 data-number="3.2.5" class="anchored" data-anchor-id="over-fitting"><span class="header-section-number">3.2.5</span> Over-fitting</h3>
<p>â€¢ This model might be â€œover-fitâ€.</p>
<p>â€¢ Over-fitting is when a model fits the data so well that it ends up fitting random variation that is not of interest.</p>
<p>â€¢ Imagine if the two data points on the right had slightly higher vote counts for Buchanan. Or if the next two to their left had slightly lower vote counts. The curve would look very different.</p>
<p>â€¢ At this point, we might just be modelling noise.</p>
<p>â€¢ Here is an extreme example of over-fitting: fitting a â€œsmootherâ€ curve to data and giving it permission to move dramatically up and down through the data.</p>
<p>â€¢ This line fits the data very well, but does it represent the general trend between total votes and votes for Buchanan? Definitely not!</p>
<p>â€¢ (Side note: â€œsmoothersâ€ are great tools for visualizing and summarizing data, but they can be extremely sensitive to degree of smoothing. We wonâ€™t use them in STAT 331)</p>
<p>â€¢ Compare the over-fit model to the linear model.</p>
<p>â€¢ The linear model may be missing out on some curvature. But it might also make better predictions.</p>
<p>â€¢ If we were to observe a new county with 450,000 total votes, would we be better guessing that votes for Buchanan fall on the highly curved line or on the straight line?</p>
</section>
<section id="back-to-basics-is-the-model-sensible" class="level3" data-number="3.2.6">
<h3 data-number="3.2.6" class="anchored" data-anchor-id="back-to-basics-is-the-model-sensible"><span class="header-section-number">3.2.6</span> Back to basics: is the model sensible?</h3>
<p>â€¢ Back to basics: regression models are typically used for two purposes:</p>
<p>o Predicting values of the response variable, using the predictor variables. This is done by plugging values for the predictor variables into the estimated model.</p>
<p>o Estimating the association between each individual predictor variable and the outcome while statistically holding other predictor variables constant. This is done by interpreted estimated slope coefficients.</p>
<section id="if-you-just-want-to-make-predictions" class="level4" data-number="3.2.6.1">
<h4 data-number="3.2.6.1" class="anchored" data-anchor-id="if-you-just-want-to-make-predictions"><span class="header-section-number">3.2.6.1</span> If you just want to make predictions</h4>
<p>â€¢ ğ‘…2 is the easiest to understand statistic for assessing how well your model makes predictions. The closer to 1, the better.</p>
<p>â€¢ Multicollinearity isnâ€™t an issue. It doesnâ€™t affect predicted values.</p>
<p>â€¢ BUT â€“ beware of overfitting! The more complex your model, the more risk you take of modeling noise instead of signal.</p>
<p>â€¢ Also, be aware that ğ‘…2 can never go down when adding predictors. You can add complete nonsense predictor variables, and the worst that will happen to ğ‘…2 is that it stays the same.</p>
</section>
<section id="if-you-want-to-interpret-slopes" class="level4" data-number="3.2.6.2">
<h4 data-number="3.2.6.2" class="anchored" data-anchor-id="if-you-want-to-interpret-slopes"><span class="header-section-number">3.2.6.2</span> If you want to interpret slopes</h4>
<p>â€¢ Always remember that each slope is interpreted under the assumption that all other predictor variables are being held constant, i.e.&nbsp;â€œcontrolled forâ€.</p>
<p>â€¢ The more predictor variables in the model, the less sense this will make.</p>
<p>â€¢ Example: wage vs.&nbsp;height study:</p>
<p>â€¢ In model 4, the estimated slope for youth height can be interpreted as:</p>
<p>â€œThe predicted difference in ln(wage) for two people one inch apart in youth height, but equal in adult height, whose mothers have the same # of years of schooling and are both in either skilled fields or work or not, whose fathers have the same # of years of schooling and are both in either skilled fields or work or not, and who have the same number of siblings.â€</p>
<p>â€¢ Maybe this is the best way to think about the association between youth height and wages. But it is fairly complicated.</p>
<p>â€¢ If youâ€™re going to try to make â€œreal worldâ€ sense out of regression results, your model should be informed by theory.</p>
<p>â€¢ This is necessarily subjective! You have to choose which variables you think are important. You have to think about what makes sense.</p>
<p>â€¢ This might require:</p>
<p>o Log transforming a variable solely because you like the multiplicative interpretation better than the additive interpretation. o Keeping a variable in a model even though it isnâ€™t statistically significant. o Removing a variable you are interested in, because it doesnâ€™t make sense to â€œhold it constantâ€ when estimating slopes for other variables.</p>
</section>
<section id="is-the-model-missing-something-important" class="level4" data-number="3.2.6.3">
<h4 data-number="3.2.6.3" class="anchored" data-anchor-id="is-the-model-missing-something-important"><span class="header-section-number">3.2.6.3</span> Is the model missing something important?</h4>
<p>â€¢ There is another variable in the Florida election data set that could be worth including: â€œReg_Reformâ€: the total number of voters registered with the Reform Party. Pat Buchanan was the Reform Party candidate. Letâ€™s add it to the model:</p>
<p>ğµğ‘¢ğ‘â„ğ‘ğ‘›ğ‘ğ‘›ğ‘– = ğ›½0 + ğ›½1ğ‘‡ğ‘œğ‘¡ğ‘ğ‘™_ğ‘‰ğ‘œğ‘¡ğ‘’ğ‘ ğ‘– + ğ›½2ğ‘…ğ‘’ğ‘”_ğ‘…ğ‘’ğ‘“ğ‘œğ‘Ÿğ‘šğ‘– + ğœ€ğ‘–</p>
<p>â€¢ This residual plot looks great!</p>
<p>â€¢ It turns out that the curvature in the previous residual plots went away when adding an important variable to the model. No need to mess with polynomials after all!</p>
<p>â€¢ Also, the ğ‘…2 is roughly the same as in the cubic model using only total votes as a predictor.</p>
<p>â€¢ So we have roughly equal fit, without having to worry about overfitting, and without having to give up interpretability of the slopes. 40</p>
<p>â€¢ One downside: there is some collinearity. Look at the VIFs.</p>
<p>â€¢ VIF of about 5 implies 1 1âˆ’ğ‘…</p>
<p>â‰ˆ 5 when using the ğ‘…2 from:</p>
<p>ğ‘‡ğ‘œğ‘¡ğ‘ğ‘™_ğ‘‰ğ‘œğ‘¡ğ‘’ğ‘ ğ‘– = ğ›½0 + ğ›½1ğ‘…ğ‘’ğ‘”_ğ‘…ğ‘’ğ‘“ğ‘œğ‘Ÿğ‘šğ‘– + ğœ€ğ‘– â€¢ 2 1</p>
<p>So, this ğ‘…</p>
<p>is about 1 âˆ’ 5</p>
<p>= 0.8. And so ğ‘Ÿ =</p>
<p>= 0.89. These predictors are</p>
<p>strongly correlated.</p>
<p>â€¢ Note also that total votes is not significant.</p>
<p>â€¢ But: the slope for Reg_Reform has a nice interpretation:</p>
<p>When comparing two counties with the same number of votes cast in the election, a county with an additional registered Reform Party member is estimated to have 2.24 additional votes, on average, for Pat Buchanan.</p>
<p>â€¢ Should total votes be taken out of the model? This is a subjective decision.</p>
</section>
<section id="what-would-you-like-to-control-for" class="level4" data-number="3.2.6.4">
<h4 data-number="3.2.6.4" class="anchored" data-anchor-id="what-would-you-like-to-control-for"><span class="header-section-number">3.2.6.4</span> What would you like to â€œcontrolâ€ for?</h4>
<p>â€¢ In regression analysis, we usually emphasize (correctly) that correlation does not imply causation.</p>
<p>â€¢ However, if you have knowledge or beliefs about causal direction, you should take these into account when choosing your variables!</p>
<p>â€¢ Example: in the rheumatoid arthritis study, we were looking at the effect of drugs on inflammation level. In particular, we were comparing how effective they were at reducing inflammation. Suppose we also asked patients to rate their mobility level (RA tends to reduce mobility).</p>
<p>â€¢ Our model might be:</p>
<p>ğ·ğ‘–ğ‘“ğ‘“ğ‘’ğ‘Ÿğ‘’ğ‘›ğ‘ğ‘’ğ‘– = ğ›½0 + ğ›½1ğ‘ğ‘”ğ‘’ğ‘– + ğ›½2ğ‘‘ğ‘Ÿğ‘¢ğ‘”ğ‘– + ğ›½3 ğ‘– + ğ›½4ğ‘šğ‘œğ‘ğ‘–ğ‘™ğ‘–ğ‘¡ğ‘¦ğ‘– + ğœ€ğ‘–</p>
<p>â€¢ Now, when interpreting the previous slopes, I am comparing average reduction in inflammation (â€œdifferenceâ€, the response variable), between two people who have the same mobility level. But if they have the same mobility level, then they will have more similar inflammation levels than if we allowed mobility level to differ.</p>
<p>â€¢ In other words, because the drug reduces inflammation <em>and</em> improves mobility, â€œcontrollingâ€ for mobility will make it look like the drugs are less effective than they really are.</p>
</section>
<section id="beware-the-kitchen-sink-approach" class="level4" data-number="3.2.6.5">
<h4 data-number="3.2.6.5" class="anchored" data-anchor-id="beware-the-kitchen-sink-approach"><span class="header-section-number">3.2.6.5</span> Beware the â€œkitchen sinkâ€ approach</h4>
<p>â€¢ Thereâ€™s an old saying: â€œtaking everything but the kitchen sinkâ€.</p>
<p>â€¢ It can be tempting to toss everything but the kitchen sink into a regression model, especially when you have loads of variables that all seem like theyâ€™d be associated with the response.</p>
<p>â€¢ But beware! Adding in one predictor can have a dramatic effect on the slopes of other predictors, as well as on their standard errors.</p>
<p>â€¢ It really really really matters that each slope is estimated as though all other predictors are held constant. This can reveal otherwise unseen effects, but it can also obscure otherwise obvious effects or induce apparent effects that arenâ€™t real. There</p>
</section>
<section id="the-model-is-simpler-than-whats-being-modeled" class="level4" data-number="3.2.6.6">
<h4 data-number="3.2.6.6" class="anchored" data-anchor-id="the-model-is-simpler-than-whats-being-modeled"><span class="header-section-number">3.2.6.6</span> The model is simpler than whatâ€™s being modeled</h4>
<p>â€¢ Letâ€™s take a step back and ask: why are we fitting data to models?</p>
<p>â€¢ Well, we are interested in the real world. And the real world in incredibly complicated. Maybe incomprehensibly complicated.</p>
<p>â€¢ So, we simplify things using models. We hope that the model captures the essence of what we care about in the real world. But we know it is a simplification; perhaps an extreme simplification.</p>
<p>â€¢ â€œAll models are wrong; some are usefulâ€ â€“ George Box</p>
<p>â€¢ Consider how the regression model describes where data comes from:</p>
<p>ğ‘Œğ‘– = ğ›½0 + ğ›½1ğ‘¥ğ‘– + ğœ€ğ‘– , ğœ€ğ‘–~ğ‘ğ‘œğ‘Ÿğ‘šğ‘ğ‘™(0, ğœ)</p>
<p>â€¢ This says that to get data, we pick a value for X, go to a line, then randomly draw a value from a normal distribution and add this to the value on the line. And thatâ€™s where data comes from!</p>
<p>â€¢ Except, thatâ€™s not where data comes from. This is a model. It is a simplification of reality. We use these models because we think they will help us answer questions we care about (e.g.&nbsp;make predictions, identify associations between variables). Donâ€™t forget that the model is not the thing itself.</p>


</section>
</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "î§‹";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./Ch2_Model_Building.html" class="pagination-link">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Chapter 2: Model building with linear regression</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./Ch4_ANOVA.html" class="pagination-link">
        <span class="nav-page-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Chapter 4: ANOVA-based methods</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->



</body></html>
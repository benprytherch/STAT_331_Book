<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.361">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>STAT 331 - 6&nbsp; Generalized Linear Models (GLMs)</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./Ch7_Mixed.html" rel="next">
<link href="./Ch5_Categorical.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

</head>

<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
      <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./Ch6_GLMs.html"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Generalized Linear Models (GLMs)</span></a></li></ol></nav>
      <a class="flex-grow-1" role="button" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
      </a>
      <button type="button" class="btn quarto-search-button" aria-label="" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">STAT 331</a> 
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Introduction to STAT 331: Intermediate Applied Statistical Methods</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Ch1_Review.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Chapter 1: Review of classical inference</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Ch2_Model_Building.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Chapter 2: Model building with linear regression</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Ch3_Model_Fit.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Chapter 3: Assessing and improving model fit</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Ch4_ANOVA.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Chapter 4: ANOVA-based methods</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Ch5_Categorical.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Chapter 5: Analyzing categorical data</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Ch6_GLMs.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Generalized Linear Models (GLMs)</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Ch7_Mixed.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Chapter 7: Mixed-effects models</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Ch8_Mind_the_Gap.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Chapter 8: Minding the gap between science and statistics</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">References</span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active" data-toc-expanded="99">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#outline-of-notes" id="toc-outline-of-notes" class="nav-link active" data-scroll-target="#outline-of-notes">Outline of notes</a></li>
  <li><a href="#logistic-regression" id="toc-logistic-regression" class="nav-link" data-scroll-target="#logistic-regression"><span class="header-section-number">6.1</span> Logistic regression</a>
  <ul class="collapse">
  <li><a href="#the-logistic-regression-model" id="toc-the-logistic-regression-model" class="nav-link" data-scroll-target="#the-logistic-regression-model"><span class="header-section-number">6.1.1</span> The logistic regression model</a></li>
  <li><a href="#writing-a-logistic-regression-model" id="toc-writing-a-logistic-regression-model" class="nav-link" data-scroll-target="#writing-a-logistic-regression-model"><span class="header-section-number">6.1.2</span> Writing a logistic regression model</a></li>
  <li><a href="#the-theoretical-logistic-regression-model" id="toc-the-theoretical-logistic-regression-model" class="nav-link" data-scroll-target="#the-theoretical-logistic-regression-model"><span class="header-section-number">6.1.3</span> The theoretical logistic regression model</a></li>
  <li><a href="#logit-log-odds" id="toc-logit-log-odds" class="nav-link" data-scroll-target="#logit-log-odds"><span class="header-section-number">6.1.4</span> Logit: log odds</a></li>
  </ul></li>
  <li><a href="#odds" id="toc-odds" class="nav-link" data-scroll-target="#odds"><span class="header-section-number">6.2</span> Odds</a>
  <ul class="collapse">
  <li><a href="#odds-vs.-probability" id="toc-odds-vs.-probability" class="nav-link" data-scroll-target="#odds-vs.-probability"><span class="header-section-number">6.2.1</span> Odds vs.&nbsp;probability</a></li>
  <li><a href="#back-to-the-logistic-regression-model" id="toc-back-to-the-logistic-regression-model" class="nav-link" data-scroll-target="#back-to-the-logistic-regression-model"><span class="header-section-number">6.2.2</span> Back to the logistic regression model</a></li>
  <li><a href="#jamovi-example" id="toc-jamovi-example" class="nav-link" data-scroll-target="#jamovi-example"><span class="header-section-number">6.2.3</span> jamovi example</a></li>
  </ul></li>
  <li><a href="#interpreting-slope-in-logistic-regression" id="toc-interpreting-slope-in-logistic-regression" class="nav-link" data-scroll-target="#interpreting-slope-in-logistic-regression"><span class="header-section-number">6.3</span> Interpreting slope in logistic regression</a></li>
  <li><a href="#poisson-regression" id="toc-poisson-regression" class="nav-link" data-scroll-target="#poisson-regression"><span class="header-section-number">6.4</span> Poisson regression</a>
  <ul class="collapse">
  <li><a href="#the-poisson-distribution" id="toc-the-poisson-distribution" class="nav-link" data-scroll-target="#the-poisson-distribution"><span class="header-section-number">6.4.1</span> The Poisson distribution</a></li>
  <li><a href="#visualizing-the-poisson-distribution" id="toc-visualizing-the-poisson-distribution" class="nav-link" data-scroll-target="#visualizing-the-poisson-distribution"><span class="header-section-number">6.4.2</span> Visualizing the Poisson distribution</a></li>
  </ul></li>
  <li><a href="#the-structure-of-a-glm" id="toc-the-structure-of-a-glm" class="nav-link" data-scroll-target="#the-structure-of-a-glm"><span class="header-section-number">6.5</span> The structure of a GLM</a>
  <ul class="collapse">
  <li><a href="#the-link-function" id="toc-the-link-function" class="nav-link" data-scroll-target="#the-link-function"><span class="header-section-number">6.5.1</span> The link function</a></li>
  <li><a href="#glm-examples" id="toc-glm-examples" class="nav-link" data-scroll-target="#glm-examples"><span class="header-section-number">6.5.2</span> GLM examples</a></li>
  </ul></li>
  <li><a href="#maximum-likelihood-estimation-conceptually" id="toc-maximum-likelihood-estimation-conceptually" class="nav-link" data-scroll-target="#maximum-likelihood-estimation-conceptually"><span class="header-section-number">6.6</span> Maximum likelihood estimation, conceptually</a></li>
  <li><a href="#poisson-regression-example" id="toc-poisson-regression-example" class="nav-link" data-scroll-target="#poisson-regression-example"><span class="header-section-number">6.7</span> Poisson regression example</a>
  <ul class="collapse">
  <li><a href="#poisson-regression-eda" id="toc-poisson-regression-eda" class="nav-link" data-scroll-target="#poisson-regression-eda"><span class="header-section-number">6.7.1</span> Poisson regression EDA</a></li>
  <li><a href="#interpreting-the-slope" id="toc-interpreting-the-slope" class="nav-link" data-scroll-target="#interpreting-the-slope"><span class="header-section-number">6.7.2</span> Interpreting the slope</a></li>
  <li><a href="#overdispersion" id="toc-overdispersion" class="nav-link" data-scroll-target="#overdispersion"><span class="header-section-number">6.7.3</span> Overdispersion</a></li>
  <li><a href="#fitting-a-larger-model" id="toc-fitting-a-larger-model" class="nav-link" data-scroll-target="#fitting-a-larger-model"><span class="header-section-number">6.7.4</span> Fitting a larger model</a></li>
  </ul></li>
  <li><a href="#negative-binomial-regression" id="toc-negative-binomial-regression" class="nav-link" data-scroll-target="#negative-binomial-regression"><span class="header-section-number">6.8</span> Negative binomial regression</a>
  <ul class="collapse">
  <li><a href="#negative-binomial-example" id="toc-negative-binomial-example" class="nav-link" data-scroll-target="#negative-binomial-example"><span class="header-section-number">6.8.1</span> Negative binomial example</a></li>
  <li><a href="#negative-binomial-results" id="toc-negative-binomial-results" class="nav-link" data-scroll-target="#negative-binomial-results"><span class="header-section-number">6.8.2</span> Negative binomial results</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Generalized Linear Models (GLMs)</span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  

</header>

<section id="outline-of-notes" class="level2 unnumbered">
<h2 class="unnumbered anchored" data-anchor-id="outline-of-notes">Outline of notes</h2>
<ol type="1">
<li>Logistic regression</li>
<li>Odds</li>
<li>Interpreting slope in logistic regression</li>
<li>Poisson regression</li>
<li>The structure of a GLM</li>
<li>Maximum likelihood estimation, conceptually</li>
<li>Poisson regression example</li>
<li>Negative binomial regression</li>
</ol>
</section>
<section id="logistic-regression" class="level2" data-number="6.1">
<h2 data-number="6.1" class="anchored" data-anchor-id="logistic-regression"><span class="header-section-number">6.1</span> Logistic regression</h2>
<p>All of the regression methods we’ve seen have involved models in which the response variable is normally distributed, given values for the predictor variables</p>
<p>In other words, the residuals have been modeled as normal.</p>
<p>What if we have a different kind of response variable? In particular, consider a binary response variable. Maybe the outcomes are “yes” and “no”, or “success” and “failure”, or “present” and “absent”.</p>
<p>Logistic regression is a type of “generalized linear model” (GLM) that works well for modeling binary outcome data.</p>
<p>Before we get into logistic regression, though, let’s see what happens if we use standard regression (sometimes called “ordinary least squares”, or OLS regression) with a binary response.</p>
<p>We’ll use simulated data corresponding to a study of sexual harassment reporting at a university. (Brooks and Perot “Reporting Sexual Harassment: Exploring a Predictive Model” (1991)).</p>
<p>Here is data on whether or not sexual harassment at a university was reported, using the offensiveness of the behavior as a predictor variable:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/Mod6_pt2_1.png" class="img-fluid figure-img" width="400"></p>
</figure>
</div>
<p>Data points are “jittered” so that they don’t fall right on top of one another.</p>
<p>Suppose we want to predict the value of “Report”, using “OffensBeh”.</p>
<p>Here is the linear regression line. In this picture, the response variable takes on the values 0 and 1, and the data are not jittered.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/Mod6_pt2_2.png" class="img-fluid figure-img" width="400"></p>
</figure>
</div>
<p>The predicted value of “Report” can be thought of as the predicted probability that Report=1 (for reported behavior)</p>
<p>Note that this line can go below zero and above one. We don’t want to predict probability greater than 1! A straight line is not great here. Logistic regression is an alternative to this straight line model.</p>
<section id="the-logistic-regression-model" class="level3" data-number="6.1.1">
<h3 data-number="6.1.1" class="anchored" data-anchor-id="the-logistic-regression-model"><span class="header-section-number">6.1.1</span> The logistic regression model</h3>
<p>By now we are well familiar with the linear regression model:</p>
<p><span class="math display">\[
Y_i=\beta_0+\beta_1x_{1i} + \beta_2x_{2i} + \dots + \beta_px_{pi} + \varepsilon_i \\
\varepsilon_i \sim \text{Normal}(0,\sigma^2)
\]</span></p>
<p>Here is an equivalent way of writing it:</p>
<p><span class="math display">\[
Y_i \sim Normal(\mu_i, \sigma^2) \\
\mu_i=\beta_0+\beta_1x_{1i} + \beta_2x_{2i} + \dots + \beta_px_{pi}
\]</span></p>
<p>In other words, the response variable is normally distributed with some mean <span class="math inline">\(\mu\)</span>, and the value of <span class="math inline">\(\mu\)</span> is determined by the predictor (<span class="math inline">\(x\)</span>) variables.</p>
</section>
<section id="writing-a-logistic-regression-model" class="level3" data-number="6.1.2">
<h3 data-number="6.1.2" class="anchored" data-anchor-id="writing-a-logistic-regression-model"><span class="header-section-number">6.1.2</span> Writing a logistic regression model</h3>
<p>We will take this approach to writing the logistic regression model.</p>
<p>What we want is a regression equation that looks like this:</p>
<p><span class="math display">\[
Y_i=\beta_0+\beta_1x_{1i} + \beta_2x_{2i} + \dots + \beta_px_{pi}
\]</span></p>
<p>But that will work when <span class="math inline">\(Y_i\)</span> does not follow a normal distribution.</p>
</section>
<section id="the-theoretical-logistic-regression-model" class="level3" data-number="6.1.3">
<h3 data-number="6.1.3" class="anchored" data-anchor-id="the-theoretical-logistic-regression-model"><span class="header-section-number">6.1.3</span> The theoretical logistic regression model</h3>
<p>Response variable <span class="math inline">\(Y\)</span> takes on the values 0 and 1.</p>
<p>Denote the probability that <span class="math inline">\(Y = 1\)</span> as <span class="math inline">\(\pi\)</span>.</p>
<p>This can be written <span class="math inline">\(Y\sim Bernoulli (\pi)\)</span></p>
<p>(The Bernoulli distribution is a distribution of 1’s and 0’s, where the probability of 1 is <span class="math inline">\(\pi\)</span> and the probability of 0 is <span class="math inline">\(1 − \pi\)</span>)</p>
<p>We will use regression to model <span class="math inline">\(\pi\)</span>, the probability that <span class="math inline">\(Y = 1\)</span>. This is often thought of as the probability of a “success”.</p>
<p>If we wanted, we could use this model:</p>
<p><span class="math display">\[
Y_i\sim Bernoulli (\pi_i)
\\
\pi_i=\beta_0+\beta_1x_{1i} + \beta_2x_{2i} + \dots + \beta_px_{pi}
\]</span></p>
<p>The standard deviation of a Bernoulli distribution is <span class="math inline">\(\pi(1 − \pi )\)</span>. So, <span class="math inline">\(\pi\)</span> is the only parameter for this distribution. This is different from the normal distribution, which has two parameters <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\sigma\)</span>.</p>
</section>
<section id="logit-log-odds" class="level3" data-number="6.1.4">
<h3 data-number="6.1.4" class="anchored" data-anchor-id="logit-log-odds"><span class="header-section-number">6.1.4</span> Logit: log odds</h3>
<p>But, as we saw in the opening example, a linear model for probability can have serious deficiencies.</p>
<p>So, instead of a linear model for probability <span class="math inline">\(\pi\)</span>, we’ll make a linear model for a function of <span class="math inline">\(\pi\)</span>, so that the variable on the left hand side of the equation is linearly related to the variable(s) on the right.</p>
<p>In logistic regression, we use the “logit” function, also known as “log odds”</p>
<p><span class="math display">\[
logit(\pi) = ln(\frac{\pi}{1-\pi}) = \textit{"log odds"}
\]</span></p>
<p>Applied to our sexual harassment example, we would like to predict the probability that harassing behavior is reported. This probability is denoted <span class="math inline">\(\pi\)</span></p>
<p>Plugging this into the logit formula:</p>
<p><span class="math display">\[
logit(P(reported)) = ln(\frac{P(reported)}{1-P(reported)}) = \textit{"log odds" of reporting}
\]</span></p>
<p>This will be our response variable for logistic regression. Note that this time we wrote <span class="math inline">\(P(reported)\)</span> rather than <span class="math inline">\(\pi\)</span>; these mean the same thing.</p>
<p>Logit vs.&nbsp;probability, visually:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/Mod6_pt2_3.png" class="img-fluid figure-img"></p>
</figure>
</div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/Mod6_pt2_4.png" class="img-fluid figure-img"></p>
</figure>
</div>
</section>
</section>
<section id="odds" class="level2" data-number="6.2">
<h2 data-number="6.2" class="anchored" data-anchor-id="odds"><span class="header-section-number">6.2</span> Odds</h2>
<section id="odds-vs.-probability" class="level3" data-number="6.2.1">
<h3 data-number="6.2.1" class="anchored" data-anchor-id="odds-vs.-probability"><span class="header-section-number">6.2.1</span> Odds vs.&nbsp;probability</h3>
<p>To understand logistic regression, you’ll need to understand odds.</p>
<p>In casual English, “odds” and “probability” are often used interchangeably.</p>
<p>In statistics, they are not the same thing. Odds tell you how likely one outcome is compared to another.</p>
<p>For instance, you might hear that a football team has been given “3 to 2” odds of winning a game. This means that their probability of winning is <span class="math inline">\(\frac{3}{2}= 1.5\)</span> times as big as their probability of losing. Or, that they’d be expected to win 3 times for every 2 times they lost.</p>
<p>Formally, consider some outcome A, where the probability of A occurring is written as “<span class="math inline">\(P(A)\)</span>”. In this case,</p>
<p><span class="math display">\[
odds(A) = \frac{P(A)}{1-P(A)} = \frac{\textit{probability A occurs}}{\textit{probability A does not occur}}
\]</span></p>
<p>This is the ratio of the probability A occurs to the probability A does not occur. To convert odds into probability, we use:</p>
<p><span class="math display">\[
P(A) = \frac{odds(A)}{odds(A) + 1}
\]</span></p>
<p>Some probabilities and their associated odds:</p>
<table class="table">
<thead>
<tr class="header">
<th><span class="math inline">\(P(A)=\)</span></th>
<th><span class="math inline">\(Odds(A)=\frac{P(A)}{1-P(A)}\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>0.001</td>
<td>0.001/0.999=0.001001</td>
</tr>
<tr class="even">
<td>0.05</td>
<td>0.05/0.95=0.0526</td>
</tr>
<tr class="odd">
<td>0.2</td>
<td>0.2/0.4=0.25</td>
</tr>
<tr class="even">
<td>0.5</td>
<td>0.5/0.5=1</td>
</tr>
<tr class="odd">
<td>0.8</td>
<td>0.8/0.2=4</td>
</tr>
<tr class="even">
<td>0.95</td>
<td>0.95/0.05=19</td>
</tr>
<tr class="odd">
<td>0.999</td>
<td>0.999/0.001=999</td>
</tr>
</tbody>
</table>
<p>Think of odds(A) as “how many times will A occur for every time A does not occur?”</p>
<p>Sometimes we add “to 1” to an odds statement, e.g.&nbsp;“odds of 4 to 1” means “this outcomes occurs 4 times for every 1 time it does not occur.”</p>
</section>
<section id="back-to-the-logistic-regression-model" class="level3" data-number="6.2.2">
<h3 data-number="6.2.2" class="anchored" data-anchor-id="back-to-the-logistic-regression-model"><span class="header-section-number">6.2.2</span> Back to the logistic regression model</h3>
<p>The response variable for logistic regression, again, is:</p>
<p><span class="math display">\[
logit(\pi) = ln(\frac{\pi}{1-\pi}) = \textit{"log odds"}
\]</span></p>
<p>So, the full logistic regression model is :</p>
<p><span class="math display">\[
Y_i\sim Bernoulli (\pi_i)
\\
ln(\frac{\pi_i}{1-\pi_i})=\beta_0+\beta_1x_{1i} + \beta_2x_{2i} + \dots + \beta_px_{pi}
\]</span></p>
<p>We are not actually interested in log odds; we only make this conversion for mathematical convenience. So, once we have <span class="math inline">\(logit(\hat{\pi}_i)\)</span>, we can <span class="math inline">\(\hat{\pi}_i\)</span> back via the inverse logit function, <span class="math inline">\(logit^{-1}(x)=\frac{e^x}{1+e^x}\)</span>:</p>
<p><span class="math display">\[
\begin{align}
logit^{-1}(logit(\pi)) \\
&amp;= \frac{e^{logit(\pi)}}{1 + e^{logit(\pi)}} \\
&amp;= \frac{odds}{1 + odds} \\
&amp;= \frac{\frac{\pi}{1-\pi}}{\frac{1-\pi}{1 - \pi} + \frac{\pi}{1-\pi}} \\
&amp;= \frac{\frac{\pi}{1-\pi}}{\frac{1}{1-\pi}} \\
&amp;= \pi
\end{align}
\]</span></p>
<p>In the context of the logistic regression model:</p>
<p><span class="math display">\[
\begin{align}
\hat{\pi}_i &amp; =logit^{-1}(logit(\hat{\pi_i}))\\
&amp;=\frac{e^{logit(\hat{\pi_i})}}{1 + e^{logit(\hat{\pi_i})}} \\
&amp;=\frac{e^{\hat{\beta}_0 + \hat{\beta}_1x_{1i} + \hat{\beta}_2x_{2i} + \dots + \hat{\beta}_px_{pi}}}{1 + e^{\hat{\beta}_0 + \hat{\beta}_1x_{1i} + \hat{\beta}_2x_{2i} + \dots + \hat{\beta}_px_{pi}}}
\end{align}
\]</span></p>
</section>
<section id="jamovi-example" class="level3" data-number="6.2.3">
<h3 data-number="6.2.3" class="anchored" data-anchor-id="jamovi-example"><span class="header-section-number">6.2.3</span> jamovi example</h3>
<p>Applying this to the harassment data, we use Linear Models / Generalized Linear Models in jamovi and select Logistic under Categorical dependent variable.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/Mod6_pt2_6.png" class="img-fluid figure-img" width="398"></p>
</figure>
</div>
<p>Note that “Target Level” defaults to zero. Changing it to 1 makes sense in this case; we want to predict 𝑃(𝑅𝑒𝑝𝑜𝑟𝑡𝑒𝑑).</p>
<p>jamovi also produces a Loglikelihood ratio test, but we will just focus on “Parameter Estimates”:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/Mod6_pt2_7.png" class="img-fluid figure-img" width="500"></p>
</figure>
</div>
<p>Here is our estimated model:</p>
<p><span class="math display">\[
\textit{log odds (Reported)} = -1.7976 + 0.4869*OffensBeh
\]</span></p>
<p>Plugging in large and small values for OffensBeh:</p>
<p><span class="math display">\[
\textit{log odds (Reported)} = -1.7976 + 0.4869*1 = -1.3107
\]</span> <span class="math display">\[
\textit{log odds (Reported)} = -1.7976 + 0.4869*8 = 2.0976
\]</span></p>
<p><span class="math display">\[
\hat{\pi}|OffensBeh = 1: \\ \frac{e^{-1.3107}}{1 + e^{-1.3107}} = \frac{0.2696}{1.2696} = 0.21
\]</span> <span class="math display">\[
\hat{\pi}|OffensBeh = 8: \\ \frac{e^{-2.0976}}{1 + e^{-2.0976}} = \frac{8.1466}{9.1466} = 0.89
\]</span></p>
</section>
</section>
<section id="interpreting-slope-in-logistic-regression" class="level2" data-number="6.3">
<h2 data-number="6.3" class="anchored" data-anchor-id="interpreting-slope-in-logistic-regression"><span class="header-section-number">6.3</span> Interpreting slope in logistic regression</h2>
<p>The slope coefficient is directly interpreted as change in log odds for a one unit increase in the predictor. “Log odds” are not of direct interest.</p>
<p>Exponentiating both sides of the equation gives straight odds</p>
<p><span class="math display">\[
odds = (\frac{\pi_i}{1-\pi_i})=e^{\beta_0+\beta_1x_{1i} + \beta_2x_{2i} + \dots + \beta_px_{pi}}
\]</span></p>
<p>This means that, for a one unit increase in <span class="math inline">\(X_1\)</span>, odds are multiplied by <span class="math inline">\(e^\beta_1\)</span></p>
<p>For the harassment data:</p>
<p><span class="math display">\[
\textit{log odds (Reported)} = -1.7976 + 0.4869*OffensBeh
\]</span></p>
<p><span class="math inline">\(\hat{\beta}_1 = 0.4869\)</span>. So, for a one unit increase in OffensBeh, predicted odds of reporting are multiplied by <span class="math inline">\(e^{0.4869} = 1.627\)</span></p>
<p>In other words, there is about a 63% increase in odds of reporting when OffensBeh increases by one. NOTE: odds are not probabilities!</p>
<p>Comparing probabilities and odds from this model:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/Mod6_pt2_8.png" class="img-fluid figure-img" width="500"></p>
</figure>
</div>
</section>
<section id="poisson-regression" class="level2" data-number="6.4">
<h2 data-number="6.4" class="anchored" data-anchor-id="poisson-regression"><span class="header-section-number">6.4</span> Poisson regression</h2>
<p>We’ll now look at two other popular GLMs: Poisson (“pwa-sawn” roughly) and negative binomial.</p>
<p>These are used for modeling count data, which can be extended to how often a categorical variable takes on some value. Thus Poisson regression can be used to model contingency table data.</p>
<section id="the-poisson-distribution" class="level3" data-number="6.4.1">
<h3 data-number="6.4.1" class="anchored" data-anchor-id="the-poisson-distribution"><span class="header-section-number">6.4.1</span> The Poisson distribution</h3>
<p>The Poisson distribution is a discrete probability distribution. A Poisson distributed variable takes on only positive integer values. The integer is referred to as “count” or “# of events”.</p>
<p>The Poisson distribution has a single parameter, <span class="math inline">\(\lambda\)</span> (“lambda”), which is sometimes called the “rate” parameter.</p>
<p><span class="math inline">\(\lambda\)</span> is both the mean and the variance of a Poisson distribution</p>
<p>The probability function for the Poisson is:</p>
<p><span class="math display">\[
P(count = k) = \frac{\lambda^k}{e^\lambda k!}
\]</span></p>
</section>
<section id="visualizing-the-poisson-distribution" class="level3" data-number="6.4.2">
<h3 data-number="6.4.2" class="anchored" data-anchor-id="visualizing-the-poisson-distribution"><span class="header-section-number">6.4.2</span> Visualizing the Poisson distribution</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/Mod6_pt2_9.png" class="img-fluid figure-img"></p>
</figure>
</div>
</section>
</section>
<section id="the-structure-of-a-glm" class="level2" data-number="6.5">
<h2 data-number="6.5" class="anchored" data-anchor-id="the-structure-of-a-glm"><span class="header-section-number">6.5</span> The structure of a GLM</h2>
<section id="the-link-function" class="level3" data-number="6.5.1">
<h3 data-number="6.5.1" class="anchored" data-anchor-id="the-link-function"><span class="header-section-number">6.5.1</span> The link function</h3>
<p>In logistic regression, the response variable was $logit(\pi) = ln(\frac{\pi}{1-\pi}) $</p>
<p>In Poisson regression, the response variable is <span class="math inline">\(ln(\lambda)\)</span></p>
<p>The reasoning will be that the natural log allows the estimated rate to be modeled as a linear function of some predictor variables.</p>
<p>This is how GLMs work: they allow us to use non-normal response variables by expressing a function of their mean as a linear function of the predictors.</p>
<p>A GLM has three parts:</p>
<ol type="1">
<li><p>A response variable with some distribution</p></li>
<li><p>A “link function”, <span class="math inline">\(g(\cdot)\)</span>, that is applied to the mean of the response variable.</p></li>
<li><p>A linear expression of the predictor variables: <span class="math inline">\(\beta_0 + \beta_1X_1 + \beta_2X_2 \dots\)</span></p></li>
</ol>
</section>
<section id="glm-examples" class="level3" data-number="6.5.2">
<h3 data-number="6.5.2" class="anchored" data-anchor-id="glm-examples"><span class="header-section-number">6.5.2</span> GLM examples</h3>
<p>Logistic regression uses <span class="math inline">\(Y\sim Bernoulli (\pi)\)</span> as the response variable and <span class="math inline">\(g(\pi) = ln(\frac{\pi}{1-\pi})\)</span> as the link function.</p>
<p>Poisson regression uses <span class="math inline">\(Y\sim Poisson(\lambda)\)</span> as the response variable and <span class="math inline">\(g(\lambda) = ln(\lambda)\)</span> as the link function.</p>
<p>Ordinary least squares (OLS) regression can also be considered a special case of a GLM. It uses <span class="math inline">\(Y\sim Normal (\mu, \sigma^2)\)</span> as the response variable and the identity function, <span class="math inline">\(g(\mu) = \mu\)</span> as the link.</p>
</section>
</section>
<section id="maximum-likelihood-estimation-conceptually" class="level2" data-number="6.6">
<h2 data-number="6.6" class="anchored" data-anchor-id="maximum-likelihood-estimation-conceptually"><span class="header-section-number">6.6</span> Maximum likelihood estimation, conceptually</h2>
<p>It’s worth briefly noting that the mathematical method used to come up with parameter estimates for GLMs is not “least squares”. So, we are not getting our <span class="math inline">\(\beta\)</span>’s by minimizing sums of squared residuals.</p>
<p>Instead, the estimation procedure we use is called “maximum likelihood”. This method finds the values of the parameter estimates that maximize (i.e.&nbsp;make as large as possible for a given set of data) something called “the likelihood function”. The likelihood function takes a fixed set of data and an assumed distribution (e.g.&nbsp;normal), and gives the “probability of the data”, given some set of parameter values.</p>
<p>So, the coefficient estimates that we get in GLM output would make our data “more likely” than our data would be under any other set of possible estimates. They are the estimates that maximize the likelihood of our data.</p>
</section>
<section id="poisson-regression-example" class="level2" data-number="6.7">
<h2 data-number="6.7" class="anchored" data-anchor-id="poisson-regression-example"><span class="header-section-number">6.7</span> Poisson regression example</h2>
<p>We’ll use some General Social Survey data for this example.</p>
<p>Poisson is good for modeling count data, so we’ll use a response variable that takes the form of counts.</p>
<p>For this example, the goal will be to look at the relationship (if any) between the number of sibling a person has, and the number of children that person has.</p>
<p>Our question will be: do people with more siblings tend to have more children? And if so, can we quantify the relationship?</p>
<p>It would be wise to collect data on covariates that we expect will also be related to the number of children someone has.</p>
<p>An obvious one is age. Older people will have more children than younger people.</p>
<p>We might also want to control for “culture”. If people from different cultural backgrounds tend to have more or fewer children, then this would definitely induce a relationship between # of siblings and # of children.</p>
<p>There are lots of possible ways to try account for cultural background. I’m choosing rate of attending religious services.</p>
<ul>
<li><p>So, the variables will be:</p>
<ul>
<li><p># of children</p></li>
<li><p># of siblings</p></li>
<li><p>Age</p></li>
<li><p>Frequency of attending religious services</p></li>
</ul></li>
</ul>
<p>We’ll just look at 2018 data. The GSS lets us choose any years we want, going back to 1972.</p>
<p>This data set is on Canvas, as GSS_Children_Siblings.jmp</p>
<section id="poisson-regression-eda" class="level3" data-number="6.7.1">
<h3 data-number="6.7.1" class="anchored" data-anchor-id="poisson-regression-eda"><span class="header-section-number">6.7.1</span> Poisson regression EDA</h3>
<p>First thing to do is plot our variables.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/Mod6_pt2_10.png" class="img-fluid figure-img" width="400"></p>
</figure>
</div>
<p>Yikes! There’s some cleaning to do. The instances of 98 siblings are not real data points.</p>
<p>GSS data explorer website lets us look in detail at each variable. Here is part of the coding for “SIBS”:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/Mod6_pt2_11.png" class="img-fluid figure-img" width="202"></p>
</figure>
</div>
<p>And here’s the coding for the variable CHILDS.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/Mod6_pt2_12.png" class="img-fluid figure-img" width="147"></p>
</figure>
</div>
<p>So, CHILDS = 9 is a value for missing data. These should also be excluded.</p>
<p>This should feel familiar – remember how messy the NLSY data was in the heights analysis?</p>
<p>Keep in mind that data in public databases often have idiosyncrasies like this.</p>
<p>Here are the row selection options that will select all rows with invalid responses.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/Mod6_pt2_13.png" class="img-fluid figure-img"></p>
</figure>
</div>
<p>Once selected, they can be excluded.</p>
<p>Here is the distribution of CHILDS.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/Mod6_pt2_14.png" class="img-fluid figure-img" width="398"></p>
</figure>
</div>
<p>This looks a lot like a Poisson distribution. Hooray!</p>
<p>To run the regression, use Linear Models / Generalized Linear Models and choose Poisson(overdispersion) for Frequencies.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/Mod6_pt2_15.png" class="img-fluid figure-img" width="300"></p>
</figure>
</div>
<p>JMP will automatically choose the log link</p>
<p>Select the “Overdispersion tests and intervals” box</p>
<p>Here are results for a simple model, where # of siblings is the sole predictor of # of children. We’ll just look at the parameter estimates and the overdispersion statistic:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/Mod6_pt2_16.png" class="img-fluid figure-img"></p>
</figure>
</div>
<p>Letting <span class="math inline">\(\hat{\lambda}\)</span> represent the predicted mean # of children, we have:</p>
<p><span class="math display">\[
ln(\hat{\lambda}) = 0.375 + 0.0631(𝑆𝐼𝐵𝑆)
\]</span></p>
</section>
<section id="interpreting-the-slope" class="level3" data-number="6.7.2">
<h3 data-number="6.7.2" class="anchored" data-anchor-id="interpreting-the-slope"><span class="header-section-number">6.7.2</span> Interpreting the slope</h3>
<p>You might not be surprised to learn that, due to the log link, the exponentiated slope is interpreted as the multiplicative change in the estimated value of the response variable, given a one unit increase in the predictor variable: <span class="math inline">\(e^{0.0627} = 1.065\)</span></p>
<p><img src="images/Mod6_pt2_17.png" class="img-fluid" width="511"><br>
So, increasing # of siblings by one is associated with an 6.5% increase in # of children.</p>
<p>We can see that this is statistically significant, but it is also small.</p>
<p>It is also not obvious that % change is the best way to quantify this. Maybe an OLS model would have been more interpretable.</p>
<p>It might be more desirable to relate an additive change in siblings to an additive change in children.</p>
<p>Downside is that # of children is not normally distributed.</p>
<p>As is often the case, we are trading some interpretability for a better fitting model.</p>
</section>
<section id="overdispersion" class="level3" data-number="6.7.3">
<h3 data-number="6.7.3" class="anchored" data-anchor-id="overdispersion"><span class="header-section-number">6.7.3</span> Overdispersion</h3>
<p>The Poisson distribution makes a strong assumption: the mean should be equal to the variance.</p>
<p>Often, we observe real data in which the variance is greater than the mean.</p>
<p>This is referred to as “overdispersion”.</p>
<p>Jamovi reports an overdispersion estimate:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/Mod6_pt2_18.png" class="img-fluid figure-img"></p>
</figure>
</div>
<p>The overdispersion statistic is the ratio of the Pearson chi-square statistic to its degrees of freedom.</p>
<p>If mean = variance, this should be equal to 1. But it rarely is. If there is strong overdispersion, a negative binomial model should fit better.</p>
</section>
<section id="fitting-a-larger-model" class="level3" data-number="6.7.4">
<h3 data-number="6.7.4" class="anchored" data-anchor-id="fitting-a-larger-model"><span class="header-section-number">6.7.4</span> Fitting a larger model</h3>
<p>For these data, it turns out that # of siblings, age, frequency of attending religious services, and the interactions between age and the other two variables are all statistically significant and all improve model fit. Here are the parameter estimate results for this model:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/Mod6_pt2_19.png" class="img-fluid figure-img"></p>
</figure>
</div>
<p>The other predictor variables and the interactions can be interpreted in the usual ways.</p>
<p>One thing to notice is that the estimate for SIBS has not changed much. So, while the other covariates and interactions matter, they don’t substantially change our interpretation of the SIBS predictor.</p>
</section>
</section>
<section id="negative-binomial-regression" class="level2" data-number="6.8">
<h2 data-number="6.8" class="anchored" data-anchor-id="negative-binomial-regression"><span class="header-section-number">6.8</span> Negative binomial regression</h2>
<p>There is also still some overdispersion, though less than there was before:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/Mod6_pt2_20.png" class="img-fluid figure-img"></p>
</figure>
</div>
<p>Remember that the Poisson distribution only has one parameter. This limits its flexibility.</p>
<p>The negative binomial distribution is similar to the Poisson distribution, but it is more flexible, and may be a better choice in the presence of overdispersion.</p>
<p>The negative binomial distribution is also a distribution for count data. It is interpreted as giving the number of “success” before a certain number of “failures” occur.</p>
<p>There are two parameters: <span class="math inline">\(p\)</span>, the probability of success, and <span class="math inline">\(r\)</span>, the number of failures at which counting stops.</p>
<p>If a variable <span class="math inline">\(Y\)</span> is distributed negative binomial, we denote it:</p>
<p><span class="math display">\[
Y\sim NB(r,p)
\]</span></p>
<p>The mean of the negative binomial distribution is <span class="math inline">\(\frac{rp}{1-p} = r*odds(success)\)</span></p>
<p>Example: suppose <span class="math inline">\(p = 0.8\)</span> and <span class="math inline">\(r = 2\)</span>. We expect <span class="math inline">\(\frac{2∗0.8}{0.2} = 8\)</span> success before we observe two failures.</p>
<p>Or suppose <span class="math inline">\(p= 0.5\)</span> and <span class="math inline">\(r= 1\)</span>. We expect <span class="math inline">\(\frac{1∗0.5}{0.5} = 1\)</span> success before we observe 1 failure.</p>
<p>For practical purposes, negative binomial regression will show better fit than Poisson regression in the presence of overdispersion.</p>
<p>The tradeoff is that the interpretation is less generally applicable. It might not make sense to think of your count variable as # of successes for a certain # of failures.</p>
<p>As with Poisson regression, negative binomial regression uses a GLM with a log link.</p>
<section id="negative-binomial-example" class="level3" data-number="6.8.1">
<h3 data-number="6.8.1" class="anchored" data-anchor-id="negative-binomial-example"><span class="header-section-number">6.8.1</span> Negative binomial example</h3>
<p>Here is where you select negative binomial regression in jamovi:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/Mod6_pt2_21.png" class="img-fluid figure-img"></p>
</figure>
</div>
<p>Under “Generalized Linear Models”, under “Frequencies” choose “Negative Binomial”.</p>
</section>
<section id="negative-binomial-results" class="level3" data-number="6.8.2">
<h3 data-number="6.8.2" class="anchored" data-anchor-id="negative-binomial-results"><span class="header-section-number">6.8.2</span> Negative binomial results</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/Mod6_pt2_22.png" class="img-fluid figure-img"></p>
</figure>
</div>
<p>These results are awfully similar to the Poisson results.</p>
<p>It is often the case that different statistical methods designed for the same purpose will with similar results.</p>


</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./Ch5_Categorical.html" class="pagination-link">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Chapter 5: Analyzing categorical data</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./Ch7_Mixed.html" class="pagination-link">
        <span class="nav-page-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Chapter 7: Mixed-effects models</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->



</body></html>
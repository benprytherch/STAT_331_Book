[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "STAT 331",
    "section": "",
    "text": "Introduction to STAT 331: Intermediate Applied Statistical Methods"
  },
  {
    "objectID": "index.html#purpose-and-intended-audience",
    "href": "index.html#purpose-and-intended-audience",
    "title": "STAT 331",
    "section": "Purpose and intended audience",
    "text": "Purpose and intended audience\nSTAT 331, as the title states, is an â€œappliedâ€ statistics course. It is intended for anyone who has taken at least one introductory level statistics course, and who wants to learn more about the use of statistical methods in quantitative research.\nIt covers many statistical tools that are usually considered too advanced for an introductory level class, but are nonetheless very popular. It also provides guidance on making data analysis decisions.\nMost assignments will involve looking up a published scientific paper for which the data are available and reproducing the main results. There are many worked examples that go through the statistical analyses in used in specific published papers.\nSTAT 331 doesnâ€™t require any coding; the software we use is jamovi, a GUI-based (meaning â€œgraphical user interfaceâ€, aka â€œpoint-and-clickâ€) statistical analysis package. Jamovi is built on to of R, and for those interested in using R it has the ability to display the R code it creates under the hood.\nSTAT 331 is not mathematically heavy in the traditional sense, but it isnâ€™t math-free either. My approach is to present mathematical formulas and expressions when they are necessary or at least helpful for understanding the statistical itâ€™s being covered. There are no mathematical character-building exercises or examples, and we wonâ€™t be computing things by hand - the software does all the computational work. Our job is to make sense of the results. And that usually requires looking at formulas and figuring out what they do. We will be constantly answering the question â€œwhat does this number mean?â€"
  },
  {
    "objectID": "index.html#structure-of-these-notes",
    "href": "index.html#structure-of-these-notes",
    "title": "STAT 331",
    "section": "Structure of these notes",
    "text": "Structure of these notes\nThese notes are broken up into 9 chapters (or â€œmodulesâ€):\n\nChapter 1: Review of classical inference\nChapter 2: Model building with linear regression\nChapter 3: Assessing and improving model fit\nChapter 4: ANOVA-based methods\nChapter 5: Analyzing categorical data\nChapter 6: Generalized Linear Models (GLMs)\nChapter 7: Mixed-effects models\nChapter 8: Minding the gap between science and statistics\nChapter 9: Brief looks at major topics we didnâ€™t cover\n\nYou can access chapters and subsections directly through the table of contents."
  },
  {
    "objectID": "index.html#really-good-online-books",
    "href": "index.html#really-good-online-books",
    "title": "STAT 331",
    "section": "Really good online books",
    "text": "Really good online books\nThese notes will frequently reference some other freely available online statistics books:\nLearning statistics with jamovi, by Danielle Navarro and David Foxcroft\nIntroduction to Regression Analysis in R, by Kayleigh Keller\nAnswering questions with data, by Matthew J. C. Crump, Danielle J. Navarro, and Jeffrey Suzuki\nStatistical Analysis with The General Linear Model, by Jeff Miller and Patricia Haden"
  },
  {
    "objectID": "Ch1_Review.html#module-1-overview",
    "href": "Ch1_Review.html#module-1-overview",
    "title": "1Â  Chapter 1: Review of classical inference",
    "section": "1.1 Module 1 overview:",
    "text": "1.1 Module 1 overview:\nThese notes briefly cover material from your introductory statistics course that will be relevant in STAT 331.\nFor a more in-depth review, consult the OpenIntro text, or just do an internet search."
  },
  {
    "objectID": "Ch1_Review.html#distributions",
    "href": "Ch1_Review.html#distributions",
    "title": "1Â  Chapter 1: Review of classical inference",
    "section": "1.2 Distributions",
    "text": "1.2 Distributions\nThe term â€œdistributionâ€ will be used a lot.\nA distribution gives the values a variable takes on, and how often it takes them on.\nExamples: normal distribution, uniform distribution, distribution of exam scores, distribution of heightsâ€¦"
  },
  {
    "objectID": "Ch1_Review.html#commonly-used-statistics",
    "href": "Ch1_Review.html#commonly-used-statistics",
    "title": "1Â  Chapter 1: Review of classical inference",
    "section": "1.3 Commonly used statistics",
    "text": "1.3 Commonly used statistics\n\n1.3.1 Mean\nMean and median identify the center of a data set or distribution.\nThe mean of a variable \\(X\\) is denoted \\(\\bar{X}\\).\nTo calculate the mean of a data set, add up all the values of a variable and divide by how many there are.\n\\[\n\\bar{X} = \\frac{\\sum^n_{i=1}x_i}{n}\n\\]\n\n\n1.3.2 Median\nMedian is the â€œmiddleâ€ number in a data set. To find the median, put the data values in order from smallest to largest, and identify the number in the middle.\nIf there are an even number of data points, the median is the average of the middle two numbers:\n\n\n\n\n\n\n\n1.3.3 Mean vs.Â Median\nIn statistical inference (the process of generalizing from sample to population), we most often draw inference on the population mean.\nSometimes, though, the median is a more sensible statistic than the mean.\nThis is usually the case when we are studying a â€œskewedâ€ distribution.\nSkewed distributions are distributions that take on values that are extreme (or outlying) values.\nExample: income. Most households have incomes between $20,000/yr and $100,000/yr. A handful of households have incomes in the millions or billions of dollars per year.\nThe mean is affected by outliers. The median is not. This is why we often hear about â€œmedian household incomeâ€ rather than â€œmean household incomeâ€.\n\n\n1.3.4 jamovi example: mean vs.Â median\nTry creating a skewed data set in jamovi, then analyzing it by selecting for mean and median in the Statistics drop down menu in Descriptives, found under Exploration in the Analysis tab\nTo create a new dataset, enter data into the blank data table jamovi creates by default:\n\n\n\n\n\n\n\n\n\n\n\n\n1.3.5 Variance and standard deviation\nThe center of a distribution is quantified by the mean or the median.\nThe variability (i.e.Â spread) of a distribution is quantified by the standard deviation, which is closely related to the variance.\nThe variance of a distribution or data set is denoted \\(s^2\\):\n\\[\ns^2 = \\frac{\\sum^n_{i=1}(x_i - \\bar{x})^2}{n-1}\n\\]\nThe standard deviation is simply the square root of the variance\n\\[\ns = \\sqrt{\\frac{\\sum^n_{i=1}(x_i - \\bar{x})^2}{n-1}}\n\\]\nThink of this as the â€œstandardâ€ amount by which values deviate from their mean.\nWe will most often look at standard deviation, because it is the more interpretable of the two statistics. It is in the same units as the original variable.\n\n\n1.3.6 The correlation coefficient\nThe correlation coefficient, ğ‘Ÿ, quantifies the extent to which two variables (call them X and Y) move together:\n\\[\nr = (\\frac{1}{n-1})\\sum^n_{i=1}\\frac{(x_i - \\bar{x})(y_i-\\bar{y})}{s_xs_y} = (\\frac{1}{n-1})\\sum^n_{i=1}z_{x_i}z_{y_i}\n\\]\nDonâ€™t worry too much about the formula. The most important things to know are:\nWhen two variables move in the opposite direction (i.e.Â when one gets bigger, the other gets smaller), \\(r\\) is negative.\nWhen they move in the same direction, \\(r\\) is positive.\n\\(r = 0\\) means no correlation. \\(r =1\\) means perfect positive correlation. \\(r = -1\\) means perfect negative correlation."
  },
  {
    "objectID": "Ch1_Review.html#statistics-and-parameters",
    "href": "Ch1_Review.html#statistics-and-parameters",
    "title": "1Â  Chapter 1: Review of classical inference",
    "section": "1.4 Statistics and parameters",
    "text": "1.4 Statistics and parameters\nA statistic is any value calculated from data.\nA parameter is any value pertaining to a population.\nThe values of unknown parameters are â€œestimatedâ€ using statistics.\nExample: a sample mean can be used to estimate a population mean. i.e.Â \\(\\bar{X}\\) estimates \\(\\mu\\)."
  },
  {
    "objectID": "Ch1_Review.html#sampling-distributions",
    "href": "Ch1_Review.html#sampling-distributions",
    "title": "1Â  Chapter 1: Review of classical inference",
    "section": "1.5 Sampling distributions",
    "text": "1.5 Sampling distributions\nA sampling distribution is the distribution of values a statistic takes on, under repeated sampling.\nFor example, the Central Limit Theorem states that the sampling distribution of \\(\\bar{X}\\) will be normal, so long as the sample size (\\(n\\)) is large enough.\nSampling distributions are important because most methods used in statistical inference invoke long run frequency properties.\nExample: if the sampling distribution of \\(\\bar{X}\\) is not very spread out, then the value of \\(\\bar{X}\\) should not change much if we take a new sample.\n\n1.5.1 Standard error\nStandard error is the standard deviation of a sampling distribution.\nIn other words, it is the amount of variability in the values a statistic takes on under repeated sampling.\nSo, if a statistic we calculate has a small standard error, we can infer that the value of that statistic is close to the value of the population parameter it is estimating. If it has a large standard error, its value might be very far away from the value of the parameter.\nExample: the standard error of \\(\\bar{X}\\) is \\(\\frac{s}{\\sqrt{n}}\\) i.e.Â \\(s_{\\bar{X}} = \\frac{s}{\\sqrt{n}}\\)\n\n\n1.5.2 Sampling dist. and standard error, visually\nOn Canvas there is a Central Limit Theorem simulator.\nWhen the sample size is large, the distribution of the sample mean is not very spread out. In other words, its standard error is small.\nWhen the sample size is small, the standard error is large."
  },
  {
    "objectID": "Ch1_Review.html#confidence-intervals",
    "href": "Ch1_Review.html#confidence-intervals",
    "title": "1Â  Chapter 1: Review of classical inference",
    "section": "1.6 Confidence intervals",
    "text": "1.6 Confidence intervals\nA confidence interval (â€œCIâ€) is an interval (i.e.Â a left endpoint and a right endpoint) constructed around a statistic, when that statistic is being treated as an estimate for the value of an unknown parameter.\nTypical confidence intervals are constructed by adding a â€œmargin of errorâ€ to, and subtracting it from, an estimate:\nğ¶ğ¼ ğ‘“ğ‘œğ‘Ÿ ğ‘ğ‘ğ‘Ÿğ‘ğ‘šğ‘’ğ‘¡ğ‘’ğ‘Ÿ = ğ‘’ğ‘ ğ‘¡ğ‘–ğ‘šğ‘ğ‘¡ğ‘’ ğ‘œğ‘“ ğ‘ğ‘ğ‘Ÿğ‘ğ‘šğ‘’ğ‘¡ğ‘’ğ‘Ÿ Â± ğ‘šğ‘ğ‘Ÿğ‘”ğ‘–ğ‘› ğ‘œğ‘“ ğ‘’ğ‘Ÿğ‘Ÿğ‘œğ‘Ÿ\nTypical margins of error are calculated by multiplying the standard error of the estimate by a â€œcritical valueâ€. A critical value comes from a known distribution and is given by a confidence level.\nExample: a 95% CI for a population mean uses a critical value from the \\(t\\) distribution (we wonâ€™t cover why this is).\nğ¶ğ¼ ğ‘“ğ‘œğ‘Ÿ ğœ‡ = \\(\\bar{x}\\) Â± \\(ğ‘¡_{ğ‘ğ‘Ÿğ‘–ğ‘¡ğ‘–ğ‘ğ‘ğ‘™}\\) âˆ— \\(s_{\\bar{x}}\\)\nAs long as \\(n\\) isnâ€™t tiny, the 95% critical value from a \\(t\\) distribution is approximately 2:\nğ¶ğ¼ ğ‘“ğ‘œğ‘Ÿ ğœ‡ â‰ˆ \\(\\bar{x}\\) Â± \\(2\\) âˆ— \\(s_{\\bar{x}}\\)\nConfidence intervals should capture the unknown parameter value being estimated. The confidence level gives how often the interval captures the parameter under repeated sampling.\nExample: 95% of all 95% CIs for \\(\\mu\\) capture \\(\\mu\\) .\nThe confidence level can also be thought of as the success rate of the method being used.\nSo, 95% confidence intervals have a 95% success rate in capturing the value of the unknown parameter.\nJust as with sampling distributions, we are invoking repeated sampling here. We say that a 95% CI can be trusted because it is created using a method that would â€œworkâ€ 95% of the time, if we were to keep taking new samples and keep constructing 95% CIs."
  },
  {
    "objectID": "Ch1_Review.html#confidence-intervals-quantify-uncertainty",
    "href": "Ch1_Review.html#confidence-intervals-quantify-uncertainty",
    "title": "1Â  Chapter 1: Review of classical inference",
    "section": "1.7 Confidence intervals quantify uncertainty",
    "text": "1.7 Confidence intervals quantify uncertainty\nThe most important characteristic of a CI is its width.\nWe are typically willing to believe that the unknown value of a parameter lies inside the confidence interval constructed from our data.\nIf the confidence interval is wide, there is a lot of uncertainty as to the true value of the parameter.\nIf the confidence interval is narrow, then our estimate for the value of the parameter is â€œpreciseâ€, in that it shouldnâ€™t be wrong by much.\nCIs are narrow when the sample size is large and / or the standard deviation of our data is small.\nCIs are wide with the sample size is small and / or the standard deviation of our data is large.\nIMPORTANT: CIs, like all inferential statistical methods, are created under assumptions. We make distributional assumptions about our data (e.g.Â normality). We assume our statistic is an unbiased estimate of the parameter, i.e.Â it will not systematically differ from the parameter value under repeated sampling."
  },
  {
    "objectID": "Ch1_Review.html#confidence-interval-simulation-apps",
    "href": "Ch1_Review.html#confidence-interval-simulation-apps",
    "title": "1Â  Chapter 1: Review of classical inference",
    "section": "1.8 Confidence interval simulation apps",
    "text": "1.8 Confidence interval simulation apps\nThere is a confidence interval simulation app on Canvas, that demonstrates creating confidence intervals â€œunder repeated samplingâ€. This app is from Brown Universityâ€™s â€œSeeing theoryâ€ series .\nThere is also a â€œsampling distribution and standard errorâ€ app on Canvas. It shows a population distribution for a normally distributed variable, a sample of data from that distribution, and the sampling distribution of the mean.\nThis app also super-imposes the standard error in pink."
  },
  {
    "objectID": "Ch1_Review.html#confidence-intervals-in-jamovi",
    "href": "Ch1_Review.html#confidence-intervals-in-jamovi",
    "title": "1Â  Chapter 1: Review of classical inference",
    "section": "1.9 Confidence intervals in jamovi",
    "text": "1.9 Confidence intervals in jamovi\nJamovi can create a 95% CI and any other summary statistics selected for using the Statistics menu of Descriptives\nFor example, when producing summary statistics for a variable using the Statistics drop down menu in Descriptives, you will need to select Confidence Interval for Mean found under Mean Dispersion. Jamovi will report â€œ95% CI mean lower boundâ€ and â€œ95% CI mean upper boundâ€. These are the endpoints for the 95% CI for \\(\\bar{X}\\)."
  },
  {
    "objectID": "Ch1_Review.html#hypothesis-testing",
    "href": "Ch1_Review.html#hypothesis-testing",
    "title": "1Â  Chapter 1: Review of classical inference",
    "section": "1.10 Hypothesis testing",
    "text": "1.10 Hypothesis testing\nHypothesis testing is an inferential method in which a null hypothesis (\\(H_0\\)) is â€œtestedâ€ against. If the data are in strong enough disagreement with \\(H_0\\) , then \\(H_0\\) is rejected.\n\\(H_0\\) typically represents the proposition that â€œthere is nothing of interest at the population levelâ€, or â€œthe proposed research hypothesis is not trueâ€.\nIf \\(H_0\\) is rejected, then the result of the test is described as â€œstatistically significantâ€.\nExample: if we have data from a controlled experiment in which \\(\\mu_1\\) represents the population mean for the control group and \\(\\mu_2\\) represents the population mean for the treatment group, then we might test against the null hypothesis:\n\\[\nH_0: \\mu_1 = \\mu_2,\\text{which is equivalent to }H_0: \\mu_1 âˆ’ \\mu_2 = 0\n\\]\nIf we reject \\(H_0\\), we say that the sample means, \\(\\bar{x}_1\\) and \\(\\bar{x}_2\\), are â€œsignificantly differentâ€. Or, equivalently, that \\(\\bar{x}_1 - \\bar{x}_2\\) is â€œsignficantly differentâ€ from zero."
  },
  {
    "objectID": "Ch1_Review.html#the-test-statistic",
    "href": "Ch1_Review.html#the-test-statistic",
    "title": "1Â  Chapter 1: Review of classical inference",
    "section": "1.11 The test statistic",
    "text": "1.11 The test statistic\nThe strength of the evidence against \\(H_0\\) is quantified by a â€œtest statisticâ€, from which a â€œp-valueâ€ is calculated.\nTest statistics are set up so that, the more the inconsistent the data are with \\(H_0\\), the larger the test statistic will be.\nExample: when testing \\(H_0: \\mu_1 âˆ’ \\mu_2 = 0\\), we use the test statistic:\n\\[\nt = \\frac{\\bar{x}_1 - \\bar{x}_2}{s_{(\\bar{x}_1 - \\bar{x}_2)}} = \\frac{\\bar{x}_1 - \\bar{x}_2}{\\sqrt{\\frac{s_1^2}{n_1} + \\frac{s_2^2}{n_2}}} \\text{(where } s_{(\\bar{x}_1 - \\bar{x}_2)}\\text{ is the standard error of }\\bar{x}_1 - \\bar{x}_2)\n\\]"
  },
  {
    "objectID": "Ch1_Review.html#the-p-value",
    "href": "Ch1_Review.html#the-p-value",
    "title": "1Â  Chapter 1: Review of classical inference",
    "section": "1.12 The p-value",
    "text": "1.12 The p-value\nThe p-value is defined as the probability of getting a test statistic at least as large as the one calculated, if we assume \\(H_0\\) is true.\nVisually, the p-value is the area in the tail of the sampling distribution of the test statistic under \\(H_0\\)\n\n\n\n\n\nIf the p-value is less than the â€œlevel of significanceâ€ (\\(\\alpha\\)), then \\(H_0\\) is rejected.\nBy far the most typical level of significance is \\(\\alpha = 0.05\\)\n\n\n\n\n\n\n1.12.1 Interpreting â€œstatistical significanceâ€\nâ€œStatistically significantâ€ results are those that produce a small p-value.\nSmall p-values result from data that would be unlikely to be obtained just by chance, if the null hypothesis were true.\nSo, when you hear that results are â€œstatistically significantâ€, you can interpret this as meaning â€œthe data we obtained donâ€™t look like the kind of data weâ€™d expect to see just by chanceâ€.\n\n\n1.12.2 Cautions regarding â€œstatistical significanceâ€\nAs with confidence intervals, hypothesis tests require assumptions.\nThese will be covered in detail in the next module.\nThe most important distinction to be made right now is the distinction between statistical significance and practical importance.\nResults can be statistically significant, but still seem weak or unimpressive by practical standards.\nExample: this is a statistically significant correlation:\n\n\n\n\n\n(\\(r = 0.22\\), p-value = 0.011)\nExample: this is a statistically significant difference in means:\n\n\n\n\n\n(\\(\\bar{x}_1 = 19.7\\), \\(\\bar{x}_2 = 22.7\\), p-value = 0.0013)\nThe use of hypothesis testing is controversial.\nI personally do not like hypothesis testing, and I think that statistical significance is usually uninteresting.\nWeâ€™ll explore the debates surrounding statistical significance in module 2.\n\n\n1.12.3 Confidence intervals vs.Â p-values\nFor now, weâ€™ll note that in many cases, confidence intervals can be used in place of p-values to perform a hypothesis test.\nIf a 95% CI excludes the null value (typically zero), then \\(H_0\\) is rejected at the \\(\\alpha = 0.05\\) level of significance.\nThe advantage of using a confidence interval rather than a p-value is that it is easier to make sense out of, and it quantifies uncertainty: the wider the CI, the more uncertainty there is regarding the value of the unknown parameter.\n\n\n1.12.4 Confidence intervals vs.Â p-values example\n\n\n\n\n\n\\(\\bar{x}_1 = 19.7\\), \\(\\bar{x}_2 = 22.7\\), p-value = 0.0013\n95% CI for \\(\\mu_1 - \\mu_2: (-4.72,-1.19)\\)\n\n\n\n\n\nHere we see 95% CIs for differences in means, along with their corresponding p-values.\nNote how much the p-values change for small changes in the CIs.\nNote also that different CIs can correspond to the same p-value."
  },
  {
    "objectID": "Ch1_Review.html#cis-and-p-values-in-jamovi",
    "href": "Ch1_Review.html#cis-and-p-values-in-jamovi",
    "title": "1Â  Chapter 1: Review of classical inference",
    "section": "1.13 CIs and p-values in jamovi",
    "text": "1.13 CIs and p-values in jamovi\nCIs and p-values can be calculated for a huge variety of statistics.\nFor now, we will consider testing for a difference in means.\nIn jamovi, select an Independent Samples T-Test from T-Tests under the Analyses tab\nMake Max_Temp_Challenge be the Dependent variable (the one containing measurements), and make Vaccine be the Grouping variable (the one identifying which group the measurement belongs to).\nHere is an example using the Vaccine data set. This example uses sheet 3 of the Excel file, titled â€œH3N2_Clinical_Maxâ€: Data will need to be prepared for test by swapping the rows so that Vaccine occurs first."
  },
  {
    "objectID": "Ch1_Review.html#cis-and-p-values-in-jmp",
    "href": "Ch1_Review.html#cis-and-p-values-in-jmp",
    "title": "1Â  Chapter 1: Review of classical inference",
    "section": "1.14 CIs and p-values in JMP",
    "text": "1.14 CIs and p-values in JMP\n\n\n\n\n\nHere, the p-value is \\(0.002\\)\nThe 95% CI for the difference in population mean max_temp is \\((âˆ’2.07, âˆ’0.58)\\)\nThe confidence interval excludes zero and \\(p &lt; 0.05\\), so the difference in sample means is statistically significant."
  },
  {
    "objectID": "Ch1_Review.html#data-format-in-jamovi",
    "href": "Ch1_Review.html#data-format-in-jamovi",
    "title": "1Â  Chapter 1: Review of classical inference",
    "section": "1.15 Data format in jamovi",
    "text": "1.15 Data format in jamovi\nA final note on data formatting: this data set is in \"long form\", meaning each row is a single observation and each column is a variable.\njamovi's Independent Samples T-test requires long form data.\nSometimes you'll have data in \"wide form\", where each column is a group, and the rows do not correspond to single observations\nExample: here's some fake data in wide form:\n\n\n\n\n\nFit Independent Samples T-test cannot be used to compare these means.Â  jamovi thinks there are 4 observations, each with a measurement on Var1 and Var 2.Â \n\n\n\n\n\nThis isn't what we want!\nTo transform the data into long form, we need to install the Rj â€“ Editor module which will allow us to run R-code in jamovi"
  },
  {
    "objectID": "Ch1_Review.html#installing-rj-in-jamovi",
    "href": "Ch1_Review.html#installing-rj-in-jamovi",
    "title": "1Â  Chapter 1: Review of classical inference",
    "section": "1.16 Installing Rj in jamovi",
    "text": "1.16 Installing Rj in jamovi\n\nNavigate to the Analyses tab\n\nClick on Modules in the top right of the jamovi window\nClick jamovi library\nScroll until you see \"Rj â€“ Editor to run R code\", click install\nYou should now see an R logo under the Analyses tab"
  },
  {
    "objectID": "Ch1_Review.html#wide-form-to-long-form-in-jamovi",
    "href": "Ch1_Review.html#wide-form-to-long-form-in-jamovi",
    "title": "1Â  Chapter 1: Review of classical inference",
    "section": "1.17 Wide form to long form in jamovi",
    "text": "1.17 Wide form to long form in jamovi\nTo switch from wide to long form, we will write a simple line of R-code using the Rj â€“ Editor module:Â \nClick on Rj, it will open an empty window where we can enter R - code\n\n\n\n\n\nThe code below transforms the data into long form by stacking the data within Var1 and Var2 into a new column Data and creates a new column Labels to identify if data is from Var1 or Var2.\nThe data will output a csv file which we can import from a new session of jamovi\n\nImporting the transformed csv file to a new jamovi window shows our transformed long form data table. Note: column names will need to be updated\nCompare the two:\n\nâ€œLong formâ€\n\n\n\n\n\n\n\nâ€œWide formâ€\n\n\n\n\n\n\nNow Independent Samples T-test can be used to compare means:"
  },
  {
    "objectID": "Ch2_Model_Building.html#outline-of-notes",
    "href": "Ch2_Model_Building.html#outline-of-notes",
    "title": "2Â  Chapter 2: Model building with linear regression",
    "section": "Outline of notes:",
    "text": "Outline of notes:\n\nThe linear regression equation\nRegression analysis in jamovi\nSums of squares and mean squares\nInterpreting regression results\nApplied example (â€œThe Binary Biasâ€)\nInteraction between variables\nApplied example: arthritis treatment data\nCentering predictor variables\nStandardizing predictor variables"
  },
  {
    "objectID": "Ch2_Model_Building.html#the-linear-regression-equation",
    "href": "Ch2_Model_Building.html#the-linear-regression-equation",
    "title": "2Â  Chapter 2: Model building with linear regression",
    "section": "2.1 The linear regression equation",
    "text": "2.1 The linear regression equation\nLinear regression, in its simplest form, is a method for finding the â€œbest fittingâ€ line through a set of bivariate (two variable) data:\n\nWhat is meant by â€œbest fittingâ€ will be addressed shortly. For now, think of the line as showing the underlying linear trend through a set of data.\nThe vertical distance between each data point and the line is called a â€œresidualâ€. On the plot above, the red lines represent residuals. They quantify the amount by which a data point deviates from the underlying linear trend. Every point has a residual; the plot above only shows a few of them.\n\n2.1.1 The linear regression equation as a statistical model\nThe line that is drawn through data comes from a statistical model. A statistical model is a mathematical expression describing how data are generated. It has a fixed component and a random component. Think of the fixed component as describing the underlying relationship between variables, and the random component as describing any additional variability in data beyond what the fixed component describes.\nBelow is the standard linear regression model. The random component is represented by \\(``\\varepsilon_i\"\\). Everything from â€œ\\(\\beta_0\\)â€ up until â€œ\\(\\varepsilon_i\\)â€ is the fixed component.\n\\[\nY_i=\\beta_0+\\beta_1x_{1i} + \\beta_2x_{2i} + \\dots + \\beta_kx_{ki} + \\varepsilon_i \\\\\ni = 1, \\dots, n \\\\\n\\varepsilon_i \\sim \\text{Normal}(0,\\sigma^2)\n\\]\nHereâ€™s what each term represents:\n\n\\(i\\) is the index term. It counts through the data, starting at \\(i = 1\\) and going through \\(i=n\\), where \\(n\\) is the sample size.\n\\(Y_i\\) is the \\(i^{th}\\) value of the outcome variable. When written in upper-case, \\(Y_i\\) is treated as a random variable whose value has not been observed. When written lower-case, \\(y_i\\) represents an observed data value.\n\\(Y_i\\) is often referred to as the â€œresponseâ€ variable, or the â€œdependentâ€ variable. These notes will use the term â€œoutcomeâ€ variable. I prefer this term on the grounds that the others seem to imply causality: if \\(Y\\) is â€œrespondingâ€ to \\(x\\), or â€œdependentâ€ on \\(x\\), then it sounds like changing the value of \\(x\\) will induce a change in the value of \\(Y\\).\n\\(x_{1i}\\) is the \\(i^{th}\\) value of the first predictor (i.e.Â independent) variable. \\(x_{2i}\\) is the \\(i^{th}\\) value of the second predictor, etc. The \\(x's\\) are always written lower-case, and technically are assumed to be fixed values, either set prior to data collection or measured without error.\n\\(\\beta_1\\) is the slope (i.e.Â regression coefficient) for the first predictor variable. \\(\\beta_2\\) is the slope of the second predictor, etc. The \\(\\beta's\\) are parameters, meaning their values are treated as fixed (existing at the â€œpopulationâ€ level) but unknown.\nWe use data to calculate estimated values for the \\(\\beta's\\), and these estimates are written using hat notation. For example, \\(\\hat{\\beta_1}\\) is the estimated value for \\(\\beta_1\\).\n\\(\\varepsilon_i\\) is the \\(i^{th}\\) random error value. This is the amount by which \\(Y_i\\) differs from \\(\\beta_0+\\beta_1x_{1i} + \\beta_2x_{2i} + \\dots + \\beta_kx_{ki}\\), i.e.Â the fixed component of the model.\nThe amount by which \\(y_i\\) (the \\(i^{th}\\) observed value of \\(y\\)) differs from \\(\\hat{\\beta_0}+\\hat{\\beta_1}x_{1i} + \\hat{\\beta_2}x_{2i} + \\dots + \\hat{\\beta_k}x_{ki}\\) is called the \\(i^{th}\\) residual, which we can denote \\(e_i\\).\nThe errors are modeled as random values that are drawn from a normal distribution with mean zero and variance \\(\\sigma^2\\).\n\n\n\n\n\n\n\nNote\n\n\n\nThe errors in a regression model do not have to come from a normal distribution. This assumption is made in order to justify inferences about the coefficients; more on this soon.\n\n\nWhen a regression model has only one predictor variable, it is called a â€œsimpleâ€ regression model. If it has more than one predictor variable, it is called a â€œmultiple regressionâ€ model.\n\n\n2.1.2 Assumptions of the regression model\nThis model implies some assumptions:\n\nThe response variable \\(Y\\) is an additive, linear function of the predictors (the \\(x\\) variables)\nIf we fix the value(s) of the \\(x\\) variable(s), all values of \\(Y\\) will be normally distributed. In other words, the errors are normally distributed.\nThe errors have the same variance regardless of the values of the \\(x's\\). This variance is denoted \\(\\sigma^2\\)\n\n\n\n\n\n\n\nNote\n\n\n\nThe square root of the variance is the standard deviation, denoted \\(\\sigma\\). Standard deviation is expressed in the same units of the original variable, whereas variance is expressed in squared units.\nFor this reason, standard deviation is typically referred to when interpreting statistical results. Variance has desirable mathematical properties, and so is more often referred to in statistical theory\n\n\nVisually, this model treats values of Y as being generated randomly from normal distributions centered on the line:\n\n(figure derived from OpenStax Introductory Business Statistics, section 13.4)\nThe â€œerrorsâ€ are the distances between the line and the values generated from the normal distributions.\nThe errors are treated as random and uncorrelated: knowing the value of one error tells you nothing about the likely value of the next."
  },
  {
    "objectID": "Ch2_Model_Building.html#regression-analysis-in-jamovi",
    "href": "Ch2_Model_Building.html#regression-analysis-in-jamovi",
    "title": "2Â  Chapter 2: Model building with linear regression",
    "section": "2.2 Regression analysis in jamovi",
    "text": "2.2 Regression analysis in jamovi\n\n2.2.1 Simulating the regression model in jamovi\nWe noted earlier that a statistical model is data generating. It describes, mathematically, how values of the outcome variable \\(Y\\) can be created. Consider the â€œsimpleâ€ (single \\(x\\) ) regression model:\n\\[\nY_i=\\beta_0+\\beta_1x_{i} + \\varepsilon_i \\\\\n\\varepsilon_i \\sim \\text{Normal}(0,\\sigma^2)\n\\]\nIf we have values for \\(\\beta_0\\), \\(\\beta_1\\), and \\(\\sigma^2\\), when we can plug in values for \\(x_i\\) to generate values for \\(Y_i\\). Letâ€™s do this using jamovi.\nIn jamovi, we first create X values by double-clicking an empty column, choosing â€œNew Computed Variableâ€ then the \\(f_x\\) drop down menu and double click UNIF.\n\nHere, we are generating 100 random values from a \\(Uniform(0,100)\\) distribution. The uniform distribution is a distribution where all values are equally likely, so we should get an even spread of values between 0 and 100.\nTo simulate values of the response variable, weâ€™ll need to make up values for each parameter in the model. Say we want to generate values from this model:\n\\(Y_i=10+0.7x_i+\\varepsilon_i \\quad \\varepsilon_i \\sim Normal(0,8^2)\\)\nThis means weâ€™ve decided that \\(\\beta_0=10\\), \\(\\beta_1=0.7\\), and \\(\\sigma=8\\). And since weâ€™ve generated 100 values for \\(x_i\\), weâ€™ve also decided that \\(i=1\\dots 100\\)\nDouble click an empty column and choose â€œNew Computed Variableâ€:\n\nNow make the formula look like the right side of the regression equation from the previous slide:\n\nNow we can take a look using Scatterplot, a downloadable jamovi module. Click the icon of the plus sign labeled â€œModulesâ€ to bring up a list of available modules you can install. We will use many modules in STAT 331.\n\nAfter installation, Scatterplot is available under Exploration in the Analyses tab. You can assign the X and Y axis variables, and get a plot that looks something like this:\n\nThese are random data, so yours will look a little bit different. But the scales of the axes and vertical spread of the data should be similar.\nNext, weâ€™ll fit a regression model to this data. In practice, we do not know the values of the parameters in our model, so we estimate them using data. This is known as â€œfittingâ€ the model to the data. The point of this simulation is to look at what kind of results we get when fiting a regression model to fake data that was produced by a mechanism we fully understand.\n\n\n2.2.2 Fitting a regression model in jamovi\nWe can use Regression / Linear Regression to fit a â€œsimpleâ€ regression model, which is a regression model with just one predictor.\n\nThe response variable is â€œDependent Variableâ€.\nThe predictor variable goes under â€œCovariatesâ€.\n\nAfter selecting variables, model will automatically be fit, and output will be generated to the right under â€œResultsâ€.\n\nBased on these results, here is the estimated regression model:\n\\[\n\\hat{y_i}=\\hat{\\beta_0}+\\hat{\\beta_1}x_i=9.1+0.71x_i\n\\]\n\\[\n\\hat{\\sigma}=\\sqrt{MSE}=\\sqrt{64.8}=7.93\n\\]\nNote that it is standard to denote estimated values using â€œhatsâ€. The â€œestimateâ€ column is where we find the values for the estimated regression coefficients \\(\\hat{\\beta_0}\\) and \\(\\hat{\\beta_1}\\). â€œRMSEâ€ (â€œroot mean square errorâ€) is the estimated standard deviation of the errors, assumed to have come from a normal distribution. Compare these results to the values used to generate our fake data:\n\\[\n\\begin{align}\n&\\beta_0=10 &\\beta_1=0.7 \\quad &\\sigma=8 \\\\\n&\\hat{\\beta_0}=9.099 &\\hat{\\beta_1}=0.712 \\quad &\\hat{\\sigma}=7.93 \\\\\n&s_{\\hat{\\beta_0}}=1.776 &s_{\\hat{\\beta_1}}=0.034\n\\end{align}\n\\]\n\n\n2.2.3 The \\(R^2\\) statistic\nNote that the output tells us \\(R^2=0.874\\). This is a statistic quantifying how well this model can â€œpredictâ€ the data used to fit it. It is found from the â€œsum of squaresâ€ values in the ANOVA table, Generically:\n\\[\nR^2=\\frac{\\text{model sum of squares}}{\\text{total sum of squares}}=\\frac{\\text{model sum of squares}}{\\text{model sum of squares + residual sum of squares}}\n\\]\n\n\n\n\n\n\nNote\n\n\n\nâ€œSums of squaresâ€ are used to quantify variance. You can think of this as being short for â€œsum of the squared distances between some values and their meanâ€. For example, the variance statistic is\n\\[\ns^2=\\frac{\\sum_{i=1}^n (y_i-\\bar{y})}{n-1}\\\n\\] The numerator, \\(\\sum_{i=1}^n (y_i-\\bar{y})\\), is a â€œsum of squaresâ€ - the sum of the squared deviations between all the values of \\(y_i\\) and their mean, \\(\\bar{y}\\).\n\n\nIn this example:\n\\[\nR^2=\\frac{29162}{29162+4211}=0.874\n\\]\nSo, in this case, \\(87.4\\%\\) of the total variance in \\(Y\\) can be accounted for using the values of \\(x\\). Hereâ€™s the data again, with the estimated regression line added:\n\n\nThe total variance in \\(Y\\) quantifies how much the data vary vertically around the horizontal red line, which is the mean of \\(Y\\).\nThe residual (or â€œerrorâ€) variance quantifies how much the data vary vertically around the regression line.\n\nHere, the data are relatively much closer to the regression line than to the the horizontal mean line, and so residual sum of squares is only a small portion of the total sum of squares, making \\(R^2\\) fairly large.\nAn alternative interpretation of \\(R^2\\) is that is quantifies the proportional decrease in residual variance when using the regression line rather than using only the mean of \\(Y\\).\nLooking at the plot above, you can imagine drawing vertical lines from each data point to the horizontal red line representing the mean of \\(Y\\). If you squared these lines and added them up, youâ€™d have the total sum of squares, which would also be the residual sum of squares if you were using only the mean of \\(Y\\) to calculate residuals. In this case, using the regression line instead of just the mean to calculate residuals would represent an \\(87.4\\%\\) decrease in residual sum of squares.\nSaid differently, \\(R^2\\) tells you how much better your predictions for \\(Y\\) would be if you use the regression line rather than only the mean.\n\n\n2.2.4 There is no â€œgoodâ€ or â€œbadâ€ value for \\(R^2\\)\nWhen residual variance in \\(Y\\) is larger, \\(R^2\\) is smaller. Visually, when the data are more spread out around the regression line, \\(R^2\\) is smaller. Is this â€œbadâ€? I want you to resist such an interpretation. A small \\(R^2\\) tells you that \\(Y\\) is being influenced by a lot more than just what is in your model. And this is often to be expected.\nFor instance, if Iâ€™m trying to predict how many tomatoes are produced per tomato plant in different parts of the country and my only predictor variable is average daily outdoor temperature, I should not expect a large \\(R^2\\). This is because there are many many more variables that influence how many tomatoes will grow (e.g.Â properties of soil, watering schedule, fertilizer, pestsâ€¦). But, a small \\(R^2\\) should not be interpreted as â€œaverage daily outdoor temperature doesnâ€™t matter when growing tomatoesâ€. It should be interpreted as â€œthere are way more other things that matter when growing tomatoes, and their combined influence is much greater than average daily outdoor temperature aloneâ€."
  },
  {
    "objectID": "Ch2_Model_Building.html#sums-of-squares-and-mean-squares",
    "href": "Ch2_Model_Building.html#sums-of-squares-and-mean-squares",
    "title": "2Â  Chapter 2: Model building with linear regression",
    "section": "2.3 Sums of squares and mean squares",
    "text": "2.3 Sums of squares and mean squares\nWe will look at some formulas in this section. Some are based on sums of squares, which are reported in the ANOVA table.\nTotal sum of squares quantifies total variability in \\(y\\):\n\\[\nSS_{Total} = \\sum^n_{i=1}(y_i - \\bar{y})^2\n\\]\nNote that this has nothing to do with the regression line, or the predictor variable. It quantifies variability in the response variable alone.\n\n2.3.1 Total sum of squares\nVisually, \\(SS_{Total}\\) is the sum of the squared vertical deviations between each data point and the mean of ğ‘¦, shown here as a horizontal line.\n\n\n\n\n\nThe two blue lines drawn are two such instances of these deviations. If we drew these for every data point, squared them, and added them up, weâ€™d have \\(SS_{Total}\\)\nAgain, note that this quantity has nothing to do with the regression line!\n\n\n2.3.2 Residual / Error sum of squares\nError sum of squares quantifies total variability in ğ‘¦ around the regression line:\n\\[\nSS_{Error} = \\sum^n_{i=1}(y_i - \\hat{y}_i)^2 = \\sum^n_{i=1}[y_i - (\\hat{\\beta}_0 + \\hat{\\beta}_1)]^2\n\\]\nThis is also known as the â€œsum of the squared residualsâ€, where a residual is the vertical distance between a data point and the regression line. The values of \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\) are chosen so as to minimize \\(SS_{Error}\\).\nIn other words, the regression line drawn through the data produced a smaller \\(SS_{Error}\\) than any other line we could possibly draw.\nVisually, \\(SS_{Error}\\) is the sum of the squared vertical deviations between each data point and the regression line.\n\n\n\n\n\nHere, the blue lines are two instances of these deviations\n\\(SS_{Error}\\) will be larger when the data are more spread out around the line, and vice versa.\n\n\n2.3.3 Mean square error\nError mean square (aka mean square error) is given by\n\\[\nMSE = MS_{Error} = \\frac{SS_{Error}}{n-k +1}\n\\]\n\\(k\\) is the number of predictor variables. Example: for simple regression, \\(k = 1\\), so \\(MSE = \\frac{SS_{Error}}{N-2}\\)\nAs seen earlier, \\(\\sqrt{MSE}\\) is the estimate for the standard deviation of the residuals around the line: \\(\\hat{\\sigma} = \\sqrt{MSE}\\)\n\n\n2.3.4 Model sum of squares\nModel sum of squares (aka â€œregression sum of squaresâ€) is given by:\n\\[\nSS_{Model} = \\sum^n_{i=1}(\\hat{y}_i - \\bar{y})^2\n\\]\nThis can be thought of as quantifying how much better the model is than \\(\\bar{y}\\) alone at accounting for variation in the values of \\(y\\).\nVisually, \\(SS_{Model}\\) is the sum of the squared vertical deviations between the regression line and the horizontal line, at each value of the predictor variable.\n\n\n\n\n\nHere, the blue lines are two instances of these deviations, associated with the circled blue data points.\n\n\n2.3.5 \\(SS_{Total} = SS_{Error} + SS_{Model}\\)\nTotal sum of squares are equal to the sum of error sum of squares and model sum of squares.\nNote that this also implies:\n\\[\nSS_{Model} = SS_{Total} - SS_{Error} \\\\\nSS_{Error} = SS_{Total} - SS_{Model}\n\\]"
  },
  {
    "objectID": "Ch2_Model_Building.html#interpreting-regression-results",
    "href": "Ch2_Model_Building.html#interpreting-regression-results",
    "title": "2Â  Chapter 2: Model building with linear regression",
    "section": "2.4 Interpreting regression results",
    "text": "2.4 Interpreting regression results\nHere again is the estimated model from the previous example:\n\\[\n\\hat{y}_i = 9.1 + 0.71x_i\n\\]\nThe intercept, \\(\\hat{\\beta}_0 = 9.1\\), gives the predicted value of the response variable (\\(y\\)) when the predictor variable (\\(x\\)) equals zero. This is not typically of practical interest.\nThe slope, \\(\\hat{\\beta}_1 = 0.71\\), gives the predicted change in \\(y\\) for a one unit increase in \\(x\\).\n\n2.4.0.1 More on interpreting the slope\nThe slope is often of practical interest. It tells us how much the response variable changes, on average, when the predictor variable increases by one unit.\nThis interpretation is very common. It is also dangerous, because it is phrased in a way that suggests changes in ğ‘¥ cause changes in \\(y\\).\nHere is an alternate, non-causal sounding interpretation:\nIf we observe two values of \\(x\\) that are one unit apart, we estimate that their corresponding average \\(y\\) values will be \\(\\hat{\\beta}_1\\) units apart.\nVisually, we can choose two values of \\(x\\), go up to the line, and record the values of \\(y\\). The slope tells us how much these \\(y\\) values are expected to differ.\n\n\n\n\n\nHere, when we compare \\(x = 20\\) and \\(x = 80\\), we expect their corresponding \\(y\\) values to differ by:\n\\[\n(80 âˆ’ 20) âˆ— 0.725 = 43.5\n\\]\n\n\n2.4.0.2 Inference on the coefficients\nFor each estimated coefficient (\\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_0\\)), jamovi reports a standard error, along with a t-test statistic and p-value testing the null that the parameter being estimated equals zero.\n\n\n\n\n\nExample: the above output shows the test of \\(H_0: \\beta_1 = 0\\)\n\\[\nt = \\frac{\\hat{\\beta}_1}{s_{\\hat{\\beta}_1}} = \\frac{0.712}{0.034} = 21.22\\\\\np-value &lt; 0.01 \\\\\n\\text{\"reject } H_0\"\n\\]\nWe can use these results to create an approximate 95% confidence for \\(\\beta_1\\):\n\\[\n95\\% \\text{ CI for } \\beta_1 \\approx \\hat{\\beta}_1 \\pm 2* s_{\\hat{\\beta}_1} = 0.712 \\pm 2*0.034 = (0.645,0.779)\n\\]\nThis is a very narrow interval, suggesting a â€œpreciseâ€ estimate of \\(\\beta_1\\)\nFor both the hypothesis test and 95% CI, the results depend on how large the estimate of \\(\\beta_1\\) is, relative to its standard error.\nIn this case, \\(\\hat{\\beta}_1\\) is very large relative to \\(s_{\\hat{\\beta}_1}\\), so the result is â€œhighly significantâ€ and the 95% CI is narrow.\n\n\n2.4.0.3 Formulas for \\(\\hat{\\beta}_1\\) and \\(s_{\\hat{\\beta}_1}\\)\nNow that youâ€™ve seen how \\(\\hat{\\beta}_1\\) and \\(s_{\\hat{\\beta}_1}\\) are used, here are their formulas:\n\\[\n\\hat{\\beta}_1 = r_{xy} * \\frac{s_y}{s_x} \\\\\ns_{\\hat{\\beta}_1} = \\sqrt{\\frac{MSE}{\\sum^n_{i=1}(x_i - \\bar{x})^2}} = \\sqrt{\\frac{1-R^2}{n-k-1}}*\\frac{s_y}{s_x}\n\\]\n\\(\\hat{\\beta}_1\\) is larger when the correlation between \\(x\\) and \\(y\\) is stronger, and when the variability in \\(y\\) is larger relative to the variability in \\(x\\).\n\\(s_{\\hat{\\beta}_1}\\) is smaller when \\(R^2\\) is larger, when \\(n\\) is larger, and when the variability in \\(x\\) is larger relative to the variability in \\(y\\)."
  },
  {
    "objectID": "Ch2_Model_Building.html#applied-example-the-binary-bias",
    "href": "Ch2_Model_Building.html#applied-example-the-binary-bias",
    "title": "2Â  Chapter 2: Model building with linear regression",
    "section": "2.5 Applied example: â€œThe binary biasâ€",
    "text": "2.5 Applied example: â€œThe binary biasâ€\nThere is a paper up on Canvas, titled â€œThe Binary Bias: A Systematic Distortion in the Integration of Informationâ€. This is a 2018 paper published in Psychological Science with open data.\nThe overall hypothesis is that people tend to assess continuous information using binary thinking. The experiments all involve testing to see if participants will give higher or lower assessments of where an average lies, based on the â€œimbalanceâ€ of the data: i.e.Â the comparative frequency with which very low or very high values turn up.\nHere is the section of the paper describing the results of Study 1a:\n\n\n\n\n\nThe data:\n\n\n\n\n\nTelling jamovi to fit the model:\n\n\n\n\n\nThe model, before being fit to the data:\n\\[\nRecorded_i = \\beta_0 + \\beta_1Imbalance_i + \\beta_2Mode_i + \\beta_3First_i + \\beta_4Last_i + \\epsilon_i\n\\]\nThe results in jamovi:\n\n\n\n\n\n\n\n\n\n\nNote that the estimated slope is being denoted as \\(b\\) rather than as \\(\\hat{\\beta}_1\\). \\(\\hat{\\sigma} = MSE\\). Note also how the 95% CI was created: \\(\\approx 4.62 Â± 2 âˆ— 0.63\\)\nThe paper does not mention \\(R^2\\). We can see it in the jamovi output, and calculate it from the ANOVA table:\n\n\n\n\n\n\\(R^2 = 0.12\\). So, about 12% of the total variance in â€œRecordedâ€ is being â€œexplainedâ€ or â€œaccounted forâ€ in the model.\nIn multiple regression, each predictor is interpreted under the assumption that the values of all other predictors are held constant (i.e.Â â€œcontrolled forâ€).\nSo, if we were to observe two participants who were equal in terms of â€œFirstâ€, â€œLastâ€, and â€œModeâ€, but were one unit apart in terms of â€œImbalanceâ€, we would expect their values for the response variable (â€œRecordedâ€) to differ by 4.62 units on average.\nOr: The predicted (or average) difference in Recorded associated with a one unit difference in Imbalance is 4.62 units, if all other predictors are held constant."
  },
  {
    "objectID": "Ch2_Model_Building.html#interaction",
    "href": "Ch2_Model_Building.html#interaction",
    "title": "2Â  Chapter 2: Model building with linear regression",
    "section": "2.6 Interaction",
    "text": "2.6 Interaction\nâ€œInteractionâ€ is the phenomenon by which the association between a predictor variable and a response variable is itself dependent on the value of another predictor.\nSay we have response \\(y\\), one predictor \\(x_1\\), and another predictor \\(x_2\\). We say that \\(x_1\\) and \\(x_2\\) interact if the amount of change in \\(y\\) associated with a change in \\(x_1\\) is different for different values of \\(x_2\\), or vice versa.\nYou can think of this as saying that the â€œslopeâ€ of \\(x_1\\) depends upon the value of \\(x_2\\).\nAnother way of saying it: if the answer to the question:\nâ€œHow much does our estimate for \\(y\\) change when \\(x_1\\) changes?â€œ\nis:\nâ€œIt depends on the value of \\(x_2\\)â€œ,\nthen \\(x_1\\) and \\(x_2\\) interact.\n\n2.6.1 Interaction example\nSuppose a drug for treating rheumatoid arthritis is more effective at reducing inflammation for younger patients than it is for older patients.\nIf we conduct an experiment in which inflammation is the response variable and the predictors are treatment group (drug vs.Â control) and age, then we expect treatment group and age to interact.\nThis is different from saying that age and treatment both affect inflammation. It means that the extent to which treatment affects inflammation is different for patients of different age.\nSometimes interaction is referred to as â€œmoderationâ€. This is common in the social and behavioral sciences, particularly Psychology.\nSo, a â€œmoderatorâ€ variable is one that changes how the primary predictor of interest relates to the response.\nExample: suppose an experiment shows that subjects holding a pen with their teeth rate cartoons as funnier vs.Â subjects holding a pen with their lips.\nSuppose also that this â€œpen in teethâ€ effect disappears if subjects see a video camera in the room.\nIn this case, the presence of the video camera â€œmoderatesâ€ the effect of the pen on cartoon ratings. In the language of interaction, the presence of the pen and the presence of the video camera â€œinteractâ€.\n\nThis is based on a real study that has generated controversy, see:\nhttps://statmodeling.stat.columbia.edu/2018/11/01/facial-feedback-findings-suggest-minute-differences-experimental-protocol-might-lead-theoretically-meaningful-changes-outcomes/\n\n\n\n2.6.2 Interaction, in the regression model\nMathematically, we create an interaction variable by multiplying predictor variables by one another.\nSo, if we want to allow \\(x_1\\) and \\(x_2\\) to interact, we simply make a new variable defined as \\(x_1*x_2\\).\nThis interaction variable will be used as an additional predictor variable in the regression model:\n\\[\nY_i = \\beta_0 + \\beta_1x_{1i} + \\beta_2x_{2i} +\\beta_3(x_1x_2)_i + \\epsilon_i \\\\\n\\text{ where } \\epsilon_i \\sim Normal(0,\\sigma^2)\n\\]\nThe interaction coefficient is \\(\\beta_3\\) in this model:\n\\[\nY_i = \\beta_0 + \\beta_1x_{1i} + \\beta_2x_{2i} +\\beta_3(x_1x_2)_i + \\epsilon_i \\\\\n\\text{ where } \\epsilon_i \\sim Normal(0,\\sigma^2)\n\\]\nTo interpret this, letâ€™s look at how it affects the coefficients (aka slopes) for \\(x_1\\) and \\(x_2\\).\nWe can think of the â€œslopeâ€ of a predictor as everything it is being multiplied by.\n\\[\nY_i = \\beta_0 + \\beta_1x_{1i} + \\beta_2x_{2i} +\\beta_3(x_1x_2)_i + \\epsilon_i\n\\]\nFactoring out \\(x_1\\) from the above regression equation gives:\n\\[\nY_i = \\beta_0 + (\\beta_1 + \\beta_3x_{2i})x_{1i} + \\beta_2x_{2i} + \\epsilon_i\n\\]\nSimilarly, factoring out \\(x_2\\) gives:\n\\[\nY_i = \\beta_0 + (\\beta_2 + \\beta_3x_{1i})x_{2i} + \\beta_1x_{1i} + \\epsilon_i\n\\]\nSo, for this model, the â€œslopeâ€ of \\(x_1\\) is \\(\\beta_1 + \\beta_3x_2\\), and the â€œslopeâ€ of \\(x_2\\) is \\(\\beta_2 + \\beta_3x_1\\)\nIn other words, the slope of \\(x_1\\) depends on the value of \\(x_2\\), and vice versa. For different values of \\(x_2\\), the â€œpredicted change in \\(y\\) for a one unit increase in \\(x_1\\)â€ (i.e.Â the slope of \\(x_1\\)) will be different.\nA simpler way of saying this is that, if two predictors interact, then the effect of one predictor on the response depends on the value of the other predictor.\n(This is a simpler interpretation, but also potentially misleading in that the term â€œeffectâ€ sounds causal. Nonetheless it is commonly used language when interpreting slopes.)"
  },
  {
    "objectID": "Ch2_Model_Building.html#jamovi-example-arthritis-data",
    "href": "Ch2_Model_Building.html#jamovi-example-arthritis-data",
    "title": "2Â  Chapter 2: Model building with linear regression",
    "section": "2.7 jamovi Example: Arthritis data",
    "text": "2.7 jamovi Example: Arthritis data\nThe data set â€œarthritis_data.csvâ€ contains simulated data from a (fictional) Randomized Control Trial comparing treatments for inflammation from rheumatoid arthritis: a disease-modifying anti-rheumatic drug (DMARD) vs.Â a non-steroidal anti-inflammatory drug (NSAID)\n\nThe variables are:\n\n-   Drug: indicator (0 / 1) variable; 1 = DMARD, 0 = NSAID\n\n-   Before: Inflammation scan score prior to treatment (scale: 0 to 50)\n\n-   After: Inflammation scan score six months after treatment\n\n-   Difference: Difference in scores, before minus after. (Note that larger values\n\n2.7.0.1 Before vs.Â after scatterplot\nWe will fit some regression models using this data, but first letâ€™s take a look at our data using the Scatterplot module.\n\n\n\n\n\nHere is â€œafterâ€ vs.Â â€œbeforeâ€, with a linear regression line superimposed. Note that most patients had greater inflammation before than after treatment.\n\n\n2.7.0.2 Differences by treatment type\nNote that the differences tend to be larger for the DMARD group: this corresponds to a greater reduction in inflammation.\n\n\n\n\n\nUse Descriptives to create the boxplot. Select difference as a Variable and Split by drug. Then select under Plots â€“ Box plot and Data Jittered to superimpose data.\n\n\n\n\n\n\n\n2.7.0.3 Before vs.Â age scatterplot\nHere we see that age is positively correlated with inflammation before the drug trial.\n\n\n\n\n\njamovi gives options to superimpose regression output on a scatterplot.\njamovi can also plot â€œstandard errorâ€ bands around the line. These show the standard error for the average value of y (â€œbeforeâ€), given x (â€œageâ€).\n\n\n\n\n\n\n\n2.7.0.4 Difference vs.Â before\nWe donâ€™t see an association between amount of inflammation before treatment and reduction in inflammationâ€¦\n\n\n\n\n\nâ€¦ but maybe we do if we add in drug! Do â€œdrugâ€ and â€œbeforeâ€ interact here?\n\n\n\n\n\n\n\n2.7.0.5 Difference vs.Â age\nWe see a small negative correlation between difference and ageâ€¦\n\n\n\n\n\nâ€¦ but when we add drug, we see no correlation for the NSAID and clear negative correlation for the DMARD. Definite interaction!\n\n\n\n\n\n\n\n2.7.0.6 Now with model outputâ€¦\nThe following slides show the plots again, plus the regression output JMP produces. First up, Difference vs.Â Drug, using t-test:\n\n\n\n\n\nAnd using regression:\n\n\n\n\n\nOMG! The estimated slope for â€œdrugâ€ is the same as the difference in means between the drugs!\n\n\n\n\n\n\n\n2.7.0.7 Before vs.Â age\n\n\n\n2.7.0.8 Difference vs.Â age, with drug interaction\nHere, we are fitting the model:\n\\[\nDifference_i = \\beta_0 + \\beta_1age_i + \\beta_2drug_i + \\beta_3(age*drug)_i + \\epsilon_i\n\\]\nTo do this, create a new column in jamovi, defined as \\(age*drug\\). May as well label it â€œ\\(age*drug\\)â€œ.\n\n\n\n\n\n\n\n\n\n\nNotice that the slope of â€œageâ€ by itself is for when drug = 0. The age*treatment interaction shows how much the slope of â€œageâ€ changes when treatment = 1.\n\\[\n\\widehat{difference} = 2.0114 + 0.0125(age) + 15.1586(drug) -0.2493(age*drug)\n\\]\nSlope of â€œageâ€ when drug = 1 is: \\(0.0125 âˆ’ 0.2493 = âˆ’0.2368\\)\nNotice also that the slope of â€œageâ€ by itself is nowhere close to being statistically significant (the estimate is less than half the standard error), but the slope of the interaction is highly significant (the estimate is 6 times as large as the standard error)\nNow take a look at the plot. There is a clear negative correlation between age and difference when drug = 1. There is essentially none when drug = 0.\n\n\n\n\n\n\nAnother way of thinking about this interaction:\n\nIf we ask the question: â€œhow does inflammation reduction differ by age?â€, the answer is â€œit depends on which drug the patient took.â€\nSimilarly, if we ask the question: â€œhow does inflammation reduction differ by drug?â€, the answer is â€œit depends on the age of the patient.â€\n\n\n\n\n2.7.0.9 Interaction is NOT correlation!\nIn this example, we see a clear interaction between drug and age: the slope for age is negative for DMARD and flat for NSAID.\nAnother way of thinking about this: DMARD appears to be more effective for younger patients than for older patients. NSAID appears to be equally effective regardless of age.\nHOWEVER â€“ this does NOT mean that treatment and age are correlated!\nThis should make sense, after all patients were randomly assigned to one of the two drugs. If drug were correlated with age, there would be bias in this study. The whole point of randomization is to remove correlations!\nJust to confirm, here is the distribution of age, split by drug:\n\n\n\n\n\nAgain, age and drug â€œinteractâ€ when it comes to their associations with the differences in inflammation scores: the association between â€œageâ€ and â€œdifferenceâ€ is different for the two different drugs.\nSimilarly, the association between â€œdrugâ€ and â€œdifferenceâ€ is different for patients of different ages.\nBut, age and drug are not correlated with one another!"
  },
  {
    "objectID": "Ch2_Model_Building.html#centering-predictor-variables",
    "href": "Ch2_Model_Building.html#centering-predictor-variables",
    "title": "2Â  Chapter 2: Model building with linear regression",
    "section": "2.8 Centering predictor variables",
    "text": "2.8 Centering predictor variables\n\\[\n\\widehat{difference} = 2.0114 + 0.0125(age) + 15.1586(drug) -0.2493(age*drug)\n\\]\nThere is a serious challenge when interpreting the slope for â€œdrugâ€: this slope is only 15.1586 if \\(age = 0\\). But \\(age = 0\\) is not of interest.\nPlug in mean age (52.096) and see what happens to the slope for drug:\n\\[\n15.1586(drug) âˆ’ 0.2493(52.096*drug) \\\\\n= (15.1586 âˆ’ 0.2493 âˆ— 52.096)(drug)\\\\\n= 2.171(ğ‘‘ğ‘Ÿğ‘¢ğ‘”)\n\\]\nSo, for patients at mean age, the predicted difference in inflammation is 2.171 units greater under the DMARD than under the NSAID\nThis process of estimating slopes at the average value of predictors can be done via â€œcenteringâ€\nCentering means subtracting the mean from a variableâ€™s distribution.\nIn this example, \\(centered\\text{ }age = age - \\overline{age} = age - 52.096\\)\nThis is very useful when using interaction terms in a regression model.\n\n2.8.0.1 Centered interaction in jamovi\nTo center age, create a new columns called â€œcentered ageâ€, defined as age â€“ VMEAN(age).\nCreate another new column for â€œcentered drugâ€, defined as drug â€“ VMEAN(drug). Now create a final column defined as centered drug * centered age, call it â€œcentered age*drugâ€\nWe canâ€™t use MEAN() to center variables in jamovi since it works across variables, one row at a time. What we want is to take the overall mean of a variable and subtract it from each measurement.\nSo, use VMEAN() to center variables in jamovi.\nCompare the new results (on the left) to the results we saw when using the interaction term we created manually (on the right)\n\n\n\n\n\n\n\n2.8.0.2 Centered interaction in JMP\nThe â€œModel Fit Measuresâ€ are identical. Centering has no effect on \\(R^2\\) or any sums of squares.\nCentering also had no effect on the slope for the interaction, or on its standard error. But, look at the individual â€œageâ€ and â€œdrugâ€ predictors.\n\n\n\n\n\nThese slopes are different, because they are being calculated at the mean value of the other.\nCentering also reduces the standard errors of the individual slopes!\n\n\n2.8.0.3 Centered interaction in jamovi\nLook at the slope for drug when using a centered interaction: itâ€™s the same value we calculated by plugging the mean of age into the interaction term in the non-centered model!\n\n\n\n\n\nThe slope for age in the centered model is harder to interpret. It is calculated at the â€œaverageâ€ for drug, which doesnâ€™t make real world sense.\n\n\n2.8.0.4 Only centering one predictor in jamovi\nIt would be best if we could center â€œageâ€ but not drug.\nBut we already created centered age when we created the centered interaction.\nNow we need to create the new interaction where only age is centered. Create a new column and define it as â€œdrug * centered ageâ€.\n\n\n\n\n\n\n\n2.8.0.5 Only centering one predictor in JMP\nHere are the results. Compare them to the previous two versions:\nOnly age centered:\n\n\n\n\n\nAge and drug centered in the interaction:\n\n\n\n\n\nNothing centered:\n\n\n\n\n\n\n\n2.8.0.6 Only centering one predictor in jamovi\nFirst, the slope for the interaction is the same in all three models.\nSecond, the slope for drug is the same in both centered models, but different in the non-centered model. It is the centering of age in the interaction that changed the slope for drug.\n\n\n\n\n\nThird, the slope for age is the same in both models for which drug is not centered. This allows us to interpret it as before: slope for age is 0.0125 for the NSAID and is 0.0125 âˆ’ 0.2493 = âˆ’0.2368 for the DMARD\nFourth, the standard error for drug is substantially smaller when age is centered in the interaction. This is typically the case when centering a continuous variable in an interaction.\n\n\n2.8.0.7 Interpreting the intercept when centering\nFinally, the intercept is different in all three models.\nNormally we donâ€™t care about the intercept, but centering allows the intercept to be meaningfully interpreted.\nSince \\([Center]age = age - \\overline{age}, [Center]age = 0 \\text{ when } age = \\overline{age}\\)\nRemember that the intercept is interpreted as the â€œpredicted value of the response when the predictors equal zeroâ€.\nSo, when centering, the intercept is the predicted value of the response when the centered predictors equal their mean.\nGoing back to our example, the predicted reduction in inflammation (before minus after) for a patient at the average age in our data set who got the NSAID is 2.665.\n\n\n\n\n\nThe predicted reduction in inflammation for a patient at the average age who got the DMARD is 2.665 + 2.171 = 4.836"
  },
  {
    "objectID": "Ch2_Model_Building.html#standardizing-predictor-variables",
    "href": "Ch2_Model_Building.html#standardizing-predictor-variables",
    "title": "2Â  Chapter 2: Model building with linear regression",
    "section": "2.9 Standardizing predictor variables",
    "text": "2.9 Standardizing predictor variables\nWe will briefly consider an extension on centering: standardizing.\nRecall from your introductory statistics course the standardization â€œzâ€ formula:\n\\[\nz = \\frac{x - \\mu}{\\sigma}, \\text{ or in words: } z = \\frac{\\text{value - mean}}{\\text{standard deviation}}\n\\]\nTo center a variable, we subtract the mean. To standardize, we subtract the mean and then divide by the standard deviation.\n\n2.9.0.1 Why standardize?\nJust like with a centered variable, the mean of a standardized variable is zero. So, standardizing has all the same benefits as centering when it comes to interpretation of interactions.\nStandardizing has an additional potential benefit: the slope can be interpreted as the predicted change in \\(Y\\) for a one standard deviation increase in \\(X\\) (while holding all other predictors constant).\n\\(Z\\) values are interpreted as â€œnumber of standard deviations from the meanâ€. And so increasing \\(Z\\) by \\(1\\) implies increasing \\(X\\) by \\(1\\) standard deviation.\n\n\n2.9.0.2 Standardizing in jamovi\nTo standardize in jamovi we will need to create a new column defined by (age â€“ VMEAN(age)) / VSTDEV(age).\nWe will also need to create a column for the new interaction and define it as drug*Standardized age\n\n\n\n\n\nHere are the results when age is standardized:\n\n\n\n\n\nCompare to the results when age is centered:\n\n\n\n\n\nNote that the intercepts are the same. In both cases, the predicted reduction in inflammation for a patient at average age getting NSAID is \\(2.665\\). Likewise, in both cases this prediction is \\(4.836\\) for DMARD.\nWhat has changed is the slope for terms involving age. Now, a one unit increase in standardized age is a one standard deviation increase in age.\nSo, for NSAID, the predicted difference in inflammation reduction for two people whose ages are one standard deviation apart is \\(0.14\\). For DMARD, it is \\(0.14 â€“ 2.79 = -2.64\\).\nHow much is a standard deviation? We can look it upâ€¦\n\n\n\n\n\n\n\n2.9.0.3 Interaction and centering / standardizing summary\nThis has been a long example. I encourage you to load up the data yourself and play around with it in jamovi. At the minimum, make sure you can re-create the results in these notes.\n\nThe most important take-aways:\n\nInteraction terms allow the slope of predictor to change for different values of another predictor.\nCentering helps make regression coefficients (slopes and intercepts) more interpretable. Standardizing allows you to interpret them in terms of one standard deviation changes, rather than â€œone unitâ€ changes."
  },
  {
    "objectID": "Ch3_Model_Fit.html#part-1-assumptions-and-assumption-violations",
    "href": "Ch3_Model_Fit.html#part-1-assumptions-and-assumption-violations",
    "title": "3Â  Chapter 3: Assessing and improving model fit",
    "section": "Part 1: assumptions and assumption violations",
    "text": "Part 1: assumptions and assumption violations"
  },
  {
    "objectID": "Ch3_Model_Fit.html#outline-of-notes",
    "href": "Ch3_Model_Fit.html#outline-of-notes",
    "title": "3Â  Chapter 3: Assessing and improving model fit",
    "section": "Outline of notes:",
    "text": "Outline of notes:\n\nRegression assumptions\nLinearity\nNormality of residuals\nHomogeneity of variance\nInfluential observations\n(Multi)collinearity\nLog transformation\nNon-linearity\nOver-fitting\nBack to basics: is the model sensible?"
  },
  {
    "objectID": "Ch3_Model_Fit.html#violating-model-assumptions",
    "href": "Ch3_Model_Fit.html#violating-model-assumptions",
    "title": "3Â  Chapter 3: Assessing and improving model fit",
    "section": "3.1 Violating model assumptions",
    "text": "3.1 Violating model assumptions\nThe previous notes outlined the fundamentals of specifying, fitting, and interpreting a linear regression model.\nThese notes focus on the assumptions that our models are based upon, and how we check to see if they are being violated.\nIf model assumptions are violated, DONâ€™T PANIC! There is nearly always a remedy. In these notes we will look at how to spot violations of assumptions. In the next set of notes we will look at ways to remedy these violations, and how to decide if they pose a serious problem to the usefulness of the model.\n\n3.1.1 The regression model and what it assumes\nOnce again, here is the regression model:\n\\[\nY_i = \\beta_0 + \\beta_1x_{1i} + \\beta_2x_{2i} + \\dots + \\beta_px_{pi} + \\epsilon_i, \\text{ where } \\epsilon_i \\sim Normal(0,\\sigma)\n\\]\n\nThis assumes:\n\nThat the response variable is a linear (straight line) function of the predictor variables\nThat the residuals will be normally distributed\nThat the standard deviation of the residuals does not vary\nThat the residuals are independent"
  },
  {
    "objectID": "Ch3_Model_Fit.html#linearity",
    "href": "Ch3_Model_Fit.html#linearity",
    "title": "3Â  Chapter 3: Assessing and improving model fit",
    "section": "3.2 Linearity",
    "text": "3.2 Linearity\nRemember the â€œsimpleâ€ (i.e.Â single predictor) regression model\n\\[\nY_i = \\beta_0 + \\beta_1x_{i} + \\epsilon_i, \\text{ where } \\epsilon_i \\sim Normal(0,\\sigma)\n\\]\nThis is linear in that it fits a straight line to the two-dimensional data.\nA two-predictor model would fit a flat plane to the three-dimensional data, and so on\nHereâ€™s a bad idea: fitting a linear model to non-linear data!\n\n\n\n\n\n\n3.2.1 Diagnosing non-linearity\nWhen running â€œLinear Regressionâ€ in jamovi, a â€œresiduals by predictedâ€ plot can be created by selecting â€œResidual plotsâ€ under â€œAssumption Checksâ€\nThe residuals are the differences between each observed values of the response variable and the value that the model predicts:\n\\[\nresidual_i = y_i - \\hat{y}_i = y_i - (\\hat{\\beta}_0 + \\hat{\\beta}_1x_1 + \\hat\\beta_2x_2 + \\dots)\n\\]\n\n\n\n\n\nFor simple regression, this plot just looks like the regression plot with the line turned horizontally.\nFor multiple regression, there is no (two dimensional) â€œregression plotâ€, so the residual plot will be very useful!\n\n\n\n\n\nIn this example, there is clear curvature in the data. A straight line model is not appropriate.\nHereâ€™s an example of what a linear relationship might look like:\n\n\n\n\n\nWhen there is non-linearity, you will see the residuals mostly on one side of zero, then on the other size of zero, then back again, and so on.\nWhen there is linearity, the residuals should randomly fall on either side of zero.\n\n\n\n\n\n\n\n3.2.2 What to look for in a residual plot\nWe will look at many more examples of residual plots in these notes.\n\nWe want a residual plot that appears to agree with the model assumptions:\n\nStraight line relationship between the predictors and response\nNormally distributed random residuals around this line\nEqual variance in residuals across line"
  },
  {
    "objectID": "Ch3_Model_Fit.html#normality-of-residuals",
    "href": "Ch3_Model_Fit.html#normality-of-residuals",
    "title": "3Â  Chapter 3: Assessing and improving model fit",
    "section": "3.3 Normality of residuals",
    "text": "3.3 Normality of residuals\nThe â€œerror termâ€ in a regression model is that \\(+ \\epsilon_i\\) on the end\nWhen we write \\(\\epsilon_i \\sim Normal(0, \\sigma)\\), we are saying that the errors (aka residuals) are normally distributed, with mean zero and some standard deviation \\(\\sigma\\).\nThis can be assessed visually: run the regression plot the residuals to see if they appear roughly normally distributed.\n\n3.3.1 The QQ plot\nWhen fitting a model using â€œLinear Regressionâ€ in jamovi, there is an option to save residuals. This will create a new column with a residual for each row.\nThe option is under the last drop-down menu, under â€œSaveâ€.\n\n\n\n\n\nTo create a plot of the residuals, select â€œQ-Q plot of residualsâ€ under â€œAssumption Checksâ€ in â€œLinear Regressionâ€\nThe Normal Quantile plot is also known as the QQ plot, for â€œquantile quantileâ€.\nIt is easier to assess normality with a QQ plot than with a histogram.\n\n\n\n\n\n\n\n3.3.2 Assessing normality with a QQ plot\nOn Canvas under Simulations there is a â€œQQ plot generatorâ€ app.\nThis app allows you to manipulate a distribution, sample from it, and then view a histogram next to a QQ plot. It is meant to give you an idea of how QQ plots work.\nBy default, it draws data from a normal distribution. But, you can add â€œskewednessâ€ or â€œpeakednessâ€ (aka kurtosis) to make the distribution non-normal. You can also adjust sample size.\n\nA QQ plot shows you how much the distribution of your data â€œagreeâ€ with a normal distribution.\n\n\n\n\n\nThe horizonal axis gives the distribution data would follow if it were perfectly normal.\nThe vertical axis gives the distribution your data actually follows.\nThe diagonal line shows perfect agreement between the two.\n\n\n\n\n\n\n\n\n\n\nThe big advantage of the QQ plot vs.Â the histogram is that very often data that come from a normal distribution donâ€™t look normal, especially if the sample size is small.\nIn this case, the histogram isnâ€™t clearly normal. But, on the QQ plot the data are close to the line.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNotice that the data still veer from the diagonal line to some extent, even though we know for a fact they came from a normal distribution.\n\n\n3.3.3 Limitations of QQ plots\nAs you can see from the app, sometimes data that come from a normal distribution donâ€™t sit right on the line.\nSometimes data that come from a skewed distribution look similar to data that come from a normal distribution\nItâ€™s easier to assess normality when sample sizes are larger.\nAs it turns out, the assumption of normality is not vital to the validity of a regression model. If the QQ plot is vague, youâ€™re probably fine. We only worry when we see extreme non-normality.\n\n\n3.3.4 Tests for normality (not recommended)\nThere are statistical tests, such as â€œShapiro-Wilksâ€ or â€œKolmgorov-Smirnovâ€, for which the null hypothesis is that the data come from some specified distribution, like the normal.\nRejecting this null means that the data â€œsignificantlyâ€ disagree with the assumption of normality.\nI do not recommend using this test. The problem is that, when the sample size is large enough, even small deviations from normality will be statistically significant. But small deviations from normality are OK. Only major deviations are concerning."
  },
  {
    "objectID": "Ch3_Model_Fit.html#the-homogeneity-of-variance-assumption",
    "href": "Ch3_Model_Fit.html#the-homogeneity-of-variance-assumption",
    "title": "3Â  Chapter 3: Assessing and improving model fit",
    "section": "3.4 The homogeneity of variance assumption",
    "text": "3.4 The homogeneity of variance assumption\nBack to the error term:\n\\(\\epsilon_i \\sim Normal(0, \\sigma)\\)\nNotice that \\(\\sigma\\) is just one number. This suggests that the standard deviation of the residuals should be the same across all values of predictor variables. In other words, there is homogeneity of variance.\nAnother name for this is â€œhomoscedasticityâ€. If this assumption is violated, then we have â€œheteroscedasticityâ€.\n\n3.4.1 Heterogeneity of variance\nTo show heterogeneity of variance, Iâ€™ll simulate data that is a function of an X variable, plus random values from a normal distribution with standard deviation equal to X.\nThus, the standard deviation of residuals will get bigger as X gets bigger:\n\n\n\n\n\n\n\n3.4.2 The residuals vs fitted plot\nHere is the regression plot and residual plot when this simulated variable (called â€œWâ€ here) is the response and X is the predictor:\n\nNotice that the residuals are more spread out for larger X\nWe also see â€œheavy tailsâ€ when plotting the residuals with a histogram and QQ plot:\n\n\n\n\n\nHeavy tails refers to a distribution with outliers on both ends.\nThis shows up on the QQ plot as the residuals being too flat in the middle and then curving out on both ends."
  },
  {
    "objectID": "Ch3_Model_Fit.html#influential-observations-outliers",
    "href": "Ch3_Model_Fit.html#influential-observations-outliers",
    "title": "3Â  Chapter 3: Assessing and improving model fit",
    "section": "3.5 Influential observations (outliers)",
    "text": "3.5 Influential observations (outliers)\nOutliers in regression can be seen on a residual plot, or on a QQ plot, or on just a regular plot of the data.\nExample: the Florida election data.\n\n\n\n\n\nOutliers can â€œpullâ€ on the regression line, especially if they are far away from the mean of the predictor(s).\nThere are many statistics that assess influence. jamovi will calculate one of the most popular: a Cookâ€™s Distance\n\n3.5.1 Cookâ€™s Distances\nAs we saw in the Florida election example, removing the outlier (Palm Beach County) had a substantial effect on the regression results.\nThe logic behind Cookâ€™s Distance is to quantify what happens to the regression model when a single observation is removed. This is sometimes referred to as a â€œleave one outâ€ method.\nCookâ€™s Distances quantify how much the predicted values of the response variable change when an observation is removed.\nRecall that, in simple regression, the predicted values are the values on the regression line.\nIt is hard to interpret the actual values for Cookâ€™s Distances. Values greater than 1 are often considered â€œinfluentialâ€.\nThe formula shows that it is based on the sum of the differences in predicted values between a model with the data point included and a model with it removed:\n\\[\n\\text{Cook's Distance for data point } \"i\" = D_i = \\frac{\\sum^n_{j=1}(\\hat{y}_i - \\hat{y}_{j(i)})^2}{MSE * p}\n\\]\nWhere \\(\\hat{y}_{j(i)}\\) is the predicted value of the response variable when the model is re- fit with the \\(i^{th}\\) data point removed, and \\(p\\) is the number of predictor variables in\nA Cookâ€™s Distance is calculated for every data point. The option to do this in jamovi is under â€œSaveâ€ in â€œLinear Regressionâ€. This creates a new column with a Cookâ€™s distance for each row.\nThe saved Cookâ€™s Distances can then be plotted. Any possibly influential points will usually stand out clearly from the rest.\n\n\n\n\n\nTo quickly narrow in on the influential counties, we can filter out all the small Cookâ€™s distances.\nAfter implementing the filter, we can see that all rows which do not meet the criteria of the filter are excluded.\n\n\n\n\n\nOnly rows 13 and 50 should be highlighted for Dade and Palmbeach county which have cookâ€™s distances of 1.983 and 3.786."
  },
  {
    "objectID": "Ch3_Model_Fit.html#multicollinearity",
    "href": "Ch3_Model_Fit.html#multicollinearity",
    "title": "3Â  Chapter 3: Assessing and improving model fit",
    "section": "3.6 (Multi)collinearity",
    "text": "3.6 (Multi)collinearity\nIn regression analysis, we want our predictor variables to be correlated with the response variable.\nBut we donâ€™t want our predictor variables to be (highly) correlated with one another!\nWhen two predictor variables are highly correlated, we say our model has â€œcollinearity.â€\nWhen more than two predictor variables are mutually highly correlated, we say our model as â€œmulticollinearityâ€.\n\n3.6.1 Why donâ€™t we want correlated predictors?\nTo understand why we donâ€™t want correlated predictors, recall that multiple regression models estimate the association between each individual predictor variable and the response, \\(\\textit{while holding all other predictor variables constant}\\).\nThis can be thought of as asking â€œwhat is the difference in the response variable when we observe data points that have different values of one predictor but the same values of the other predictors?â€\nIf \\(Y\\) is the response and \\(x_1\\) and \\(x_2\\) are predictors, we want to know how different \\(Y\\) is when \\(x_1\\) values differ but \\(x_2\\) values are the same, or vice versa.\nBut, if \\(x_1\\) and \\(x_2\\) are highly correlated, then we donâ€™t get to observe cases where values of one variable differ substantially while values of the other are the same! Consider instances of no correlation vs.Â heavy correlation:\n\n\n\n\n\nWhen \\(x_1\\) and \\(x_2\\) are uncorrelated, we see lots of instances of \\(x_1\\) values differing a lot when \\(x_2\\) values are equal.\n\n\n\n\n\nWhen \\(x_1\\) and \\(x_2\\) are highly correlated, we never see instances where \\(x_2\\) values are equal but \\(x_1\\) values are highly correlated.\n\n\n\n\n\nThe upshot is that, when \\(x_1\\) and \\(x_2\\) are highly correlated, the regression procedure has a difficult time distinguishing between the â€œeffectâ€ of \\(x_1\\) on \\(Y\\) and the â€œeffectâ€ of \\(x_2\\) on \\(Y\\).\nExtreme example: if we had degrees Celsius and degrees Fahrenheit as predictors in the same model, it would be impossible to tell their effects apart. You canâ€™t change Celsius while holding Fahrenheit constant!\nThe practical consequence is that the standard errors for the slopes of (multi)collinear predictors are much larger than they would be if it were not for the (multi)collinearity.\nIf two or more predictors are perfectly correlated (\\(r=1\\)), then the model cannot be fit and jamovi produces an error:\n\n\n\n\n\nHere, \\(X1\\) and \\(X2\\) are perfectly correlated. jamovi cannot estimate a slope for \\(X2\\).\n\n\n3.6.2 Collinearity example: Florida election data\nIn the Florida election data, we used total votes for each county as our predictor variable.\nThere is another variable called â€œTotal_Regâ€. This is the total number of registered voters in each county.\nUnsurprisingly, Total_Votes and Total_Reg are highly correlated:\n\n\n\n\n\nIf we run two separate simple regression models, we get very similar results:\n\\[\nBuchanin_i = \\beta_0 + \\beta_1Total\\_Votes_i + \\epsilon_i\n\\]\n\n\n\n\n\n\\[\nBuchanin_i = \\beta_0 + \\beta_1Total\\_Reg_i + \\epsilon_i\n\\]\n\n\n\n\n\nBut look what happens if we use Total_Votes and Total_Reg as predictors in the same model:\n\\[\nBuchanin_i = \\beta_0 + \\beta_1Total\\_Votes_i + \\beta_2Total\\_Reg_i + \\epsilon_i\n\\]\n\n\n\n\n\nTwo important things to note:\nP-values on slopes are much larger than for the individual models\n\\(R^2\\) is larger than on either individual model!\nLooking at the estimates and standard errors in all three models, we see that the standard errors are much larger in the multiple regression model. These estimates are â€œunstableâ€ â€“ their values will change a lot if the data change a little.\n\n\n\n\n\nWe know the association between Total_Reg are Buchanan is actually positive. But with so high a standard error, the slope for Total_Reg turned out negative!\n\n\n3.6.3 Variance inflation factor (VIF)\n(Multi)collinearity can be assessed using a â€œVariance Inflation Factorâ€, or VIF. A VIF is calculated for the \\(j^{th}\\) predictor variable as:\n\\[\nVIF_j = \\frac{1}{1-R^2_j}\n\\]\nWhere \\(R^2_j\\) is the \\(R^2\\) from a regression model with predictor \\(j\\) as the response variable and all other predictors still as predictors.\nIn the Florida election example, the VIF for Total_Votes can be found using the \\(R^2\\) for the model:\n\\[\nTotal\\_Votes_i = \\beta_0 + \\beta_1Total\\_Reg_i + \\epsilon_i\n\\]\n\n\n\n\n\n\nThis \\(R^2\\) is huge! Plugging it into the formula:\n\\[\nVIF_{Total\\_Votes}=\\frac{1}{1-0.997} = 333.33\n\\]\nThankfully we donâ€™t have to do this by hand. In jamovi, under â€œLinear Regressionâ€ select â€œCollinearity statisticsâ€:\n\n\n\n\n\nVIF &gt; 10 typically is considered large (note that this would imply \\(R^2 = 0.9\\) between predictor variables).\nThe most obvious thing to do in the presence of (multi)collinearity is to remove one or more correlated predictor variable. From a scientific standpoint, you also may not want highly correlated predictors in the same model.\n\n\n3.6.4 When should we worry about (multi)collinearity?\n(Multi)collinearity is a potentially huge problem if the goal of the regression model is to interpret the estimated slopes.\nThis is because it increases the standard error of these slopes, making their values less reliable. Some people say it makes slopes â€œunstableâ€.\nIt may also complicate the interpretation of slopes: you are trying to statistically â€œhold constantâ€ a predictor variable that doesnâ€™t naturally stay constant when the other predictor varies. This isnâ€™t necessarily a problem, but it is something to be aware of.\nHowever, (multi)collinearity does not negatively impact the predicted values themselves. Remember that it didnâ€™t hurt the \\(R^2\\) value in the Florida election example. \\(R^2\\) tells you how good your predictions are.\nSo, if the model is only for predicting, you probably donâ€™t need to worry about using correlated predictor variables. Just beware when interpreting the slopes."
  },
  {
    "objectID": "Ch3_Model_Fit.html#part-2-improving-models",
    "href": "Ch3_Model_Fit.html#part-2-improving-models",
    "title": "3Â  Chapter 3: Assessing and improving model fit",
    "section": "Part 2: Improving models",
    "text": "Part 2: Improving models\nIn the the first part of chapter 3, we looked at some things that can go wrong in regression modeling, including:\n-    Non-linear relationships between predictor(s) and response\n\n-   Non-normality of residuals\n\n-   Non-constant (heterogeneous) variance of residuals\n\n-   Influential outliers\n\n-   Multicollinearity\nNow weâ€™ll look at some tools available for dealing with these problems."
  },
  {
    "objectID": "Ch3_Model_Fit.html#log-transformation",
    "href": "Ch3_Model_Fit.html#log-transformation",
    "title": "3Â  Chapter 3: Assessing and improving model fit",
    "section": "3.7 Log transformation",
    "text": "3.7 Log transformation\nRecall the regression model:\n\\[\nY_i = \\beta_0 + \\beta_1x_{1i} + \\beta_2x_{2i} + \\dots + \\beta_px_{pi} + \\epsilon_i, \\text{ where } \\epsilon_i \\sim Normal(0,\\sigma)\n\\]\nSometimes we can correct violations of model assumptions by applying a mathematical transformation to the response or predictor variables.\nThe most common transformation in Statistics is the log transformation:\n\\[\nln(x) = log_e(x)\n\\]\n\\(ln(ğ‘¥)\\) is the inverse function of \\(e^ğ‘¥\\), where \\(ğ‘’ = 2.718 \\dots\\)\nIn other words, \\(ln(e^x) = x\\)\nExample: \\(ğ‘’^3 = 20.086; ln(20.086) = 3\\)\nSo, the natural log of \\(x\\) is the number you would have to raise \\(e\\) to so that youâ€™d get \\(x\\).\nNote: in statistics, when we say â€œlogâ€, we usually mean â€œnatural logâ€. It turns out that the distinction is not very important. Iâ€™ll say â€œlog transformâ€\n\n3.7.1 Why log transform?\n\nThere are two main reasons for log transforming a variable:\n\nTo correct for skew in data or residuals\nTo interpret increases in a variable as multiplicative rather than additive.\n\n\nBoth can be understood by recognizing an important property of logarithms; they â€œturn addition into multiplicationâ€\n\\[\nlog(ğ´) + log(ğµ) = log(ğ´ğµ)\n\\]\nIn this sense, logarithms turn addition into multiplication.\nExample: suppose we have data for a skewed variable \\(X_1\\):\n\n\n\n\n\nNow we define \\(x_2 = ln(x_1)\\):\n\n\n\n\n\nThis is a toy â€œdata setâ€. I chose \\(x_1\\) so that \\(x_2 = ln(x_1)\\) would just be the integers \\(1\\) through \\(10\\).\nNote: there is no more skew.\nAlso note: increasing \\(x_2\\) by one unit results in multiplying \\(x_1\\) by \\(e\\). Addition in \\(x_2 = ln(x_1)\\) is the same thing as multiplication in \\(x_1\\).\n\n\n3.7.2 Same again, with log base 2\nEven simpler: define \\(x_2\\) as log base \\(2\\) of \\(x_1\\), i.e.Â \\(log_2(x_1)\\)\n\n\n\n\n\nNow increasing \\(x_2\\) by one unit is equivalent to multiplying \\(x_1\\) by \\(2\\). Addition in \\(x_2 = log_2(x_1)\\) is the same thing as multiplication in \\(x_1\\).\n\n\n3.7.3 Log transforming right-skewed data\n\nSkewed data can be bad for regression, in that it can lead to:\n\n-   Non-linear relationship between X and Y\n\n-   Influential outliers\n\n-   Non-normal residuals\n\n-   Non-constant variance in residuals\nSo a simple log transformation can sometimes go a long way toward making the regression model fit the better!\nIt is most common to log transform a response variable, because assumptions about residuals apply to \\(Y\\), not \\(X\\).\nBut if \\(X\\) is skewed, the model can benefit from a log transformation of \\(X\\).\nBear in mind that log transformation will affect the interpretation of slope coefficients!\nIf \\(X\\) is log transformed, then a one unit increase in \\(ln(ğ‘‹)\\) corresponds to multiplying \\(X\\) by \\(e \\approx 2.72\\). So the slope for \\(ln(ğ‘‹)\\) tells you how much \\(Y\\) increases when \\(X\\) is multiplied by \\(2.72\\). Or, even better, use log base \\(2\\) and the slope will give how much \\(Y\\) changes when \\(X\\) is doubled.\nIf \\(Y\\) is log transformed, then the interpretations of slopes get more complicated. Hereâ€™s the math, with the error term omitted for convenience:\n\\[\nln(y_i) = \\beta_0 + \\beta_1X_i\n\\]\n\\[\n\\therefore y_i = e^{\\beta_0 + \\beta_1X_i}\n\\]\nIncrease \\(X\\) by \\(1 \\dots\\)\n\\[\ny_i^* = e^{\\beta_0 + \\beta_1(X_i + 1)} = e^{\\beta_0 + \\beta_1X_i} \\cdot e^{\\beta_1}\n\\]\nSo, when \\(Y\\) is log transformed, a one unit increase in \\(X\\) multiplies predicted \\(Y\\) by \\(e^{\\beta_1}\\)\n\n\n3.7.4 Interpreting slope as a % change in outcome\nRecall the heights vs.Â wages data from group project 1. The paper reported this estimated model:\n\\[\nln(\\text{wage}) = \\hat{\\beta}_0 + 0.002\\text{(Adult Height)} + 0.027\\text{(Youth Height)} + 0.024\\text{(Age)}\n\\]\nSo, when comparing two adults \\(1\\) inch apart in height but with the same youth height and age predicted wage is multiplied by \\(e^{0.027} = 1.027\\) for the taller adult.\nMultiplying by \\(1.027\\) can be thought of as increasing by \\(2.7\\%\\)\n\n\n3.7.5 Log transformation applied example\nHere is the percent change formula:\n\\[\n\\%\\text{ change (from A to B)} = \\frac{B-A}{A}*100\\%\n\\]\nIf B is \\(1.027*\\)A, then\n\\[\n\\%\\text{ change} = \\frac{1.027*A - A}{A}*100 = \\frac{0.027A}{A}*100 = 2.7\\%\n\\]\nSo, when comparing two adults 1 inch apart in height but with the same youth height and age, predicted wage is \\(2.7\\%\\) higher for the taller adult.\n\n\n3.7.6 Log transformation in \\(Y\\) vs.Â in \\(X\\)\nRemember that log transformation â€œturns addition into multiplicationâ€. So, to keep track of how log transforming \\(Y\\) vs.Â log transforming \\(X\\) affects your model:\n\\[\n\\text{log}(Y_i) = \\beta_0 + \\beta_1X_i + \\epsilon_i\n\\]\n\\[\n\\text{vs.}\n\\]\n\\[\nY_i = \\beta_0 + \\beta_1\\text{log}(X_i) + \\epsilon_i\n\\]\nIf you log transform \\(Y\\) but not \\(X\\), your model estimates the multiplicative change in predicted \\(Y\\) for an additive change in \\(X\\).\nIf you log transform \\(X\\) but not \\(Y\\), your model estimates the additive change in predicted \\(Y\\) for a multiplicative change in \\(X\\)."
  },
  {
    "objectID": "Ch3_Model_Fit.html#non-linearity",
    "href": "Ch3_Model_Fit.html#non-linearity",
    "title": "3Â  Chapter 3: Assessing and improving model fit",
    "section": "3.8 Non-linearity",
    "text": "3.8 Non-linearity\nSometimes data show obvious curvature, in the sense that \\(Y\\) is clearly not a straight line function of \\(X\\).\nThis will be visible on a plot of \\(Y\\) vs.Â \\(X\\). It will also be visible on a residuals vs.Â predicted values plot after running a regression.\nIf there is curvature in the relationship between \\(Y\\) and \\(X\\), then it might be sensible to add a polynomial \\(X\\) term:\n\\[\nY_i = \\beta_0 + \\beta_1x_i + \\beta_2x_i^2 + \\epsilon_i\n\\]\n\n3.8.1 â€œPolynomialâ€ review\nA â€œpolynomialâ€ expression is typically one in which variables are included at different powers. For example, a generic third degree polynomial equation might look like:\n\\[\ny = a + bx + cx^2 + dx^3\n\\]\nA â€œsecond degreeâ€ polynomial is one in which an \\(x\\) and \\(x^2\\) term are both included. This is by far the most common type of polynomial seen in regression models.\n\n\n3.8.2 \\(2^{nd}\\) degree and \\(3^{rd}\\) degree polynomials\n\\(2^{nd}\\) degree polynomials are often called â€œquadraticâ€. \\(3^{rd}\\) degree polynomials are often called â€œcubicâ€. Here are visual examples of simulated quadratic and cubic relationships between \\(Y\\) and \\(X\\):\n\n\n\n\n\n\n\n3.8.3 Curvature in residuals\nHere is regression output comparing a linear model to a quadratic model when the relationship between \\(Y\\) and \\(X\\) is quadratic:\n\n\n\n\n\n\n\n3.8.4 Example: Florida election data\nHere is what the Florida election data look like with the Palm Beach County outlier removed, along with regression results for the simple linear regression model:\n\\[\nBuchanan_i = \\beta_0 + \\beta_1Total\\_Votes_i + \\epsilon_i\n\\]\n\n\n\n\n\nNow we will fit a quadratic polynomial model to the same data:\n\\[\nBuchanan_i = \\beta_0 + \\beta_1Total\\_Votes_i + \\beta_2Total\\_Votes^2_i + \\epsilon_i\n\\]\nTo create this polynomial predictor in jamovi we first need to install the GAMLj module. Then, select â€œGeneralized Linear Modelsâ€ and last our â€œDependent Variableâ€ and â€œCovariatesâ€.\nUnder the â€œModelâ€ drop down menu, click on Total_Votes in the â€œComponentsâ€ table.\nAn up and down arrow will appear with the degree of Total_Votes, click the up arrow so the 1 changes to 2. Then click the right arrow to add Total_Votes2 to â€œModel Termsâ€\n\n\n\n\n\n\\[\nBuchanan_i = \\beta_0 + \\beta_1Total\\_Votes_i + \\beta_2Total\\_Votes^2_i + \\epsilon_i\n\\]\n\n\n\n\n\nThis is better, but we still see curvature in the residual plot.\nLetâ€™s try a cubic model:\n\\[\nBuchanan_i = \\beta_0 + \\beta_1Total\\_Votes_i + \\beta_2Total\\_Votes^2_i +\\beta_3Total\\_Votes_i^3 + \\epsilon_i\n\\]\n\n\n\n\n\n\\[\nBuchanan_i = \\beta_0 + \\beta_1Total\\_Votes_i + \\beta_2Total\\_Votes^2_i +\\beta_3Total\\_Votes_i^3 + \\epsilon_i\n\\]\n\n\n\n\n\n\n\n3.8.5 Example: Florida election data: check note\nItâ€™s debatable whether this is much better. For one, the \\(Total\\_Votes^2\\) term is non-significant.\nBut think back to multicollinearity. Each polynomial term will be correlated with the other terms â€“ after all, \\(Total\\_Votes\\), \\(Total\\_Votes^2\\), and \\(Total\\_Votes^3\\) must all be correlated.\nNote that jamovi will not produce VIFs in GLM. Intuitively we know these will be highly correlated with each other.\nIt turns out that centering helps in polynomial models:\n\n\n\n\n\nBy default jamovi centers polynomials which makes their standard errors smaller than if they were not centered. But, these estimates are not interpretable; you cannot hold \\(Total\\_Votes^2\\) constant while increasing \\(Total\\_Votes\\).\nIn this example, the two counties with the highest total votes are heavily pulling on the regression line."
  },
  {
    "objectID": "Ch3_Model_Fit.html#over-fitting",
    "href": "Ch3_Model_Fit.html#over-fitting",
    "title": "3Â  Chapter 3: Assessing and improving model fit",
    "section": "3.9 Over-fitting",
    "text": "3.9 Over-fitting\nThis model might be â€œover-fitâ€.\nOver-fitting is when a model fits the data so well that it ends up fitting random variation that is not of interest.\n\n\n\n\n\nImagine if the two data points on the right had slightly higher vote counts for Buchanan. Or if the next two to their left had slightly lower vote counts. The curve would look very different.\nAt this point, we might just be modelling noise.\nHere is an extreme example of over-fitting: fitting a â€œsmootherâ€ curve to data and giving it permission to move dramatically up and down through the data.\n\n\n\n\n\nThis line fits the data very well, but does it represent the general trend between total votes and votes for Buchanan? Definitely not!\n(Side note: â€œsmoothersâ€ are great tools for visualizing and summarizing data, but they can be extremely sensitive to degree of smoothing. We wonâ€™t use them in STAT 331)\nCompare the over-fit model to the linear model.\n\n\n\n\n\nThe linear model may be missing out on some curvature. But it might also make better predictions.\nIf we were to observe a new county with \\(450,000\\) total votes, would we be better guessing that votes for Buchanan fall on the highly curved line or on the straight line?"
  },
  {
    "objectID": "Ch3_Model_Fit.html#back-to-basics-is-the-model-sensible",
    "href": "Ch3_Model_Fit.html#back-to-basics-is-the-model-sensible",
    "title": "3Â  Chapter 3: Assessing and improving model fit",
    "section": "3.10 Back to basics: is the model sensible?",
    "text": "3.10 Back to basics: is the model sensible?\n\nBack to basics: regression models are typically used for two purposes:\n\nPredicting values of the response variable, using the predictor variables. This is done by plugging values for the predictor variables into the estimated model.\nEstimating the association between each individual predictor variable and the outcome while statistically holding other predictor variables constant. This is done by interpreted estimated slope coefficients.\n\n\n\n3.10.1 If you just want to make predictions\n\\(R^2\\) is the easiest to understand statistic for assessing how well your model makes predictions. The closer to \\(1\\), the better.\nMulticollinearity isnâ€™t an issue. It doesnâ€™t affect predicted values.\nBUT â€“ beware of overfitting! The more complex your model, the more risk you take of modeling noise instead of signal.\nAlso, be aware that \\(R^2\\) can never go down when adding predictors. You can add complete nonsense predictor variables, and the worst that will happen to \\(R^2\\) is that it stays the same.\n\n\n3.10.2 If you want to interpret slopes\nAlways remember that each slope is interpreted under the assumption that all other predictor variables are being held constant, i.e.Â â€œcontrolled forâ€.\nThe more predictor variables in the model, the less sense this will make.\nExample: wage vs.Â height study:\n\n\n\n\n\nIn model 4, the estimated slope for youth height can be interpreted as:\nâ€œThe predicted difference in ln(wage) for two people one inch apart in youth height, but equal in adult height, whose mothers have the same # of years of schooling and are both in either skilled fields or work or not, whose fathers have the same # of years of schooling and are both in either skilled fields or work or not, and who have the same number of siblings.â€\n\n\n\n\n\nMaybe this is the best way to think about the association between youth height and wages. But it is fairly complicated.\nIf youâ€™re going to try to make â€œreal worldâ€ sense out of regression results, your model should be informed by theory.\nThis is necessarily subjective! You have to choose which variables you think are important. You have to think about what makes sense.\n\nThis might require:\n\nLog transforming a variable solely because you like the multiplicative interpretation better than the additive interpretation.\nKeeping a variable in a model even though it isnâ€™t statistically significant.\nRemoving a variable you are interested in, because it doesnâ€™t make sense to â€œhold it constantâ€ when estimating slopes for other variables.\n\n\n\n\n3.10.3 Is the model missing something important?\nThere is another variable in the Florida election data set that could be worth including: â€œReg_Reformâ€: the total number of voters registered with the Reform Party. Pat Buchanan was the Reform Party candidate. Letâ€™s add it to the model:\n\\[\nBuchanan_i = \\beta_0 + \\beta_1Total\\_Votes_i + \\beta_2Reg\\_Reform_i + \\epsilon_i\n\\]\n\n\n\n\n\nThis residual plot looks great!\n\n\n\n\n\nIt turns out that the curvature in the previous residual plots went away when adding an important variable to the model. No need to mess with polynomials after all!\nAlso, the \\(R^2\\) is roughly the same as in the cubic model using only total votes as a predictor.\nSo we have roughly equal fit, without having to worry about overfitting, and without having to give up interpretability of the slopes.\nOne downside: there is some collinearity. Look at the VIFs.\n\n\n\n\n\nVIF of about \\(5\\) implies \\(\\frac{1}{1-R^2}\\approx 5\\) when using the \\(R^2\\) from:\n\\[\nTotal\\_Votes_i = \\beta_0 + \\beta_1Reg\\_Reform_i + \\epsilon_i\n\\]\nSo, this \\(R^2\\) is about \\(1 âˆ’ \\frac{1}{5} = 0.8\\). And so \\(r = \\sqrt{0.8} =0.89\\) . These predictors are strongly correlated.\n\nNote also that total votes is not significant.\nBut: the slope for \\(Reg\\_Reform\\) has a nice interpretation:\nWhen comparing two counties with the same number of votes cast in the election, a county with an additional registered Reform Party member is estimated to have \\(2.24\\) additional votes, on average, for Pat Buchanan.\nShould total votes be taken out of the model? This is a subjective decision.\n\n\n3.10.4 What would you like to â€œcontrolâ€ for?\nIn regression analysis, we usually emphasize (correctly) that correlation does not imply causation.\nHowever, if you have knowledge or beliefs about causal direction, you should take these into account when choosing your variables!\nExample: in the rheumatoid arthritis study, we were looking at the effect of drugs on inflammation level. In particular, we were comparing how effective they were at reducing inflammation. Suppose we also asked patients to rate their mobility level (RA tends to reduce mobility).\nOur model might be:\n\\[\nDifference_i = \\beta_0 + \\beta_1age_i + \\beta_2drug_i + \\beta_3(age*drug)_i + \\beta_4mobility_i + \\epsilon_i\n\\]\nNow, when interpreting the previous slopes, I am comparing average reduction in inflammation (â€œdifferenceâ€, the response variable), between two people who have the same mobility level. But if they have the same mobility level, then they will have more similar inflammation levels than if we allowed mobility level to differ.\nIn other words, because the drug reduces inflammation and improves mobility, â€œcontrollingâ€ for mobility will make it look like the drugs are less effective than they really are.\n\n\n3.10.5 Beware the â€œkitchen sinkâ€ approach\nThereâ€™s an old saying: â€œtaking everything but the kitchen sinkâ€.\nIt can be tempting to toss everything but the kitchen sink into a regression model, especially when you have loads of variables that all seem like theyâ€™d be associated with the response.\nBut beware! Adding in one predictor can have a dramatic effect on the slopes of other predictors, as well as on their standard errors.\nIt really really really matters that each slope is estimated as though all other predictors are held constant. This can reveal otherwise unseen effects, but it can also obscure otherwise obvious effects or induce apparent effects that arenâ€™t real. There is no substitute for scientific reasoning when choosing a model.\n\n\n3.10.6 The model is simpler than whatâ€™s being modeled\nLetâ€™s take a step back and ask: why are we fitting data to models?\nWell, we are interested in the real world. And the real world in incredibly complicated. Maybe incomprehensibly complicated.\nSo, we simplify things using models. We hope that the model captures the essence of what we care about in the real world. But we know it is a simplification; perhaps an extreme simplification.\nâ€œAll models are wrong; some are usefulâ€ â€“ George Box\nConsider how the regression model describes where data comes from:\n\\[\nY_i = \\beta_0 + \\beta_1x_{i} + \\epsilon_i, \\text{ where } \\epsilon_i \\sim Normal(0,\\sigma)\n\\]\nThis says that to get data, we pick a value for X, go to a line, then randomly draw a value from a normal distribution and add this to the value on the line. And thatâ€™s where data comes from!\nExcept, thatâ€™s not where data comes from. This is a model. It is a simplification of reality. We use these models because we think they will help us answer questions we care about (e.g.Â make predictions, identify associations between variables). Donâ€™t forget that the model is not the thing itself."
  },
  {
    "objectID": "Ch4_ANOVA.html#outline-of-notes",
    "href": "Ch4_ANOVA.html#outline-of-notes",
    "title": "4Â  Chapter 4: ANOVA-based methods",
    "section": "Outline of notes",
    "text": "Outline of notes\n\nWhat is ANOVA?\nOne-way ANOVA\nFactorial ANOVA\nANCOVA"
  },
  {
    "objectID": "Ch4_ANOVA.html#what-is-anova",
    "href": "Ch4_ANOVA.html#what-is-anova",
    "title": "4Â  Chapter 4: ANOVA-based methods",
    "section": "4.1 What is ANOVA?",
    "text": "4.1 What is ANOVA?\nANOVA stands for â€œanalysis of varianceâ€\nANOVA is regression with categorical predictors. Thatâ€™s it.\n\n4.1.1 ANOVA is regression presented differently\nOK, thereâ€™s more to say about ANOVA than just â€œregression with categorical predictors.â€\nANOVA is typically used to analyze data from experiments. In experiments, the categorical predictors are usually groups to which experimental units (aka subjects) are assigned. ANOVA tends to focus on comparing means of different groups to one another. Although ANOVA is â€œjustâ€ regression, there are conventions for reporting ANOVA results that are simpler and cleaner than what weâ€™ve seen for regression.\n\n\n4.1.2 Indicator variables\nWeâ€™ve seen how to incorporate a categorical predictor into a regression model when the predictor takes on two values. We create an â€œindicatorâ€ (aka â€œdummyâ€ aka â€œbinaryâ€) variable that takes on the values 0 or 1. For example:\n\\[\nY_i=\\beta_0+\\beta_1x_{1i} + \\epsilon_i\n\\]\n\\[\nx_i = 1 \\text{ if \"treatment;\"} x_i = 0 \\text{ if \"control\"}\n\\]\nHere, \\(\\hat{y}_i = \\hat{\\beta}_0\\) for control, and \\(\\hat{y}_i = \\hat{\\beta}_0 + \\hat{\\beta}_1\\) for treatment.\nWhat if our categorical predictor takes on more than two categories? Suppose we have three groups: treatment 1, treatment 2, and control. We can add another indicator:\n\\[\nY_i=\\beta_0+\\beta_1x_{1i} + \\beta_2x_{2i} + \\epsilon_i\n\\]\n\\[\nx_{1i} = 1 \\text{ if treatment }1; x_{1i} = 0 \\text{ otherwise} \\\\\nx_{2i} = 1 \\text{ if treatment }2; x_{1i} = 0 \\text{ otherwise}\n\\]\n\\(\\hat{y}_i = \\hat{\\beta}_0\\) for control, \\(\\hat{y}_i = \\hat{\\beta}_0 + \\hat{\\beta}_1\\) for treatment 1, and \\(\\hat{y}_i = \\hat{\\beta}_0 + \\hat{\\beta}_1 + \\hat{\\beta}_2\\) for treatment 2.\nControl is serving as the â€œbaselineâ€ category, represented by the intercept."
  },
  {
    "objectID": "Ch4_ANOVA.html#one-way-anova",
    "href": "Ch4_ANOVA.html#one-way-anova",
    "title": "4Â  Chapter 4: ANOVA-based methods",
    "section": "4.2 One-Way ANOVA",
    "text": "4.2 One-Way ANOVA\nOne-way ANOVA models have a single categorical predictor variable. They can be written as:\n\\[\nY_i = \\mu + \\alpha_j + \\epsilon_{ij}, \\text{ where } \\epsilon_{ij} \\sim Normal(0,\\sigma^2) \\text{ and } i = 1, \\dots ,n_j, j = 1, \\dots, p\n\\]\nWhere there are â€œ\\(p\\)â€ groups (i.e.Â the categorical predictor takes on â€œ\\(p\\)â€ values), â€œ$n_j$â€ is the sample size of the \\(j^{th}\\) group, and \\(y_{ij}\\) is the \\(i^{th}\\) observation in the \\(j^{th}\\) group.\nHere, \\(\\mu\\) is the overall mean and \\(\\alpha_j\\) is the deviation of the \\(j^{th}\\) group mean from the overall mean.\nSuppose we have 4 groups. The ANOVA model is:\n\\[\nY_i = \\mu + \\alpha_j + \\epsilon_{ij}, \\text{ where } \\epsilon_{ij} \\sim Normal(0,\\sigma^2) \\text{ and } i = 1, \\dots ,n_j, j = 1, \\dots, 4\n\\]\nWritten as a regression model instead:\n\\[\nY_i = \\beta_0 +\\beta_1Group1 + \\beta_2Group2 + \\beta_3Group3 + \\epsilon_{i}, \\text{ where } \\epsilon_{i} \\sim Normal(0,\\sigma^2) \\text{ and } i = 1, \\dots ,n\n\\]\nHere, the â€œGroupâ€ predictors are \\(0\\) / \\(1\\) indicator variables. For group \\(4\\), the mean of \\(y\\) is the intercept, \\(\\beta_0\\)."
  },
  {
    "objectID": "Ch4_ANOVA.html#factorial-anova",
    "href": "Ch4_ANOVA.html#factorial-anova",
    "title": "4Â  Chapter 4: ANOVA-based methods",
    "section": "4.3 Factorial ANOVA",
    "text": "4.3 Factorial ANOVA\nIf we have more than one categorical predictor variable, and interactions between the predictors, we have a factorial ANOVA model. Generically:\n\\[\nY_{ijk} = \\mu + \\alpha_j + \\beta_k + (\\alpha\\beta)_{jk} + \\epsilon_{ijk}, \\text{ where } \\epsilon_{ijk} \\sim Normal(0,\\sigma^2) \\\\\n\\text{ and } i = 1, \\dots ,n_{jk}, j = 1, \\dots, p, k = 1, \\dots,q\n\\]\nNow predictor \\(1\\) is \\(\\alpha\\) and predictor \\(2\\) is \\(\\beta\\) and they interact. There are â€œ$p$â€ groups for predictor \\(1\\) and â€œ$q$â€ groups for predictor \\(2\\).\nThe subscripts start getting messy pretty fast.\n\n4.3.1 Factorial ANOVA example\nHereâ€™s an example of a â€œ5x2 factorialâ€ ANOVA, meaning that one variable has 5 groups and the other has 2.\nThe study is on memory: how many words, on average, do people recall when given certain processing tasks?\nThis example is where the graph on our Canvas home page comes from. It uses data simulated to mimic data from a 1974 Hans Eysenck study.\n\n100 subjects were split into 5 recall groups:\n\nâ€œCountingâ€: subjects counted how many letters were in each presented word\nâ€œRhymingâ€ subjects thought of words that rhymed with each presented word\nâ€œAdjectiveâ€: subjects thought of an adjective that could be used to modify each presented word\nâ€œImageryâ€: subjects were told to form vivid images of each word\nâ€œIntentionalâ€: subjects were told to memorize the word for later recall\n\n\nCounting and rhyming are lower level processing tasks, so the hypothesis was that this group would recall fewer words than the others. Subjects were also classified as â€œyoungâ€ or â€œoldâ€.\nHereâ€™s the ANOVA model:\n\\[\nRecall_{ijk} = \\mu + Group_j + Age_k + (Group \\cdot Age)_{jk} + \\epsilon_{ijk}, \\text{ where } \\epsilon_{ijk} \\sim Normal(0,\\sigma^2) \\\\\n\\text{ and } j = 1, \\dots ,5, k = 1, 2\n\\]\nJamovi has an â€œANOVAâ€ function and a â€œregressionâ€ function. Weâ€™ll use both the analyze these data.\n\n\n\n\n\nFirst weâ€™ll use ANOVA, using items recalled as the response (dependent) variable, and recall condition and age as factors. Note that jamovi automatically includes their interaction.\n\n\n\n\n\nI am usually not interested in the sums of squares or mean squares in the ANOVA table, as these are not interpretable. We do have p-values for each â€œmain effectâ€ along with the interaction. More importantly, we have effect size statistics: eta-squared (\\(\\eta^2\\)) and partial eta-squared (\\(\\eta^2p\\))\n\n\n4.3.2 ANOVA effect sizes: \\(\\eta^2\\) and partial-\\(\\eta^2\\)\n\\(\\eta^2\\) is akin to \\(R^2\\) for one factor (main effect or interaction). It gives the proportion of variance in the response attributable to the factor in question:\n\\[\n\\eta^2 = \\frac{SS_{factor}}{SS_{total}}\n\\]\n\n\n\n\n\nHere, we see that recall condition explains the most variance by far, followed by age, followed by their interaction.\n\\(\\eta^2p\\) (partial eta-squared) is like \\(\\eta^2\\), but with the variance accounted for by the other factors removed from the denominator:\n\\[\n\\eta^2p = \\frac{SS_{factor}}{SS_{factor} + SS_{residuals}}\n\\] \nExample: for recall condition,\n\\[\n\\eta^2p = \\frac{1515}{1515 + 722} = 0.677, \\text{ and } \\eta^2 = \\frac{1515}{1515 + 240 + 190 + 722} = 0.568\n\\]\nI personally prefer \\(\\eta^2\\), as their sum cannot exceed \\(1\\). Some prefer \\(\\eta^2p\\), because it quantifies the â€œeffectâ€ of a factor relative to remaining unexplained variance,\n\n\n4.3.3 The means / interaction plot\nWe can make a nice plot under â€œestimated marginal meansâ€:\n\n\n\n\n\nThis plot shows every combination of means across recall condition and age. It also has 95% CIs around each mean, and raw data displayed.\n\n\n\n\n\nThis is sometimes called a â€œmeans plotâ€ or an â€œinteraction plotâ€. The interaction is represented by non-parallel lines, e.g.Â a positive change going from â€œImageryâ€ to â€œIntentionâ€ for young people, but a negative change for old people.\nThe previous plot showed means for recall condition, with separate lines for age. If we flip the order of the variables, we get this:\n\n\n\n\n\nHere we see that mean items recalled for young people is substantially higher than for old people in the last three conditions, but differs only slightly in the first two conditions.\n\n\n4.3.4 ANOVA diagnostics\nWe can also run diagnostic tests, under â€œAssumption checksâ€:\n\n\n\n\n\nThe QQ plot looks pretty good.\n\n\n\n\n\nI personally do not recommend paying attention to Leveneâ€™s â€œhomogeneity of variancesâ€ test, nor to the Shapiro-Wilk normality tests\n\n\n\n\n\nThese tests use null hypotheses of â€œpopulation variance is the same in all groupsâ€, or â€œthe residuals were drawn from a normal distributionâ€. As I donâ€™t think these model assumptions could be literally true, I am not interested in whether they can be rejected by the data.\nA â€œsignificantâ€ violation of modeling assumptions does not imply a consequential violation. In particular, if sample size is large, trivial violations of assumption\n\n\n4.3.5 Doing all of this as regression\nThe beginning of these slides claimed that ANOVA is just regression with categorical predictors. Letâ€™s see what our results look like if we use jamoviâ€™s â€œlinear regressionâ€ function rather than â€œANOVAâ€.\n\n\n\n\n\nThe predictor variables are entered as â€œfactorsâ€ here, rather than as â€œcovariatesâ€, to ensure they are treated as categorical rather than as quantitative variables.\n\n\n\n\n\nFinally, the interaction must be specified under â€œmodel builderâ€; jamovi does not create regression interactions by default.\nThere are five recall condition groups, giving four indicator variables, all compared against the baseline group â€œcountingâ€, whose mean is represented by the intercept.\nThere are two â€œageâ€ groups; â€œyoungâ€ is the baseline group.\n\n\n\n\n\nSo the intercept of \\(6.500\\) is the mean words recalled for a young person in the â€œcountingâ€ condition.\nThe first indicator under RecallCondition is â€œRhyming â€“ Countingâ€. Its slope of \\(1.1\\) is the difference in mean recall for these two groups, when Age = Young\nThe interaction slope for â€œRhyming â€“ Countingâ€ is \\(-1.2\\).\nSo, for Age = Old, the difference in mean recall is \\(1.1 +(-1.2) = âˆ’0.1\\)\nThe interaction terms are all indicators, that â€œturn onâ€ when Age = Old, and â€œturn offâ€ when Age = Young.\nNotice that the slope for Age (â€œOld â€“ Youngâ€) is 0.5, suggesting greater recall for older participants.\nHowever, the interaction slopes are all larger negative values.\nSo, when RecallCondition = Counting, the mean items recalled for Age = Old is larger than for Age = Young. But for all other recall conditions, the interaction slopes turn this negative, and mean items recalled is larger for younger participants.\n\n4.3.5.1 Main regression results\nâ€œEstimated Marginal Meansâ€ under regression will produce a similar plot to the one made under ANOVA, just without the connecting lines and raw data:\n\n\n\n\n\nIf you look carefully, you should be able to see how the regression results correspond to this plot. For instance, we see that the only condition where Old &gt; Young is Counting.\n\n\n\n4.3.5.2 Residual plot\nWe can get a plot of residuals vs.Â fitted values.\n\n\n\n\n\nNotice that the residuals are all vertically stacked? This is to be expected when predictor variables are categorical.\nIn this case, there are 5x2 = 10 possible combinations of groups that participants could be assigned to. And so there are only 10 possible â€œfittedâ€ (i.e.Â predicted) values that the model can produce."
  },
  {
    "objectID": "Ch4_ANOVA.html#ancova",
    "href": "Ch4_ANOVA.html#ancova",
    "title": "4Â  Chapter 4: ANOVA-based methods",
    "section": "4.4 ANCOVA",
    "text": "4.4 ANCOVA\nANCOVA (analysis of covariance) is ANOVA with an additional continuous predictor variable.\nTypically, this additional continuous predictor is not of primary interest; the primary interest is still comparing group means.\nThe continuous predictor is often thought of as a â€œcovariateâ€ â€“ a variable that should be accounted for when drawing inference on the other variables.\nA common use of ANCOVA is for modeling an outcome when â€œbaselineâ€ or â€œpre-studyâ€ or â€œpre-testâ€ scores are available.\nFor instance, consider testing different educational models on different sections of a class. Some get traditional lecture, some are completely â€œflippedâ€, and some are a combination of the two.\nIn this study, a preliminary quiz is given on the first day of class. Score on the preliminary quiz will be the covariate. Score on an end of semester quiz (â€œpostâ€) will be the response variable.\n\\[\nPost_{ij} = \\mu + type_j + \\beta(pre_{ij} - \\overline{pre}) + \\epsilon_{ij}\n\\]\nHere, \\(j\\) goes from \\(1\\) to \\(3\\), for the three teaching types being compared.\nThe pre-test predictor is centered (note that \\(\\overline{pre}\\) is mean for pretests).\nImagine we believe that the mean differences between teaching types will be larger for students with lower pretest scores. To account for this possibility, let type and (centered) pretest interact:\n\\[\nCourse\\_avg_{ij} = \\mu + type_j + \\beta(pre_{ij} - \\bar{pre}) + \\gamma_j(type_j)(pre_j - \\overline{pre}) + \\epsilon_{ij}\n\\]\n\n4.4.1 Looking at the data\nThe data file is called â€œtest_pretestâ€. Here is the formatting:\n\n\n\n\n\nThis plot was made using Analyses / Exploration / Scatterplot. Density curves are there, just for fun:\n\n\n\n\n\n\n\n\n\n\nThe three lines look pretty close to parallel, so there either isnâ€™t an obvious interaction here, or itâ€™s small.\n\n\n\n\n\n\n\n4.4.2 Analysis using ANCOVA\nNotice that the effect size for pre- score is far greater than the effect size for type, or for the interaction. This is not surprising.\n\n\n\n\n\nType is still significant; itâ€™s \\(\\eta^2p\\) value is much larger than its \\(\\eta^2\\) value.\nCareful interpreting the â€œEstimated marginal meansâ€ plot â€“ it takes into account only â€œpostâ€ scores!\n\n\n4.4.3 Analysis using regression w/ indicators\nFor the regression analysis, weâ€™ll use two indicator variables. We donâ€™t have to do it this way; if we just include â€œtypeâ€, jamovi will create the indicators for us, using the first class type listed in the data.\n\n\n\n\n\nNotice that the interaction estimates are small relative to their standard errors (thus producing large p- values).\n\n\n\n\n\nAccording to this regression model, there is not a â€œsignificantâ€ interaction.\n\n\n4.4.4 Analysis using regression w/factor\nHereâ€™s what happens if we put in â€œtypeâ€ as a factor variable rather than making our own indicators. The results are the same.\n\n\n\n\n\nThe QQ plot and residual plot both look great\n\n\n\n\n\n\n\n4.4.5 Analyzing paired data: ANCOVA vs ANOVA\nAnother approach we could take would be to compute the differences in the two scores for each person, then do a regular ANOVA or regression analysis on those.\n\n\n\n\n\nWe see that there are significant differences in mean test score change between teaching types ($F=8.94, p&lt;0.001$)\n\n\n\n\n\n\n\n\n\n\n\n\n4.4.6 Analyze differences, regression approach\nThis agrees with the ANOVA results from the previous slide: we have a statistically significant difference in mean test score differences (post â€“ pre) when comparing â€œcomboâ€ to â€œflippedâ€ or â€œtraditionalâ€. And \\(R^2 = \\eta^2\\)!\n\n\n\n\n\nIt turns out here that taking the post â€“ pre differences first and then comparing mean differences across teaching types produces similar results to predicting post-test scores using pre-test and teaching type as predictors.\nBut, these methods are not answering the exact same question. Using pre-test as a covariate, we answer the question â€œwhat difference do I expect in post-test scores when comparing two students with the same pre- test score but different teaching typesâ€?\nWhen differencing first and then doing the analysis, we answer the question â€œwhat differences in the mean post-pre score change do I expect when comparing class typesâ€?\nThese question sound similar, but they arenâ€™t the same! Whether to use ANCOVA or do the differencing first is a matter of subjective judgement, and the experts donâ€™t all agree (see â€œLordâ€™s Paradoxâ€ for more fun on this)."
  },
  {
    "objectID": "Ch5_Categorical.html#outline-of-notes",
    "href": "Ch5_Categorical.html#outline-of-notes",
    "title": "5Â  Chapter 5: Analyzing categorical data",
    "section": "Outline of notes",
    "text": "Outline of notes\n\nConfidence intervals and hypothesis tests for proportions\nRelative Risk\nThe chi-square test"
  },
  {
    "objectID": "Ch5_Categorical.html#confidence-intervals-and-hypothesis-tests-for-proportions",
    "href": "Ch5_Categorical.html#confidence-intervals-and-hypothesis-tests-for-proportions",
    "title": "5Â  Chapter 5: Analyzing categorical data",
    "section": "5.1 Confidence intervals and hypothesis tests for proportions",
    "text": "5.1 Confidence intervals and hypothesis tests for proportions\nAt the beginning of the class we reviewed confidence intervals and hypothesis tests for means. These methods can also be used for proportions\nA proportion is just a special kind of mean, where the data are all ones and zeros.\nExample: 8 out of 10 people say â€œyesâ€ to the question â€œIs politics too polarized?â€ Let yes = 1 and no = 0. Average is:\n\\[\n\\bar{x} = \\frac{1 + 1+ 1 + 1+ 1+1+1+1+0+0}{10} = \\frac{8}{10} =0.8\n\\]\n\nWe make confidence intervals and perform hypothesis tests for proportions using the exact same methods used for means. Only differences are:\n\nSampling distribution of proportions follow z, rather than t (the difference is usually trivial)\nWe denote the population proportion \\(\\pi\\) and the sample proportion \\(\\hat{\\pi}\\)\nStandard error of a sample proportion is \\(s_{\\hat{\\pi}} = \\sqrt{\\frac{\\hat{\\pi}(1-\\hat{\\pi})}{\\pi}}\\)\n\n\n\n5.1.0.1 Political polling example\nHere are results from a recent political poll:\n\n\n\n\n\nSay we want to make a confidence interval for the proportion of registered voters who say that â€œthings in the country are going in the right directionâ€. Denote this population proportion \\(\\pi\\).\n\n\n\n\n\n\\[\n95\\% \\text{ CI for }\\pi:\n\\hat{\\pi} \\pm z_{critical} *s_{\\hat{\\pi}} \\\\\n= 0.39 Â± 1.96 âˆ— \\sqrt{\\frac{0.39*0.61}{1978}}\\\\\n= 0.39 Â± 0.022 \\\\\n= (0.368, 0.412)\n\\]\nIn jamovi, enter a column of frequencies for each value of the response variable, then use Frequencies / N â€“ Outcomes or Frequencies / 2 Outcomes:\n\n\n\n\n\n\n\n5.1.1 Toy example: skin cream and rashes\nHere is an example from the 2013 paper â€œMotivated Reasoning and Enlightened Self Governmentâ€, by Kahan et. al.:\n\n\n\n\n\n\n\n\n\n\nPutting the data into jamovi:\n\n\n\n\n\nAnalyzing the data using Frequencies / Independent Samples:\n\n\n\n\n\nRash is the response variable\nSkin cream is the predictor variable\nFrequency tells how often each combination occurred. (note: if you had raw data where each row was a single response, you would not use Freq)\nResults! Thereâ€™s a lot in hereâ€¦\n\n\n\n\n\n\n\n\n\n\nSplit bar plot: displays each cell in the contingency table as a bar:\n\n\n\n\n\nHere we can easily see that the largest number of people were those who got the skin cream and whose rash got better.\nBut, we can also see that rashes got better at a higher rate for those who did not get the skin cream.\nWeâ€™ll cover the chi-square results soon. Right now, letâ€™s have jamovi directly compare proportions, using Frequencies / Independent Samples:\n\n\n\n\n\nHere, jamovi quantifies what we saw in the split bar plot: that the proportion of those who got better without the skin cream is greater than the proportion of those who got better with the skin cream:\n\n\n\n\n\n\\[\n\\text{\"Probability of Better, given Yes\" - \"Probability of Better, given No\" } =-0.0876 \\\\\nP(Better|Yes) - P(Better|No) = -0.0876\n\\]\nWe also see a 95% CI for this difference, which is fairly wide and just barely excludes zero\n\n\n\n\n\nAnd we see the p-value testing against:\n\\[\nH_0:\\pi_{(Better|Yes)} - \\pi_{(Better|No)} = 0\n\\]\nThe two-sided p-value is 0.047, so this result is just barely significant. Woohoo!"
  },
  {
    "objectID": "Ch5_Categorical.html#relative-risk",
    "href": "Ch5_Categorical.html#relative-risk",
    "title": "5Â  Chapter 5: Analyzing categorical data",
    "section": "5.2 Relative risk",
    "text": "5.2 Relative risk\nInstead of knowing the difference in proportions / probabilities, we may want to know their ratio. This would tell us how many times larger one is than the other.\nThis is quantified by the â€œrelative riskâ€, a.k.a. â€œrisk ratioâ€ :\n\\[\nRR = \\frac{\\text{Proportion A}}{\\text{Proportion B}}\n\\]\nThe phrase â€œriskâ€ is used because this method is popular for comparing the risk of a negative outcome under two conditions (e.g.Â treatment and control, or drug A and drug B)\n\n5.2.1 Relative risk in jamovi\nâ€œRelative Riskâ€ is an option under Comparative Measures. jamovi will give conditional probabilities based on the order of the data. Here, rash getting worse is Better is selected:\n\n\n\n\n\nWe see here that \\(RR = 0.895\\).\n\n\n\n\n\nNotice the 95% CI is not very wide.\nWe could also flip this ratio:\n\\[\n\\frac{P(Better|No)}{P(Better|Yes)}\n\\]\nNow everything is reciprocated, e.g.\n\\[\n\\frac{1}{0.809}=1.236\n\\]\nNotice that, for â€œBetterâ€, RR is small but (just barely) statistically significant, because the CI does not contain 1.\nFor â€œWorseâ€, RR is large but (just barely) not statistically significant, because the CI contains 1.\nThere is another very popular kind of ratio called an â€œodds ratioâ€, which we will consider more when we cover logistic regression."
  },
  {
    "objectID": "Ch5_Categorical.html#the-chi-square-test",
    "href": "Ch5_Categorical.html#the-chi-square-test",
    "title": "5Â  Chapter 5: Analyzing categorical data",
    "section": "5.3 The chi-square test",
    "text": "5.3 The chi-square test\nThe chi-square ( \\(\\chi^2\\)) test is a popular hypothesis test for comparing observed frequencies to expected frequencies under a null hypothesis.\nThe null is typically that of â€œindependenceâ€ between two categorical variables, meaning the probability an observation falls into a category for one variable does not depend on its category for the other variable\n(As a side note, \\(\\chi^2\\) is a distribution that is used for many purposes, including modeling distributions of variances. A statistic that is distributed chi-square is not necessarily being used in the context of the chi-square test outlined here)\nStaying with the skin cream example, here is the contingency table as jamovi initially reports it:\n\n\n\n\n\nThere is a lot being shown here. Top rows are observed frequencies, what jamovi calls â€œobservedâ€.\nThe two middle rows are column % and row %.\nNotice that the column %â€™s sum to 100% down the columns, and the row %â€™s sum to 100% across the rows.\nWe can have jamovi display the components of a chi-square test.\nThe first of these are the expected counts under the null hypothesis of independence.\n\n\n\n\n\nTo see how these â€œexpectedâ€ counts would suggest independence, consider the relative risk:\n\\[\nRR = \\frac{P(Worse|No)}{P(Worse|Yes)} \\\\\n= \\frac{(\\frac{28.8451}{28.8451 + 99.1549})}{(\\frac{67.1549}{67.1549 + 230.845})} \\\\\n= \\frac{(\\frac{28.8451}{128})}{(\\frac{67.1549}{298})} \\\\\n= \\frac{0.2254}{0.2254} \\\\\n=1\n\\]\n\n5.3.0.1 Chi-square statistic\nThe chi-square statistic compares the expected frequencies under the null (which we denote E) to the observed frequencies in the data (which we denote O).\n\\[\n\\chi^2 = \\sum\\frac{(O-E)^2}{E}\n\\]\nThis is a general formula that can be used when you have one variable, two variable, three variables, etc.\nMost popular use is for two variables, as in this skin cream example.\nJust to verify the math, hereâ€™s the chi-square calculation for the upper left cell of the table:\n\\[\n\\sum\\frac{(O-E)^2}{E} = \\frac{(107-99.2)^2}{230.8} + \\frac{(223-230.8)^2}{230.8} + \\frac{(75-67.2)^2}{67.2} + \\frac{(21-28.8)^2}{28.8} =3.94\n\\]\nAnd here is the jamovi output showing the full chi-square test:\n\n\n\n\n\n(â€œPearsonâ€ chi-square is the classic chi-square test)\nVerifying that this chi-square statistic is indeed the sum of the chi-square values for each of the four cells:\n\\[\n\\sum\\frac{(O-E)^2}{E} = 0.6207 + 2.1336 + 0.2666 + 0.9165 = 3.937\n\\]\nThe p-value is found using the appropriate degrees of freedom for the chi-square distribution. We wonâ€™t cover this part.\nIn this case, we see that the chi-square test just barely meets the standard for statistical significance (\\(p = 0.0472\\)).\nThis is in line with the other methods we used to analyze these data.\nRecall that the test for a difference in proportions was barely significant, and the risk ratios were just on either side of significance, depending on whether â€œrash got worseâ€ or â€œrash got betterâ€ was used as the outcome variable.\nThe chi-square test can be used when we have frequencies for a single variable. All we have to do is specify expected counts or probabilities.\nGoing back to the polling data, we can select Frequencies / N - Outcomes, and then make Answer â€œVariableâ€ and Frequency â€œCountsâ€.\n\n\n\n\n\nSuppose the null hypothesis is that equal numbers of voters feel the country is on the right vs.Â wrong track. Enter 0.5 for hypothesized probability:\nHere we get a very large chi-square statistic and a very small p-value.\n\n\n\n\n\nNo surprise; the frequencies were very different!\nSide note: there are lots of different â€œchi-square testsâ€; this term refers to any test whose test statistic follows a chi-square distribution. So you may see â€œchi-square testsâ€ that are not being used to test against a null of independence for categorical variables.\n\n\n5.3.1 Wrapping up\nMy personal view is that hypothesis testing should only be used when a null hypothesis is of direct scientific interest. In general, Iâ€™m more interested in statistics that quantify the size of an â€œeffectâ€ than I am in deciding whether or not to reject a null of â€œno effectâ€. So, I am not a big fan of this chi-square test. To compare rates for categorical variables, I prefer a 95% CI around either a relative risk or a difference in proportions, whichever seems more meaningful for the question at hand.\nIn the case of a political poll, I donâ€™t believe that identical proportions of voters in the country would endorse two different statements, and so Iâ€™m not very interested testing this as a null hypothesis. Iâ€™m interested in how large a difference might be, and how much statistical uncertainty there is an estimate for that difference.\nThe methods weâ€™ve covered in these notes are useful for analyzing fairly simple categorical data. In the next set of notes, we will look at logistic regression, which is a way to use regression modeling to predict the outcome of a categorical variable."
  },
  {
    "objectID": "Ch6_GLMs.html#outline-of-notes",
    "href": "Ch6_GLMs.html#outline-of-notes",
    "title": "6Â  Generalized Linear Models (GLMs)",
    "section": "Outline of notes",
    "text": "Outline of notes\n\nLogistic regression\nOdds\nInterpreting slope in logistic regression\nPoisson regression\nThe structure of a GLM\nMaximum likelihood estimation, conceptually\nPoisson regression example\nNegative binomial regression"
  },
  {
    "objectID": "Ch6_GLMs.html#logistic-regression",
    "href": "Ch6_GLMs.html#logistic-regression",
    "title": "6Â  Generalized Linear Models (GLMs)",
    "section": "6.1 Logistic regression",
    "text": "6.1 Logistic regression\nAll of the regression methods weâ€™ve seen have involved models in which the response variable is normally distributed, given values for the predictor variables\nIn other words, the residuals have been modeled as normal.\nWhat if we have a different kind of response variable? In particular, consider a binary response variable. Maybe the outcomes are â€œyesâ€ and â€œnoâ€, or â€œsuccessâ€ and â€œfailureâ€, or â€œpresentâ€ and â€œabsentâ€.\nLogistic regression is a type of â€œgeneralized linear modelâ€ (GLM) that works well for modeling binary outcome data.\nBefore we get into logistic regression, though, letâ€™s see what happens if we use standard regression (sometimes called â€œordinary least squaresâ€, or OLS regression) with a binary response.\nWeâ€™ll use simulated data corresponding to a study of sexual harassment reporting at a university. (Brooks and Perot â€œReporting Sexual Harassment: Exploring a Predictive Modelâ€ (1991)).\nHere is data on whether or not sexual harassment at a university was reported, using the offensiveness of the behavior as a predictor variable:\n\n\n\n\n\nData points are â€œjitteredâ€ so that they donâ€™t fall right on top of one another.\nSuppose we want to predict the value of â€œReportâ€, using â€œOffensBehâ€.\nHere is the linear regression line. In this picture, the response variable takes on the values 0 and 1, and the data are not jittered.\n\n\n\n\n\nThe predicted value of â€œReportâ€ can be thought of as the predicted probability that Report=1 (for reported behavior)\nNote that this line can go below zero and above one. We donâ€™t want to predict probability greater than 1! A straight line is not great here. Logistic regression is an alternative to this straight line model.\n\n6.1.1 The logistic regression model\nBy now we are well familiar with the linear regression model:\n\\[\nY_i=\\beta_0+\\beta_1x_{1i} + \\beta_2x_{2i} + \\dots + \\beta_px_{pi} + \\varepsilon_i \\\\\n\\varepsilon_i \\sim \\text{Normal}(0,\\sigma^2)\n\\]\nHere is an equivalent way of writing it:\n\\[\nY_i \\sim Normal(\\mu_i, \\sigma^2) \\\\\n\\mu_i=\\beta_0+\\beta_1x_{1i} + \\beta_2x_{2i} + \\dots + \\beta_px_{pi}\n\\]\nIn other words, the response variable is normally distributed with some mean \\(\\mu\\), and the value of \\(\\mu\\) is determined by the predictor (\\(x\\)) variables.\n\n\n6.1.2 Writing a logistic regression model\nWe will take this approach to writing the logistic regression model.\nWhat we want is a regression equation that looks like this:\n\\[\nY_i=\\beta_0+\\beta_1x_{1i} + \\beta_2x_{2i} + \\dots + \\beta_px_{pi}\n\\]\nBut that will work when \\(Y_i\\) does not follow a normal distribution.\n\n\n6.1.3 The theoretical logistic regression model\nResponse variable \\(Y\\) takes on the values 0 and 1.\nDenote the probability that \\(Y = 1\\) as \\(\\pi\\).\nThis can be written \\(Y\\sim Bernoulli (\\pi)\\)\n(The Bernoulli distribution is a distribution of 1â€™s and 0â€™s, where the probability of 1 is \\(\\pi\\) and the probability of 0 is \\(1 âˆ’ \\pi\\))\nWe will use regression to model \\(\\pi\\), the probability that \\(Y = 1\\). This is often thought of as the probability of a â€œsuccessâ€.\nIf we wanted, we could use this model:\n\\[\nY_i\\sim Bernoulli (\\pi_i)\n\\\\\n\\pi_i=\\beta_0+\\beta_1x_{1i} + \\beta_2x_{2i} + \\dots + \\beta_px_{pi}\n\\]\nThe standard deviation of a Bernoulli distribution is \\(\\pi(1 âˆ’ \\pi )\\). So, \\(\\pi\\) is the only parameter for this distribution. This is different from the normal distribution, which has two parameters \\(\\mu\\) and \\(\\sigma\\).\n\n\n6.1.4 Logit: log odds\nBut, as we saw in the opening example, a linear model for probability can have serious deficiencies.\nSo, instead of a linear model for probability \\(\\pi\\), weâ€™ll make a linear model for a function of \\(\\pi\\), so that the variable on the left hand side of the equation is linearly related to the variable(s) on the right.\nIn logistic regression, we use the â€œlogitâ€ function, also known as â€œlog oddsâ€\n\\[\nlogit(\\pi) = ln(\\frac{\\pi}{1-\\pi}) = \\textit{\"log odds\"}\n\\]\nApplied to our sexual harassment example, we would like to predict the probability that harassing behavior is reported. This probability is denoted \\(\\pi\\)\nPlugging this into the logit formula:\n\\[\nlogit(P(reported)) = ln(\\frac{P(reported)}{1-P(reported)}) = \\textit{\"log odds\" of reporting}\n\\]\nThis will be our response variable for logistic regression. Note that this time we wrote \\(P(reported)\\) rather than \\(\\pi\\); these mean the same thing.\nLogit vs.Â probability, visually:"
  },
  {
    "objectID": "Ch6_GLMs.html#odds",
    "href": "Ch6_GLMs.html#odds",
    "title": "6Â  Generalized Linear Models (GLMs)",
    "section": "6.2 Odds",
    "text": "6.2 Odds\n\n6.2.1 Odds vs.Â probability\nTo understand logistic regression, youâ€™ll need to understand odds.\nIn casual English, â€œoddsâ€ and â€œprobabilityâ€ are often used interchangeably.\nIn statistics, they are not the same thing. Odds tell you how likely one outcome is compared to another.\nFor instance, you might hear that a football team has been given â€œ3 to 2â€ odds of winning a game. This means that their probability of winning is \\(\\frac{3}{2}= 1.5\\) times as big as their probability of losing. Or, that theyâ€™d be expected to win 3 times for every 2 times they lost.\nFormally, consider some outcome A, where the probability of A occurring is written as â€œ\\(P(A)\\)â€. In this case,\n\\[\nodds(A) = \\frac{P(A)}{1-P(A)} = \\frac{\\textit{probability A occurs}}{\\textit{probability A does not occur}}\n\\]\nThis is the ratio of the probability A occurs to the probability A does not occur. To convert odds into probability, we use:\n\\[\nP(A) = \\frac{odds(A)}{odds(A) + 1}\n\\]\nSome probabilities and their associated odds:\n\n\n\n\\(P(A)=\\)\n\\(Odds(A)=\\frac{P(A)}{1-P(A)}\\)\n\n\n\n\n0.001\n0.001/0.999=0.001001\n\n\n0.05\n0.05/0.95=0.0526\n\n\n0.2\n0.2/0.4=0.25\n\n\n0.5\n0.5/0.5=1\n\n\n0.8\n0.8/0.2=4\n\n\n0.95\n0.95/0.05=19\n\n\n0.999\n0.999/0.001=999\n\n\n\nThink of odds(A) as â€œhow many times will A occur for every time A does not occur?â€\nSometimes we add â€œto 1â€ to an odds statement, e.g.Â â€œodds of 4 to 1â€ means â€œthis outcomes occurs 4 times for every 1 time it does not occur.â€\n\n\n6.2.2 Back to the logistic regression model\nThe response variable for logistic regression, again, is:\n\\[\nlogit(\\pi) = ln(\\frac{\\pi}{1-\\pi}) = \\textit{\"log odds\"}\n\\]\nSo, the full logistic regression model is :\n\\[\nY_i\\sim Bernoulli (\\pi_i)\n\\\\\nln(\\frac{\\pi_i}{1-\\pi_i})=\\beta_0+\\beta_1x_{1i} + \\beta_2x_{2i} + \\dots + \\beta_px_{pi}\n\\]\nWe are not actually interested in log odds; we only make this conversion for mathematical convenience. So, once we have \\(logit(\\hat{\\pi}_i)\\), we can \\(\\hat{\\pi}_i\\) back via the inverse logit function, \\(logit^{-1}(x)=\\frac{e^x}{1+e^x}\\):\n\\[\n\\begin{align}\nlogit^{-1}(logit(\\pi)) \\\\\n&= \\frac{e^{logit(\\pi)}}{1 + e^{logit(\\pi)}} \\\\\n&= \\frac{odds}{1 + odds} \\\\\n&= \\frac{\\frac{\\pi}{1-\\pi}}{\\frac{1-\\pi}{1 - \\pi} + \\frac{\\pi}{1-\\pi}} \\\\\n&= \\frac{\\frac{\\pi}{1-\\pi}}{\\frac{1}{1-\\pi}} \\\\\n&= \\pi\n\\end{align}\n\\]\nIn the context of the logistic regression model:\n\\[\n\\begin{align}\n\\hat{\\pi}_i & =logit^{-1}(logit(\\hat{\\pi_i}))\\\\\n&=\\frac{e^{logit(\\hat{\\pi_i})}}{1 + e^{logit(\\hat{\\pi_i})}} \\\\\n&=\\frac{e^{\\hat{\\beta}_0 + \\hat{\\beta}_1x_{1i} + \\hat{\\beta}_2x_{2i} + \\dots + \\hat{\\beta}_px_{pi}}}{1 + e^{\\hat{\\beta}_0 + \\hat{\\beta}_1x_{1i} + \\hat{\\beta}_2x_{2i} + \\dots + \\hat{\\beta}_px_{pi}}}\n\\end{align}\n\\]\n\n\n6.2.3 jamovi example\nApplying this to the harassment data, we use Linear Models / Generalized Linear Models in jamovi and select Logistic under Categorical dependent variable.\n\n\n\n\n\nNote that â€œTarget Levelâ€ defaults to zero. Changing it to 1 makes sense in this case; we want to predict ğ‘ƒ(ğ‘…ğ‘’ğ‘ğ‘œğ‘Ÿğ‘¡ğ‘’ğ‘‘).\njamovi also produces a Loglikelihood ratio test, but we will just focus on â€œParameter Estimatesâ€:\n\n\n\n\n\nHere is our estimated model:\n\\[\n\\textit{log odds (Reported)} = -1.7976 + 0.4869*OffensBeh\n\\]\nPlugging in large and small values for OffensBeh:\n\\[\n\\textit{log odds (Reported)} = -1.7976 + 0.4869*1 = -1.3107\n\\] \\[\n\\textit{log odds (Reported)} = -1.7976 + 0.4869*8 = 2.0976\n\\]\n\\[\n\\hat{\\pi}|OffensBeh = 1: \\\\ \\frac{e^{-1.3107}}{1 + e^{-1.3107}} = \\frac{0.2696}{1.2696} = 0.21\n\\] \\[\n\\hat{\\pi}|OffensBeh = 8: \\\\ \\frac{e^{-2.0976}}{1 + e^{-2.0976}} = \\frac{8.1466}{9.1466} = 0.89\n\\]"
  },
  {
    "objectID": "Ch6_GLMs.html#interpreting-slope-in-logistic-regression",
    "href": "Ch6_GLMs.html#interpreting-slope-in-logistic-regression",
    "title": "6Â  Generalized Linear Models (GLMs)",
    "section": "6.3 Interpreting slope in logistic regression",
    "text": "6.3 Interpreting slope in logistic regression\nThe slope coefficient is directly interpreted as change in log odds for a one unit increase in the predictor. â€œLog oddsâ€ are not of direct interest.\nExponentiating both sides of the equation gives straight odds\n\\[\nodds = (\\frac{\\pi_i}{1-\\pi_i})=e^{\\beta_0+\\beta_1x_{1i} + \\beta_2x_{2i} + \\dots + \\beta_px_{pi}}\n\\]\nThis means that, for a one unit increase in \\(X_1\\), odds are multiplied by \\(e^\\beta_1\\)\nFor the harassment data:\n\\[\n\\textit{log odds (Reported)} = -1.7976 + 0.4869*OffensBeh\n\\]\n\\(\\hat{\\beta}_1 = 0.4869\\). So, for a one unit increase in OffensBeh, predicted odds of reporting are multiplied by \\(e^{0.4869} = 1.627\\)\nIn other words, there is about a 63% increase in odds of reporting when OffensBeh increases by one. NOTE: odds are not probabilities!\nComparing probabilities and odds from this model:"
  },
  {
    "objectID": "Ch6_GLMs.html#poisson-regression",
    "href": "Ch6_GLMs.html#poisson-regression",
    "title": "6Â  Generalized Linear Models (GLMs)",
    "section": "6.4 Poisson regression",
    "text": "6.4 Poisson regression\nWeâ€™ll now look at two other popular GLMs: Poisson (â€œpwa-sawnâ€ roughly) and negative binomial.\nThese are used for modeling count data, which can be extended to how often a categorical variable takes on some value. Thus Poisson regression can be used to model contingency table data.\n\n6.4.1 The Poisson distribution\nThe Poisson distribution is a discrete probability distribution. A Poisson distributed variable takes on only positive integer values. The integer is referred to as â€œcountâ€ or â€œ# of eventsâ€.\nThe Poisson distribution has a single parameter, \\(\\lambda\\) (â€œlambdaâ€), which is sometimes called the â€œrateâ€ parameter.\n\\(\\lambda\\) is both the mean and the variance of a Poisson distribution\nThe probability function for the Poisson is:\n\\[\nP(count = k) = \\frac{\\lambda^k}{e^\\lambda k!}\n\\]\n\n\n6.4.2 Visualizing the Poisson distribution"
  },
  {
    "objectID": "Ch6_GLMs.html#the-structure-of-a-glm",
    "href": "Ch6_GLMs.html#the-structure-of-a-glm",
    "title": "6Â  Generalized Linear Models (GLMs)",
    "section": "6.5 The structure of a GLM",
    "text": "6.5 The structure of a GLM\n\n6.5.1 The link function\nIn logistic regression, the response variable was $logit(\\pi) = ln(\\frac{\\pi}{1-\\pi}) $\nIn Poisson regression, the response variable is \\(ln(\\lambda)\\)\nThe reasoning will be that the natural log allows the estimated rate to be modeled as a linear function of some predictor variables.\nThis is how GLMs work: they allow us to use non-normal response variables by expressing a function of their mean as a linear function of the predictors.\nA GLM has three parts:\n\nA response variable with some distribution\nA â€œlink functionâ€, \\(g(\\cdot)\\), that is applied to the mean of the response variable.\nA linear expression of the predictor variables: \\(\\beta_0 + \\beta_1X_1 + \\beta_2X_2 \\dots\\)\n\n\n\n6.5.2 GLM examples\nLogistic regression uses \\(Y\\sim Bernoulli (\\pi)\\) as the response variable and \\(g(\\pi) = ln(\\frac{\\pi}{1-\\pi})\\) as the link function.\nPoisson regression uses \\(Y\\sim Poisson(\\lambda)\\) as the response variable and \\(g(\\lambda) = ln(\\lambda)\\) as the link function.\nOrdinary least squares (OLS) regression can also be considered a special case of a GLM. It uses \\(Y\\sim Normal (\\mu, \\sigma^2)\\) as the response variable and the identity function, \\(g(\\mu) = \\mu\\) as the link."
  },
  {
    "objectID": "Ch6_GLMs.html#maximum-likelihood-estimation-conceptually",
    "href": "Ch6_GLMs.html#maximum-likelihood-estimation-conceptually",
    "title": "6Â  Generalized Linear Models (GLMs)",
    "section": "6.6 Maximum likelihood estimation, conceptually",
    "text": "6.6 Maximum likelihood estimation, conceptually\nItâ€™s worth briefly noting that the mathematical method used to come up with parameter estimates for GLMs is not â€œleast squaresâ€. So, we are not getting our \\(\\beta\\)â€™s by minimizing sums of squared residuals.\nInstead, the estimation procedure we use is called â€œmaximum likelihoodâ€. This method finds the values of the parameter estimates that maximize (i.e.Â make as large as possible for a given set of data) something called â€œthe likelihood functionâ€. The likelihood function takes a fixed set of data and an assumed distribution (e.g.Â normal), and gives the â€œprobability of the dataâ€, given some set of parameter values.\nSo, the coefficient estimates that we get in GLM output would make our data â€œmore likelyâ€ than our data would be under any other set of possible estimates. They are the estimates that maximize the likelihood of our data."
  },
  {
    "objectID": "Ch6_GLMs.html#poisson-regression-example",
    "href": "Ch6_GLMs.html#poisson-regression-example",
    "title": "6Â  Generalized Linear Models (GLMs)",
    "section": "6.7 Poisson regression example",
    "text": "6.7 Poisson regression example\nWeâ€™ll use some General Social Survey data for this example.\nPoisson is good for modeling count data, so weâ€™ll use a response variable that takes the form of counts.\nFor this example, the goal will be to look at the relationship (if any) between the number of sibling a person has, and the number of children that person has.\nOur question will be: do people with more siblings tend to have more children? And if so, can we quantify the relationship?\nIt would be wise to collect data on covariates that we expect will also be related to the number of children someone has.\nAn obvious one is age. Older people will have more children than younger people.\nWe might also want to control for â€œcultureâ€. If people from different cultural backgrounds tend to have more or fewer children, then this would definitely induce a relationship between # of siblings and # of children.\nThere are lots of possible ways to try account for cultural background. Iâ€™m choosing rate of attending religious services.\n\nSo, the variables will be:\n\n# of children\n# of siblings\nAge\nFrequency of attending religious services\n\n\nWeâ€™ll just look at 2018 data. The GSS lets us choose any years we want, going back to 1972.\nThis data set is on Canvas, as GSS_Children_Siblings.jmp\n\n6.7.1 Poisson regression EDA\nFirst thing to do is plot our variables.\n\n\n\n\n\nYikes! Thereâ€™s some cleaning to do. The instances of 98 siblings are not real data points.\nGSS data explorer website lets us look in detail at each variable. Here is part of the coding for â€œSIBSâ€:\n\n\n\n\n\nAnd hereâ€™s the coding for the variable CHILDS.\n\n\n\n\n\nSo, CHILDS = 9 is a value for missing data. These should also be excluded.\nThis should feel familiar â€“ remember how messy the NLSY data was in the heights analysis?\nKeep in mind that data in public databases often have idiosyncrasies like this.\nHere are the row selection options that will select all rows with invalid responses.\n\n\n\n\n\nOnce selected, they can be excluded.\nHere is the distribution of CHILDS.\n\n\n\n\n\nThis looks a lot like a Poisson distribution. Hooray!\nTo run the regression, use Linear Models / Generalized Linear Models and choose Poisson(overdispersion) for Frequencies.\n\n\n\n\n\nJMP will automatically choose the log link\nSelect the â€œOverdispersion tests and intervalsâ€ box\nHere are results for a simple model, where # of siblings is the sole predictor of # of children. Weâ€™ll just look at the parameter estimates and the overdispersion statistic:\n\n\n\n\n\nLetting \\(\\hat{\\lambda}\\) represent the predicted mean # of children, we have:\n\\[\nln(\\hat{\\lambda}) = 0.375 + 0.0631(ğ‘†ğ¼ğµğ‘†)\n\\]\n\n\n6.7.2 Interpreting the slope\nYou might not be surprised to learn that, due to the log link, the exponentiated slope is interpreted as the multiplicative change in the estimated value of the response variable, given a one unit increase in the predictor variable: \\(e^{0.0627} = 1.065\\)\n\nSo, increasing # of siblings by one is associated with an 6.5% increase in # of children.\nWe can see that this is statistically significant, but it is also small.\nIt is also not obvious that % change is the best way to quantify this. Maybe an OLS model would have been more interpretable.\nIt might be more desirable to relate an additive change in siblings to an additive change in children.\nDownside is that # of children is not normally distributed.\nAs is often the case, we are trading some interpretability for a better fitting model.\n\n\n6.7.3 Overdispersion\nThe Poisson distribution makes a strong assumption: the mean should be equal to the variance.\nOften, we observe real data in which the variance is greater than the mean.\nThis is referred to as â€œoverdispersionâ€.\nJamovi reports an overdispersion estimate:\n\n\n\n\n\nThe overdispersion statistic is the ratio of the Pearson chi-square statistic to its degrees of freedom.\nIf mean = variance, this should be equal to 1. But it rarely is. If there is strong overdispersion, a negative binomial model should fit better.\n\n\n6.7.4 Fitting a larger model\nFor these data, it turns out that # of siblings, age, frequency of attending religious services, and the interactions between age and the other two variables are all statistically significant and all improve model fit. Here are the parameter estimate results for this model:\n\n\n\n\n\nThe other predictor variables and the interactions can be interpreted in the usual ways.\nOne thing to notice is that the estimate for SIBS has not changed much. So, while the other covariates and interactions matter, they donâ€™t substantially change our interpretation of the SIBS predictor."
  },
  {
    "objectID": "Ch6_GLMs.html#negative-binomial-regression",
    "href": "Ch6_GLMs.html#negative-binomial-regression",
    "title": "6Â  Generalized Linear Models (GLMs)",
    "section": "6.8 Negative binomial regression",
    "text": "6.8 Negative binomial regression\nThere is also still some overdispersion, though less than there was before:\n\n\n\n\n\nRemember that the Poisson distribution only has one parameter. This limits its flexibility.\nThe negative binomial distribution is similar to the Poisson distribution, but it is more flexible, and may be a better choice in the presence of overdispersion.\nThe negative binomial distribution is also a distribution for count data. It is interpreted as giving the number of â€œsuccessâ€ before a certain number of â€œfailuresâ€ occur.\nThere are two parameters: \\(p\\), the probability of success, and \\(r\\), the number of failures at which counting stops.\nIf a variable \\(Y\\) is distributed negative binomial, we denote it:\n\\[\nY\\sim NB(r,p)\n\\]\nThe mean of the negative binomial distribution is \\(\\frac{rp}{1-p} = r*odds(success)\\)\nExample: suppose \\(p = 0.8\\) and \\(r = 2\\). We expect \\(\\frac{2âˆ—0.8}{0.2} = 8\\) success before we observe two failures.\nOr suppose \\(p= 0.5\\) and \\(r= 1\\). We expect \\(\\frac{1âˆ—0.5}{0.5} = 1\\) success before we observe 1 failure.\nFor practical purposes, negative binomial regression will show better fit than Poisson regression in the presence of overdispersion.\nThe tradeoff is that the interpretation is less generally applicable. It might not make sense to think of your count variable as # of successes for a certain # of failures.\nAs with Poisson regression, negative binomial regression uses a GLM with a log link.\n\n6.8.1 Negative binomial example\nHere is where you select negative binomial regression in jamovi:\n\n\n\n\n\nUnder â€œGeneralized Linear Modelsâ€, under â€œFrequenciesâ€ choose â€œNegative Binomialâ€.\n\n\n6.8.2 Negative binomial results\n\n\n\n\n\nThese results are awfully similar to the Poisson results.\nIt is often the case that different statistical methods designed for the same purpose will with similar results."
  },
  {
    "objectID": "Ch7_Mixed.html#outline-of-notes",
    "href": "Ch7_Mixed.html#outline-of-notes",
    "title": "7Â  Chapter 7: Mixed-effects models",
    "section": "Outline of notes",
    "text": "Outline of notes\n\nRepeated measures\nThe independence assumption\nCheating with repeated measures\nCheating ourselves with repeated measures\nChallenges and opportunities with repeated measures data\nRandom effects\nThe mixed effects model\nApplied example: â€œThe liking gapâ€\nReplicating a published mixed model analysis"
  },
  {
    "objectID": "Ch7_Mixed.html#repeated-measures",
    "href": "Ch7_Mixed.html#repeated-measures",
    "title": "7Â  Chapter 7: Mixed-effects models",
    "section": "7.1 Repeated measures",
    "text": "7.1 Repeated measures\nâ€œRepeated measuresâ€ refers to measuring the same subjects (people, trees, dogs, cities, cells, widgets, etc) more than once.\nStudies that use repeated measures are often referred to as â€œwithin subjectsâ€ studies. The idea is that multiple measurements within the same subject will be compared.\nMixed models are popular tools for analyzing repeated measures data; weâ€™ll get to these later in the notes. For now, weâ€™ll consider the problems and opportunities that arise from the use of repeated measures data."
  },
  {
    "objectID": "Ch7_Mixed.html#the-independence-assumption",
    "href": "Ch7_Mixed.html#the-independence-assumption",
    "title": "7Â  Chapter 7: Mixed-effects models",
    "section": "7.2 The independence assumption",
    "text": "7.2 The independence assumption\nWe have looked at model assumptions in this class, e.g.\n\\[\n\\epsilon_i \\sim Normal(0, \\sigma)\n\\]\nThere is one that has been left out. The complete way to write this statement is:\n\\[\n\\epsilon_i \\overset{iid}\\sim Normal(0, \\sigma)\n\\]\nThe â€œiidâ€ means â€œindependent and identically distributed.\n\\(\\epsilon_i \\overset{iid}\\sim Normal(0, \\sigma)\\) says that \\(\\epsilon_i\\) takes on values that are independently sampled from the same normal distribution.\nâ€œindependently sampledâ€ means that the value of the next data point is not dependent upon the value of the previous data point.\nEvery method we have used assumes independence of the response variable, conditional on the predictor(s).\nBut, if the same subjects are measured multiple times, then the multiple observations on each subject will not be independent.\nExample: suppose we take a random sample of 10 people, have each solve a maze, and record how much time it takes each person to solve it. Call this time \\(T\\).\nWe might model the times as i.i.d. normal:\n\\[\nT_i \\overset{iid}\\sim Normal( \\mu, \\sigma)\n\\]\nThis means that, given a mean and standard deviation, each observed time \\(T_i\\) is uncorrelated with any previous times. This should make sense: there is no reason that the \\(3^{rd}\\) personâ€™s maze time should depend upon the \\(2^{nd}\\) personâ€™s maze time.\nBut, if we take repeated measures on each subject, then our data will not be independent.\nFor instance, suppose we have 10 observations on maze completion from 2 people, where the first 5 come from one person and the last 5 come from the other.\nThen I would expect correlation (non-independence) between the first 5 maze times, and between the last 5 times. Observation 5 should be closer to observations 1-4 than it is to observations 6-10."
  },
  {
    "objectID": "Ch7_Mixed.html#cheating-with-repeated-measures",
    "href": "Ch7_Mixed.html#cheating-with-repeated-measures",
    "title": "7Â  Chapter 7: Mixed-effects models",
    "section": "7.3 Cheating with repeated measures",
    "text": "7.3 Cheating with repeated measures\nWe will look at a fake data simulation of repeated measures data.\nIn this simulation, suppose we have cows infected with the Staph bacteria.\nWe are comparing two treatments for Staph. We have 6 cows, and each treatment is randomly assigned to 3 cows. We will do a t-test to compare mean infection levels (quantified on some arbitrary scale).\nWe will also assume that the two treatments have an identical effect. Therefore the null hypothesis of no difference in means is true.\nThe file cows_ttest.jmp has some fake data, simulated as \\(y_i \\sim Normal(12,4)\\) for both groups:\n\n\n\n\n\n\n\n\n\n\nNot surprisingly, the t-test gives no significant difference in means:\n\\[\n95\\% \\textit{ CI for } \\mu_1 - \\mu_2 = (-18.00, 8.07), \\text{ p-value} = 0.321\n\\]\n\n\n\n\n\nNow suppose we measure infection level for each cow seven times after receiving treatment.\nThe file cows_repeated_ttest.csv contains data simulating this scenario.\n\n\n\n\n\nFor each cow, there are 7 observations. These all average out to the one observation per cow from the last data set.\n\n\n\n\n\nBut check out these t-test results!\n\n\n\n\n\nNotice where it says degrees of freedom = 29.2. If your degrees of freedom exceeds your sample size, something is wrong.\n\n7.3.1 What happened?\nComparing the two t-tests:\n\n\n\n\n\n\n\n\n\n\nThe mean difference is nearly the same for both.\nThe standard error for the repeated measures data is much smaller.\nRemember that the t-test is just a simple regression model:\n\\[\ny_i = \\beta_0 + \\beta_1Group_i + \\epsilon_i \\text{ where } \\epsilon_i \\overset{iid}\\sim Normal(0, \\sigma)\n\\]\nIn this case, \\(\\hat{\\beta}_1\\) is the estimated mean difference in infection levels between the two groups.\nThereâ€™s nothing wrong with this estimate on its own. However its standard error is computing using an incorrect assumption: that all of the observations within each group are independent.\n\n\n7.3.2 Effective sample size\nIn this case, the apparent sample size is \\(n = 42\\), or \\(n = 21\\) per group.\nBut the â€œeffectiveâ€ sample size is \\(n = 6\\), or \\(n = 3\\) per group.\nTaking lots of observations from each animal and doing a t-test on all the data amounts to falsely inflating the sample size without having to actually collect more data.\nMain lesson: observations that are correlated should not be treated as independent!"
  },
  {
    "objectID": "Ch7_Mixed.html#cheating-ourselves-with-repeated-measures",
    "href": "Ch7_Mixed.html#cheating-ourselves-with-repeated-measures",
    "title": "7Â  Chapter 7: Mixed-effects models",
    "section": "7.4 Cheating ourselves with repeated measures",
    "text": "7.4 Cheating ourselves with repeated measures\nTreating repeated measures as independent can fool us into thinking weâ€™ve discovered an association when really there is none.\nIt can also do the opposite: it can fool us into thinking we see no association when really there is one.\nThis will be illustrated with another toy example. In this case, we will again consider collecting data on infection level in cows under two treatments. This time, weâ€™ll imagine collecting repeated observations over the course of a week.\nHere are the data in â€œwide formâ€. Treatment group B is highlighted to distinguish it from treatment group A.\n\n\n\n\n\n\nTwo things to note:\n\nThere is a lot of variability across cows.\nEvery cowâ€™s infection rating goes down over time\n\n\n\n\n\n\n\nHere are the data in â€œlong form.â€\n\n\n\n\n\nLong form is needed for most analyses.\nWe can use jamoviâ€™s Rj-code editor to wide form to long form. (Though using Excel might be easier)\nThe following code will convert the data from wide to long form. Note you will need to open the data in a new session of jamovi and rename the Day and Infection_level columns.\n\n\n\n\n\n\n7.4.1 Converting from wide to long form\nWe must now transform the Day column of the longform data. We want the variable Day to take on nominal integer values\nDouble click the Day column and in the Data menu select Transform / Using Transform / Create New Transformation.\n\n\n\n\n\n\n\n7.4.2 Plotting the data\nHere is a basic plot of infection rates across time.\n\n\n\n\n\nThere appears to be a small downward trend, and lots of noise in the data.\nBut we can account for this noise! It is due to different cows having different overall infection rates.\nHere is a similar plot, with the Y axis grouped by treatment.\n\n\n\n\n\nThis shows a small decrease for treatment B.\nBut, we are still treating these observations as though they are independent.\nThey are not independent; they are correlated within each cow.\nHere is the same plot, but with â€œcow_IDâ€ as the overlay variable.\n\n\n\n\n\nEach cow now gets its own line. The downward trend for B is clear.\nAlso, it is now clear that most of the variation in infection rate was due to differences between cows. Once this is accounted for, variability is low.\n\n\n\n\n\n\n\n7.4.3 Analyzing the data\nThese plots illustrate the idea behind accounting for repeated measures.\nIn this module, we will learn how to incorporate repeated measures in statistical models.\nFor now, letâ€™s look at some different way of analyzing the data that we just plotted.\n\n\n7.4.4 Analyzing the data, assuming independence\nWe can run regression models where infection rate is the response variable and some combination of treatment and day are the predictors.\nWe know both variables matter, and that they interact â€“ the â€œeffectâ€ of day on infection level depends on treatment, and the â€œeffectâ€ of treatment on infection level depends on day.\nThe next slide shows output for fitting this model:\n\\[\n\\textit{Infection level}_i = \\beta_0 + \\beta_1Treatment + \\beta_2Day + \\beta_3Treatment*Day + \\epsilon_i \\\\\n\\text{ where } \\epsilon_i \\overset{iid}\\sim Normal(0, \\sigma)\n\\]\n\n\n\n\n\n\\(R^2\\) is small, suggesting this model does not explain much variability in infection rates.\nRemember, we know that a lot of variability is due to differences between cows. This is all getting â€œabsorbedâ€ by the error term.\nLarge error variance gives small \\(R^2\\) and large standard errors for slopes\n\n\n7.4.5 Analyzing the data, accounting for cows\nHere are results from analyzing these data using a mixed model. This mixed model accounts for differences between cows, and the fact that repeated measures taken on each cow are correlated.\n\n\n\n\n\nWe will cover the details of how mixed models work in the next set of notes. For now, note the much larger \\(R^2 = 0.83\\)\n\n\n\n\n\n\n\n7.4.6 Side by side comparison\nThe mixed model (on the bottom) gives smaller standard errors for Day and the interaction.\n\nNote what didnâ€™t change: the parameter estimates!"
  },
  {
    "objectID": "Ch7_Mixed.html#challenges-and-opportunities-with-repeated-measures-data",
    "href": "Ch7_Mixed.html#challenges-and-opportunities-with-repeated-measures-data",
    "title": "7Â  Chapter 7: Mixed-effects models",
    "section": "7.5 Challenges and opportunities with repeated measures data",
    "text": "7.5 Challenges and opportunities with repeated measures data\nRepeated measures data can be challenging to analyze, if the design gets complicated. We will see some study designs in which it isnâ€™t immediately obvious how to set up an appropriate model.\nRepeated measures data can also be analyzed incorrectly (assuming independence) to artificially increase apparent sample size and get strong looking results that are not valid.\nBut repeated measures data can also be very useful! â€œWithin subjectâ€ designs, in which subjects are measured repeatedly across time and / or conditions, can greatly enhance the precision and power of our inferences. This is because variability that would normally be accounted for by the error term can instead be attributed to overall differences between the subjects from whom repeated measurements were taken."
  },
  {
    "objectID": "Ch7_Mixed.html#random-effects",
    "href": "Ch7_Mixed.html#random-effects",
    "title": "7Â  Chapter 7: Mixed-effects models",
    "section": "7.6 Random effects",
    "text": "7.6 Random effects\nThe â€œmixedâ€ in â€œmixed modelsâ€ refers to a mix of random effects and fixed effects.\nAll of the predictor variables weâ€™ve seen all semester have been â€œfixed effectsâ€. What this means will only make sense in comparison to a new kind of predictor: a â€œrandom effectâ€.\nFixed effects variables are predictor variables for which we â€œestimateâ€ a slope coefficient, usually denoted with a \\(\\beta\\). We calculate a value for this coefficient using our data, and then we treat this value (usually denoted as a \\(\\hat{\\beta}\\)) as an estimate for some fixed but unknown population-level parameter.\nSometimes, though, we want our model to account for a variable that is important for explaining variability in the response variable, but for which we do not want to treat its coefficients as estimates of unknown parameters. For instance, in the previous â€œcowsâ€ examples, weâ€™ll want to calculate the mean infection levels for each cow and take these into account. But, it probably doesnâ€™t make sense to treat each cowâ€™s mean as a population level parameter value of interest. After all, if we conducted the study again, weâ€™d have new cows!\nTreating this predictor variable as a random effect (or random factor) will accomplish this. Random factors take on values that are treated as having been drawn at random from a larger population of possible values that might be different if we take a new sample. These random factors have coefficients (aka slopes) that are also treated as taking on random values.\nSo, with the â€œcowsâ€ examples, we imagined measuring the same cows over and over again. â€œCowâ€ should probably be treated as a random factor in these cases. The cows themselves were drawn from a larger population; we would not get the same cows again in a new study.\nAlso, we wanted to account for differences in the mean infection levels for each individual cow, so that we could estimate standard errors appropriate to our study designs. And these mean infection levels should also be treated as random, in that they came from some population of possible mean infection levels.\nSo, the cows are random, and their coefficients are random.\nThere are many interpretations of a â€œrandom effectâ€, and they arenâ€™t always helpful. From a 2005 paper by Andrew Gelman:\n\nFixed effects are constant across individuals, and random effects vary.\n\n\n\nEffects are fixed if they are interesting in themselves or random if there is interest in the underlying population.\nWhen a sample exhausts the population, the corresponding variable is fixed; when the sample is a small (i.e., negligible) part of the population the corresponding variable is randomâ€.\nâ€œIf an effect is assumed to be a realized value of a random variable, it is called a random effect.â€\n\nSo, interpretations of what random vs.Â fixed effects â€œreally meanâ€ will vary.\nBut, formally, it isnâ€™t so ambiguous. â€œFixedâ€ effects are treated as having â€œfixedâ€ coefficients whose values we estimate and draw inference on (e.g.Â with confidence intervals and hypothesis tests).\nâ€œRandomâ€ effects are treated as having â€œrandomâ€ coefficients drawn from some distribution. We wonâ€™t estimate individual coefficients, but we will estimate the variance of the distribution from which they came.\n\n7.6.1 Subjects and Nesting\nOur main motivation right now is to have a method of accounting for repeated measurements on the same subjects.\nSo, in this class, we will look at mixed models for which the random effect is â€œsubjectâ€. In other words, we will make models that account for differences between subjects measured multiple times.\nIn a study in which subjects are assigned to groups, each subject is assigned to one group.\nFrom a modeling perspective, subject is â€œnestedâ€ within group. In general, one variable is nested within the other if values of the nested variable only occur in certain categories of the variable it is nested within.\nIn this case, cows are nesting within treatments because each cow is measures only in one treatment. A non-nested design would have each cow measured under each value of the other predictors. In other words, each cow would be measured under both treatment.\nAnother example: say we are doing an education research study and we randomly sample 4 districts in a state, then 3 schools in each district, then 7 classrooms in each school. In this case, classroom is nested within school, and school is nested within district. This is because each classroom exists in only one school, and each school exists in only one district.\nThe notation for â€œsubject nested within groupâ€ is \\(subject(group)\\)"
  },
  {
    "objectID": "Ch7_Mixed.html#the-mixed-effects-model",
    "href": "Ch7_Mixed.html#the-mixed-effects-model",
    "title": "7Â  Chapter 7: Mixed-effects models",
    "section": "7.7 The mixed effects model",
    "text": "7.7 The mixed effects model\nA mixed effects model contains both random and fixed effects.\nApplying this to the cows example, in which cows are assigned to one of two treatment groups and measured daily for seven days:\n\\[\n\\textit{Infection rate}_i = \\beta_0 + \\beta_1Treatment_i + \\beta_2Day_i + \\beta_3Treatment_i*Day_i + \\alpha_jCow_j(Treatment_i) + \\epsilon_i \\\\\n\\text{ where } \\alpha_j \\sim Normal(0, \\sigma^2_\\alpha) \\\\\n\\text{ and } \\epsilon_i \\sim Normal(0, \\sigma^2_\\epsilon)\n\\]\nDonâ€™t worry too much about the notation details. The most important part is that â€œCowâ€ is random, while â€œTreatmentâ€ and â€œDayâ€ are fixed.\nWe will not estimate coefficients for the different cows. This model treats each cow as having its own â€œrandom interceptâ€, meaning that the intercept, \\(\\beta_0\\), gets adjusted by some amount for each individual cow.\nAs we saw in the last set of notes, the purpose of this is to let the model estimate the effects of the â€œTreatmentâ€ and â€œDayâ€ variables, while taking accounts of the fact that different cows will have different overall mean infection rates.\n\n7.7.1 The mixed effects model in jamovi\nTo fit this model in jamovi, use â€œLinear Modelsâ€, select â€œMixed Modelâ€, and then add the fixed predictors as factors and covariates and random predictors as cluster variables:\n\n\n\n\n\nNote that Cow_ID is the random effect. Treatment, Day, and their interaction are fixed effects.\nNote also jamovi will also require you to specify the random effects.\n\n\n\n\n\njamovi gives the usual parameter estimates output, as well as a residual plot:\n\n\n\n\n\nNotice that â€œCow_IDâ€ is not listed under parameter estimates; only fixed coefficients are estimated.\nEach cow has its own random coefficient, modeled as having been drawn from a normal distribution.\n\n\n\n\n\n\n\n7.7.2 Slope coding in â€œMixed Modelâ€\nNotice the categorical predictor â€œTreatmentâ€ lists â€œEffectâ€ as â€œA â€“ (B, A)â€. jamovi is coding this slope as the difference between Treatment A and the â€œmeanâ€ of Treatments A and B.\nSo, if we were looking at Treatment B, weâ€™d apply a slope of -0.513."
  },
  {
    "objectID": "Ch7_Mixed.html#applied-example-the-liking-gap",
    "href": "Ch7_Mixed.html#applied-example-the-liking-gap",
    "title": "7Â  Chapter 7: Mixed-effects models",
    "section": "7.8 Applied example: â€œThe liking gapâ€",
    "text": "7.8 Applied example: â€œThe liking gapâ€\nThe journal Psychological Science published many studies in which data are publicly available.\nThe remaining slides reproduce results from a recent study published in Psychological Science on the difference between how much individuals â€œlikeâ€ other people and how much they perceive others â€œlikeâ€ them.\nThe paper is available at: https://doi.org/10.1177%2F0956797618783714\nThe basic setup is that volunteers were paired up (each pair is called a â€œdyadâ€) and directed to have a conversation for five minutes.\nAfter this, participants rated their partners on some survey questions that the authors take as a measure of liking the other person.\nParticipants also rated how much they thought they were liked.\nThe study is looking for a â€œgapâ€ (i.e.Â difference) between volunteersâ€™ self-perception of how much their partners liked them, and how much their partners actually liked them.\n\n7.8.1 The liking gap example\nFrom the results section of the paper:\n\n\n\n\n\n\n\n\n\n\n\n\n7.8.2 Fitting the model in jamovi\n\n\n\n\n\n\\[\n\\textit{Liking index}_i = \\beta_0 + \\beta_1\\textit{self_other} + \\beta_2Day + \\alpha_jpid_j(did_k) + \\gamma_kdid_k + \\epsilon_i \\\\\n\\text{ where } \\alpha_{j(k)} \\sim Normal(0, \\sigma^2_\\alpha), \\\\\n\\gamma_k \\sim Normal(0, \\sigma^2_\\gamma), \\\\\n\\text{ and } \\epsilon_i \\sim Normal(0, \\sigma^2_\\epsilon)\n\\]\nThe model statement starts looking complicated. We have two different random effects, one nested in the other.\nThe \\(\\alpha\\)â€™s and \\(\\gamma\\)â€™s are modeled as random values.\nWe have two fixed effects: self_other and Day. Their coefficients will be estimated as normal.\n\\[\n\\textit{Liking index}_i = \\beta_0 + \\beta_1\\textit{self_other} + \\alpha_jpid_j(did_k) + \\gamma_kdid_k + \\epsilon_i \\\\\n\\text{ where } \\alpha_{j(k)} \\sim Normal(0, \\sigma^2_\\alpha), \\\\\n\\gamma_k \\sim Normal(0, \\sigma^2_\\gamma), \\\\\n\\text{ and } \\epsilon_i \\sim Normal(0, \\sigma^2_\\epsilon)\n\\]\nThe random effect variances, \\(\\sigma^2_\\alpha\\) and \\(\\sigma^2_\\gamma\\), are estimated in the mixed model. These variances could be interpreted, but we will stick to interpreted the fixed effects in this class.\n\n\n7.8.3 Adding an interactionâ€¦\nThe next section of the paper looks at personality variables as predictors:\n\n\n\n\n\nIn jamovi:\n\n\n\n\n\nWeâ€™ll stop here. The paper goes on through many additional studies, and the data are all available via the supplemental materials link."
  },
  {
    "objectID": "Ch7_Mixed.html#replicating-a-published-mixed-model-analysis",
    "href": "Ch7_Mixed.html#replicating-a-published-mixed-model-analysis",
    "title": "7Â  Chapter 7: Mixed-effects models",
    "section": "7.9 Replicating a published mixed model analysis",
    "text": "7.9 Replicating a published mixed model analysis\nThe remainder of this chapter covers the analysis from a 2017 Psychological Science paper by Goudeau and Croizet, titled:\nâ€œHidden Advantages and Disadvantages of Social Class: How Classroom Settings Reproduce Social Inequality by Staging Unfair Comparisonâ€\nThis paper presents three studies on one topic. These notes cover studies 1 and 3; study 2 is reserved for a homework assignment.\n\n7.9.1 Worked example overview\nThis study was performed in France. The data are available at https://osf.io/rkj7y/ and the data file has a codebook sheet that defines the variables:\n\n\n\n\n\nIn each study, 6th grade students are given a challenging reading comprehension assignment.\nPerformance on the assignment is the dependent variable.\nThe researchers investigate whether studentsâ€™ performance is associated with their awareness of their classmatesâ€™ performance, and whether this association can be moderated by studentsâ€™ awareness of the different levels of preparation given to different students.\n\n\n7.9.2 Paper abstract\n\n\n\n\n\n\n\n7.9.3 Study 1\n\n\n\n\n\n\nThe variables used in this study are:\n\nPerformance (response)\nVisibility condition (fixed effect)\nSocial class (fixed effect)\nVisibility X Social class interaction (fixed effect)\nSchool (random effect)\nClassroom, nested within School (random effect)\n\n\n\n7.9.3.1 Study 1 model\n\\[\n\\textit{PERF}_i = \\beta_0 + \\beta_1X_1 + \\beta_2X_2 + \\beta_3X_1*PCS + \\alpha_{j(k)}CLASSE_{j(k)}(ETABLISSEMENT_k) + \\gamma_kETABLISSEMENT_k + \\epsilon_i \\\\\n\\text{ where } \\alpha_{j} \\sim Normal(0, \\sigma^2_\\alpha), \\\\\n\\gamma_k \\sim Normal(0, \\sigma^2_\\gamma), \\\\\n\\text{ and } \\epsilon_i \\sim Normal(0, \\sigma^2_\\epsilon)\n\\]\nRefer to the codebook for variable definitions\nNote that in this study, subjects (students) are only measured once. The repeated measurements are on classrooms and schools.\nAlso note that PCS compares working class (PCS=1) to upper class (PCS=3.)\n\n\n7.9.3.2 Study 1 in JMP\n\n\n\n\n\nNote that the response variable, PERF, is score on a reading comprehension test, on a scale of 0 to 20 points.\n\n\n7.9.3.3 Notes on Study 1 random effects output\nâ€œThe random effects covariance parameter estimatesâ€ table shows that there is much more residual (error) variance than there is variance across classrooms or across schools.\nOne very odd result: the classroom variance estimate is negative! But variances cannot be negative.\nIf we had deselected â€œunbounded variance estimatesâ€ under Fit Model, this would be zeroed out. It is a strange quirk of maximum likelihood variance estimation that negative estimates are possible. We can ignore this, and treat the classroom variance as just very small.\n\n\n7.9.3.4 Notes on Study 1 fixed effects output\nWe see strong â€œmain effectsâ€ for visibility condition (X1), social class (PCS), and their interaction.\nA general principle for models with strong interactions is that our interpretation should focus on the interactions.\n\nHere, the interaction coefficient is -2.69, and the coefficient for \\(X_1\\) is 1.12.\nFor the Effect of \\(X_1\\) â€œ-1 - 1â€ refers to the â€œdifferences not visibleâ€ condition, in which students do not raise their hands when they have the answer. For the Effect of PCS â€œ3 - 1â€ refers to change in score for upper class students vs.Â working class students. So, the â€œeffectâ€ of visibility seems to apply to working class students but not to upper class students, since â€“2.69 and 1.12 nearly cancel each other out.\n\n\n7.9.3.5 Plotting the fixed effects\nHere is the plot reported in the paper, and the same plot in JMP:\n\n\n\n\n\nThe practice of using a bar plot to show means is common, but flawed. The bars donâ€™t mean anything; all they do is go up to the means.\nHere is what this looks like using boxplots instead.\n\n\n\n\n\nNotice that this plot shows variability in the data, where the bar plot does not.\n\n\n\n7.9.4 Study 3\nIn Study 2, social class is not used. Rather, some students are given better preparation for the reading comprehension test than others.\nSimilar results are found: those with worse preparation perform more poorly when students are told to raise their hands after determining the answer.\nIn Study 3, the authors attempt to â€œundoâ€ this effect by informing the class that some students were given better preparation than others. Half the classrooms are made aware of this; the other half are not.\nThe variables in this study are almost the same as in study 1, with these changes:\n\nX1 is â€œcontextâ€:\n\n-1 = awareness of the disadvantage\n1 = unawareness of the disadvantage\n\nX2 is â€œlevel of familiarityâ€ with the reading material, based on intentional preparation given by the researchers:\n\n-1 = high level\n1 = low level\n\n\n\n7.9.4.1 Study 3 model\n\\[\n\\textit{PERF}_i = \\beta_0 + \\beta_1X_1 + \\beta_2X_2 + \\beta_3X_1*X_2 + \\alpha_{j}CLASSE_{j}+ \\epsilon_i \\\\\n\\text{ where } \\alpha_{j} \\sim Normal(0, \\sigma^2_\\alpha), \\\\\n\\text{ and } \\epsilon_i \\sim Normal(0, \\sigma^2_\\epsilon)\n\\]\nIn this data set, there is no variable for â€œschoolâ€, only one for â€œclassroomâ€. This isnâ€™t explained in the paper.\n\n\n7.9.4.2 Study 3 in jamovi\n\n\n\n\n\n\n\n\n\n\nAs with before, the variance across classrooms is very small relative to the error variance.\nThe largest overall effect is \\(X_2\\): level of familiarity with the material. Students who are better prepared score higher.\nThe negative interaction shows that the difference in performance between those with higher vs.Â low familiarity is lower when students are aware of the difference in familiarity.\nThe interaction of -5.81 just about cancels out the estimated slope for \\(X_1\\)[-1] of 4.24.\nThis shows that there is very little difference in scores between students with high preparation who are and are not aware of the advantage.\nIt helps to look at the data.\n\n\n\n\n\nThe generic â€œ\\(X_1\\)â€ and â€œ\\(X_2\\)â€ have been replaced with more meaningful titles.\nAnd here are the bar plots shows in the paper.\n\n\n\n\n\nAgain, the boxplots show more information.\nIn this case, the boxplots reveal a potentially concerned â€œceiling effectâ€: many students earned the maximum possible score. This canâ€™t be seen from the bar plot."
  },
  {
    "objectID": "Ch8_Mind_the_Gap.html#outline-of-notes",
    "href": "Ch8_Mind_the_Gap.html#outline-of-notes",
    "title": "8Â  Chapter 8: Minding the gap between science and statistics",
    "section": "Outline of notes",
    "text": "Outline of notes\n\nThe â€œreplication crisisâ€\nPower\nSome history on â€œNHSTâ€\nWhen â€œp &lt; 0.05â€ isnâ€™t that meaningful\nWhat kind of Type I error are you referring to?\nWhat do people think small p-values mean?\nAssessing bias using meta-analysis\nType â€œMâ€ errors\nShould we throw away non-significant results?\nDebates over statistical significance\nProposed reforms"
  },
  {
    "objectID": "Ch8_Mind_the_Gap.html#the-replication-crisis",
    "href": "Ch8_Mind_the_Gap.html#the-replication-crisis",
    "title": "8Â  Chapter 8: Minding the gap between science and statistics",
    "section": "8.1 The â€œreplication crisisâ€",
    "text": "8.1 The â€œreplication crisisâ€\nâ€œThe replication crisisâ€ refers to a recent realization in many areas of science that previously published results often fail to replicate.\nArguably, this started with Daryl Bemâ€™s 2011 paper â€œFeeling the Futureâ€, published in the Journal of Personality and Social Psychology.\nThis paper used standard statistical tools to show strong evidence for pre-cognition, a.k.a. ESP. Many scientists were bothered by this, because they did not believe in ESP but they did believe in the statistical methods used in this paper!\n\n\n\n\n\n2015â€™s â€œReproducibility Project: Psychologyâ€ by the Center for Open Science found that a large number of published experimental results in top Psychology journals failed to replicate.\nSimilar studies in Cancer Biology, Medicine, Economics, Marketing, and Sports Science have found high rates of non-replication.\nWhy do so many studies fail to replicate? There are many reasons, and statistical analysis plays a prominent role. These notes cover the role of statistics in the replication crisis."
  },
  {
    "objectID": "Ch8_Mind_the_Gap.html#power",
    "href": "Ch8_Mind_the_Gap.html#power",
    "title": "8Â  Chapter 8: Minding the gap between science and statistics",
    "section": "8.2 Power",
    "text": "8.2 Power\nMany of the statistical issues surrounding the replication crisis concern â€œpowerâ€.\nStatistical power is the probability of rejecting a null hypothesis (\\(H_0\\)), in the case that \\(H_0\\) is false.\nIn terms of real-world effects, if power is high, then there is a good chance of â€œdetectingâ€ an effect â€“ i.e.Â of declaring statistical significance.\nIf power is low, then even if a real-world effect exists, the result of a hypothesis test will likely be â€œfail to rejectâ€ \\(H_0\\); i.e.Â non-significance.\n\n8.2.1 Type I and Type II errors\nA â€œType Iâ€ error occurs when a true \\(H_0\\) is rejected. In other words, if we declare a result â€œstatistically significantâ€ even though no real-world effect exists, we are committing a Type I error. This is sometimes called a â€œfalse positiveâ€ outcome.\nA â€œType IIâ€ error occurs when a false \\(H_0\\) is not rejected. In other words, if we declare a result â€œnot significantâ€ even though a real-world effect does exist, we are committing a Type II error. This is sometimes called a â€œfalse negativeâ€ outcome.\nLow statistical power implies a high chance of a false negative.\n\n\n8.2.2 Problem: power is unknown!\nThe â€œtrueâ€ power of a statistical test is almost never known. To calculate power, one must assume the â€œtrueâ€ size of the effect being studied.\nFor instance, power to reject the null hypothesis that a new drug is equally effective as a previous drug depends in part upon how different the two drugs are in effectiveness. But if we knew that, we wouldnâ€™t need to conduct a study!\nThere has been research estimating average statistical power in various research fields, and reason to believe that low power studies are not uncommon. Which means that a lot of these replication failures might be â€œfalse negativesâ€, rather than the original studies being â€œfalse positivesâ€."
  },
  {
    "objectID": "Ch8_Mind_the_Gap.html#some-history-on-nhst",
    "href": "Ch8_Mind_the_Gap.html#some-history-on-nhst",
    "title": "8Â  Chapter 8: Minding the gap between science and statistics",
    "section": "8.3 Some history on â€œNHSTâ€",
    "text": "8.3 Some history on â€œNHSTâ€\nThe form of hypothesis testing used today is sometimes called Null Hypothesis Significance Testing (NHST). It combines what used to be two different methods created by two â€œcampsâ€. The camps disagreed with one another, and did not get along personally.\nRonald Fisher\n\n\n\n\n\nJerzy Neyman and Egon Pearson\n\n\n\n\n\n\n8.3.1 Ronald Fisher\nIntroduced the null hypothesis and p-value\nOn interpreting small p-values: â€œEither an exceptionally rare chance has occurred, or the theory of random distribution is not trueâ€\nOn the use of p &lt; 0.05 as a standard of evidence: â€œIn order to assert that a natural phenomenon is experimentally demonstrable we need, not an isolated record, but a reliable method of procedure. In relation to the test of significance, we may say that a phenomenon is experimentally demonstrable when we know how to conduct an experiment which will rarely fail to give us a statistically significant resultâ€\n\n\n8.3.2 Neyman and Pearson\nIntroduced the alternative hypothesis, Type I and II errors, and power.\nReported only Type I and Type II error probabilities in testing; e.g.Â p-values of 0.001 and 0.04 would both be reported as falling under a pre-specified \\(\\alpha = 0.05\\) Type I error rate. A Type II error rate should also be reported.\nâ€œWe are inclined to think that as far as a particular hypothesis is concerned, no test based upon a theory of probability can by itself provide any valuable evidence of the truth or falsehood of a hypothesisâ€\n\n\n8.3.3 NHST\nNHST is a mix between the two approaches.\nHypothesis testing comes from Neyman and Pearson. They did not believe p-values should be interpreted directly as quantifying evidence. They saw their procedure as simply a method for making a decision.\nThe direct interpretation of p-values as quantifying the probability of obtaining results at least as extreme, assuming the null hypothesis is true, comes from Fisher. He did not use an alternative hypothesis, and he did not accept formal power analysis."
  },
  {
    "objectID": "Ch8_Mind_the_Gap.html#when-p-0.05-isnt-that-meaningful",
    "href": "Ch8_Mind_the_Gap.html#when-p-0.05-isnt-that-meaningful",
    "title": "8Â  Chapter 8: Minding the gap between science and statistics",
    "section": "8.4 When â€œp < 0.05â€ isnâ€™t that meaningful",
    "text": "8.4 When â€œp &lt; 0.05â€ isnâ€™t that meaningful\nWe say results are â€œstatistically significantâ€ when we calculate a p-value less than the \\(\\alpha\\) level of significance, which is commonly 0.05.\nSo, â€œsignificantâ€ results are the kind that would be unlikely to occur by chance, if the null hypothesis were true.\nThere are some issues hereâ€¦\nThe reason small p-values are considered â€œevidenceâ€ for a research hypothesis is that they are supposed to be unlikely to occur by chance alone.\nIf there is flexibility in how to conduct an analysis, then it becomes more likely that small p-values will occur by chance.\nThis has a derogatory name: â€œP-hackingâ€.\nLess derogatory alternatives: â€œResearcher degrees of freedomâ€, â€œThe garden of forking pathsâ€\n\nExamples of flexibility:\n\n-   Trying out different combinations of independent and dependent variables\n\n-   Trying out different models and testing methods\n\n-   Redefining variables (e.g. averaging over different combinations of survey responses, choosing different cut points for placing responses into categories)\n\n-   Discarding / retaining outliers\n\n-   Transforming variables\n\n-   Collecting more data than originally planned\n\n-   Ceasing data collection earlier than originally planned\n\n8.4.1 Capitalizing upon flexibility, so that \\(P(p &lt; 0.05) &gt; 0.05\\)\nFlexible practices are perfectly justifiable in many contexts\nBut â€“ to interpret a p-value as â€œthe probability of obtaining results at least this extreme, assuming the null is trueâ€, there can be no flexibility, unless accounted for using a correction (e.g.Â Bonferroni). Flexibility renders the classical p-value interpretation invalid.\nAnother way of putting it: if you report a p-value as â€œthe probability of obtaining results this extreme, assuming the null is trueâ€, you are implicitly claiming you would have conducted the identical analysis, even if the data had been different in any way.\n\n\n8.4.2 The null could be false due to something you havenâ€™t thought of\nPerhaps â€œ\\(H_0\\) is falseâ€ does not imply that the version of â€œnot \\(H_0\\)â€ you have in mind is true.\nIn observational studies, we must always think about possible confounders. Maybe there is a â€œsignificant associationâ€ between variables for some reason that isnâ€™t being considered.\nIn experimental studies, we must consider all the possible effects of our interventions. Did the experiment produce â€œsignificantâ€ results for a reason we didnâ€™t consider, e.g.Â poor control, biased question wording?\nExample: CSU has found that students who complete their Math and Composition AUCC requirements during their first year earn higher grades and have higher graduation rates than those who do not.\nDoes this suggest that completing these requirements in the first year results in higher grades and a higher chance of graduation?\nCould this â€œstatistically significantâ€ effect be due to confounding factors? Is it even reasonable to suspect that students who complete these classes during their first year should have identical grades and graduation rates as those who donâ€™t? This is what the rejected \\(H_0\\) states. Are we impressed that it is rejected?\n\n\n8.4.3 Do we even think the null could be true?\nâ€œAll we know about the world teaches us that the effects of A and B are always different â€“ in some decimal place â€“ for any A and B. Thus asking â€˜Are the effects different?â€™ is foolishâ€ - John Tukey\nThe null can be used as a â€œStraw Manâ€ that no one really believes. Overturning a straw man null may not be that impressive.\nA good question to ask: â€œwould we expect this null to be true?â€\nIf we are performing an experiment, the null is that the treatment does literally nothing. Is this common?\nIf we are analyzing observational data and testing for â€œsignificant correlationâ€, the null is that there is precisely zero correlation at the population level. Do we think this is possible?\nPsychology/Philosopher/Statistician Paul Meehl called this the â€œcrud factorâ€: the extent to which everything is correlated with everything, at least at some small level.\nBut, there are times when the null is plausible.\nIn manufacturing quality control analysis, the null is that â€œeverything is being produced the normal wayâ€. Defects show up as large deviations from this observed null distribution.\nIn Ronald Fisherâ€™s â€œThe Lady Tasting Teaâ€, the subject of the story claimed she could tell whether milk or tea had been poured into a cup first, simply by tasting the result. The null is that she couldnâ€™t tell. Seems plausible.\nMy personal rule is that I am only interested in p-values when I think the null is plausible.\n\n\n8.4.4 Is there a decision to be made?\nA significance test returns a binary outcome: the results are significant, or they are non-significant.\nBinary outcomes can produce binary thinking: the temptation to think â€œsignificantâ€ means â€œrealâ€ and â€œnon-significantâ€ means â€œdue to chanceâ€.\nIs coming to a binary choice even necessary? Why not just report a point estimate and standard error or 95% CI?\nIf there is an actionable outcome, a binary choice might be necessary.\nExample: deciding whether to continue investing money into a research program.\nIf the analysis is being done for the purpose of decision making, then a decision rule must be established, and â€œsignificanceâ€ can be such a rule. If the analysis is being done for the purpose of conveying information in data, I personally see no reason to add in a declaration of â€œsignificantâ€ or â€œnot significantâ€.\n\n\n8.4.5 Big sample sizes give small p-values\nConsider the t-test statistic:\n\\[\nt = \\frac{\\bar{x}_1 - \\bar{x}_2}{\\textit{std. error of }\\bar{x}_1 - \\bar{x}_2}\n\\]\nAs sample size increases, standard error decreases, the t-test statistic increases, and the p-value decreases.\nSo, the bigger the sample size, the smaller the value of \\(\\bar{x}_1 - \\bar{x}_2\\) that is needed to achieve statistical significance.\nUpshot: if \\(n\\) is larger, very small effects will be â€œsignificantâ€.\n\n\n8.4.6 eBay study example\n\n\n\n\n\nâ€œWomen had a slightly higher percentage of transactions for which positive feedback had been given in the year preceding the current transaction (99.60% for women and 99.58% for men, P &lt; 0.05)â€\n(http://advances.sciencemag.org/content/2/2/e1500599.full)"
  },
  {
    "objectID": "Ch8_Mind_the_Gap.html#big-sample-sizes-give-small-p-values",
    "href": "Ch8_Mind_the_Gap.html#big-sample-sizes-give-small-p-values",
    "title": "8Â  Chapter 8: Minding the gap between science and statistics",
    "section": "8.5 Big sample sizes give small p-values",
    "text": "8.5 Big sample sizes give small p-values\nConsider the t-test statistic:\n\\[\nt = \\frac{\\bar{x}_1 - \\bar{x}_2}{\\textit{std. error of }\\bar{x}_1 - \\bar{x}_2}\n\\]\nAs sample size increases, standard error decreases, the t-test statistic increases, and the p-value decreases.\nSo, the bigger the sample size, the smaller the value of \\(\\bar{x}_1 - \\bar{x}_2\\) that is needed to achieve statistical significance.\nUpshot: if \\(n\\) is larger, very small effects will be â€œsignificantâ€.\n\n8.5.1 eBay study example\n\n\n\n\n\nâ€œWomen had a slightly higher percentage of transactions for which positive feedback had been given in the year preceding the current transaction (99.60% for women and 99.58% for men, P &lt; 0.05)â€\n(http://advances.sciencemag.org/content/2/2/e1500599.full)"
  },
  {
    "objectID": "Ch8_Mind_the_Gap.html#what-kind-of-type-i-error-are-you-referring-to",
    "href": "Ch8_Mind_the_Gap.html#what-kind-of-type-i-error-are-you-referring-to",
    "title": "8Â  Chapter 8: Minding the gap between science and statistics",
    "section": "8.5 What kind of Type I error are you referring to?",
    "text": "8.5 What kind of Type I error are you referring to?\nGoing back to Type I errors, here is the â€œType I error rateâ€:\n\\[\nP(\\textit{Reject } H_0|H_0\\textit{ is true})\n\\]\nWhat if weâ€™re actually interested in the reverse?\n\\[\nP(H_0\\textit{ is true}|\\textit{Reject } H_0)\n\\]\nClassical hypothesis testing controls the probability of rejecting \\(H_0\\), given \\(H_0\\) is false, typically at 5%.\nIt says nothing about the probability \\(H_0\\) is false, given that \\(H_0\\) has been rejected. \\(P(\\textit{reject } H_0|H_0\\textit{ is false}) \\neq P(H_0\\textit{ is false}|\\textit{reject } H_0)\\)\nUpshot: donâ€™t imagine that a Type I error rate of 0.05 implies that only 5% of significant results you see should be Type I errors!"
  },
  {
    "objectID": "Ch8_Mind_the_Gap.html#what-do-people-think-small-p-values-mean",
    "href": "Ch8_Mind_the_Gap.html#what-do-people-think-small-p-values-mean",
    "title": "8Â  Chapter 8: Minding the gap between science and statistics",
    "section": "8.6 What do people think small p-values mean?",
    "text": "8.6 What do people think small p-values mean?\nJacob Cohen:\nWhatâ€™s wrong with NHST? Well, among many other things, it does not tell us what we want to know, and we so much want to know what we want to know that, out of desperation, we nevertheless believe that it does! What we want to know is â€œGiven these data, what is the probability that \\(H_0\\) is true?â€ But as most of us know, what it tells us is â€œGiven that \\(H_0\\) is true, what is the probability of these (or more extreme) data?â€œ - The Earth is Round (p&lt;0.05) (1994)\nâ€œStatistical tests, P values, confidence intervals, and power: a guide to misinterpretationsâ€ (2016) lists a large number of popular misinterpretations. Some highlightsâ€¦\nâ€œThe p-value is the probability that the test hypothesis is true; for example, if a test of the null hypothesis gave P = 0.01, the null hypothesis has only a 1 % chance of being true; if instead it gave P = 0.40, the null hypothesis has a 40 % chance of being true.â€\nâ€œThe p-value for the null hypothesis is the probability that chance alone produced the observed associationâ€\nâ€œA null-hypothesis p-value greater than 0.05 means that no effect was observed, or that absence of an effect was shown or demonstrated.â€\nâ€œStatistical significance is a property of the phenomenon being studied, and thus statistical tests detect significance.â€\nâ€œWhen the same hypothesis is tested in two different populations and the resulting p-values are on opposite sides of 0.05, the results are conflicting.â€\n(note: all of these are wrong)"
  },
  {
    "objectID": "Ch8_Mind_the_Gap.html#assessing-bias-using-meta-analysis",
    "href": "Ch8_Mind_the_Gap.html#assessing-bias-using-meta-analysis",
    "title": "8Â  Chapter 8: Minding the gap between science and statistics",
    "section": "8.7 Assessing bias using meta-analysis",
    "text": "8.7 Assessing bias using meta-analysis\n\n8.7.1 The sampling distribution of the p-value\nIt is intuitive to think that larger p-values are more likely to occur than smaller p-values when the null is true.\nThis is intuitive, but it is false. In most testing scenarios, all p-values are equally likely when the null is true. The distribution of p under \\(H_0\\) is uniform:\n\n\n\n\n\nSimulation: https://csu-statistics.shinyapps.io/visualize_power/\nWhen the null is false, the distribution of p is right skewed. The greater the statistical power, the greater the skew.\nHere is the distribution of p for Cohenâ€™s d = 0.5 and n = 30, implying power \\(\\approx\\) 75%\n\n\n\n\n\nWhen null is false, smaller p-values are more likely than larger ones.\nSome people have the intuition that for weaker effects or lower power, significant p-values should be close to 0.05.\nThis is false; even for low power, very small p-values are more likely than â€œmarginally significantâ€ p-values. Here is the distribution of p for Cohenâ€™s d = 0.2 and n = 30, impying power \\(\\approx\\) 19%\n\n\n\n\n\nUpshot: we should never see lots of p-values just below 0.05, even under low power.\nBut â€“ there are papers in which many studies are performed, all of which produce p-values just below 0.05. There are bodies of published research in which p- values just below 0.05 occur too frequently.\nThis suggests some combination of â€œpublication biasâ€ and â€œp-hackingâ€\nFormal test: â€œP-curveâ€ (www.p-curve.com)\nâ€œP-curveâ€ takes a collection of p-values less than 0.05 and compares them to the uniform distribution expected under \\(H_0\\)\n\n\n\n\n\nIf p-values close to 0.05 occur too frequently, the p-curve is consistent with a true \\(H_0\\), despite all p-values being less than 0.05.\n\n\n8.7.2 What N is required for sufficient power to detect obvious effects?\n\nSimmons, Simonsohn, and Leif conducted an Amazon M-Turk study (n = 697) to estimate required sample size to detect a variety of â€œeffectsâ€ at \\(\\alpha = 0.05\\). Examples:\n\nMen are taller than women (n = 12; i.e.Â n = 6 per group) People who like spicy food eat more Indian food than people who donâ€™t like spicy food (n = 52)\nPeople who like eggs eat more egg salad than people who donâ€™t like eggs (n = 96)\nSmokers think smoking is less likely to kill someone than do non-smokers (n = 288)\nMen weigh more than women (n = 92) (!!!!!!!!!!!)\n\n\n\n\n8.7.3 More on publication bias\nâ€œPublication biasâ€ is the phenomenon by which statistically significant results are more likely to be published in a journal than results that are not statistically significant.\nThere are many tools for trying to diagnose this (including P-curve)\nTest of Insufficient Variance: convert p-values to z-statistics. Expected variance of z-statistics is 1. Variance less than 1 could suggest â€œmissingâ€ studies.\n\n\n8.7.4 Funnel plot\nPlot effect size vs.Â standard error. Variance in effect sizes should decrease as standard error decreases, but effect size and standard error should not be correlated.\n\n\n\n\n\nCorrelation could suggest â€œmissingâ€ studies.\n\n\n8.7.5 Inter-ocular trauma test\nâ€œDo the results hit you between the eyes?â€\n\n\n\n\n\n(z = 1.96 is the threshold for statistical significance)"
  },
  {
    "objectID": "Ch8_Mind_the_Gap.html#type-m-errors",
    "href": "Ch8_Mind_the_Gap.html#type-m-errors",
    "title": "8Â  Chapter 8: Minding the gap between science and statistics",
    "section": "8.8 Type â€œMâ€ errors",
    "text": "8.8 Type â€œMâ€ errors\nDespite the fact that large sample sizes are needed to detect what seem like â€œmediumâ€ effects, we see lots of studies will small sample sizes reporting significant effects.\nAlso, significant effects from small sample sizes are always large.\nA likely culprit: the combination of publication bias and low power.\n\nLow power: a typical consequence ofâ€¦\n\nSmall sample sizes\nSmall effects\nNoisy or imprecise measurements\nNoisy or imprecise manipulations\n\n\nA nasty consequence of publication bias combined with low power: published effect sizes are biased upward. The lower the power, the greater the bias.\nA diagram of low power:\n\n\n\n\n\nNotice: â€œthe truthâ€ is NOT in the â€œreject the nullâ€ region. So when power is low, â€œthe truthâ€ is not statistically significant!\n\n8.8.1 Visually, using confidence intervals\n\n\n\n\n\nLow power =&gt; wide CIs\nHere, wide CI centered on true effect =&gt; not significant\nFor CI to exclude zero, sample effect must greatly overestimate true effect\n\n\n8.8.2 Visually, using sampling distributions\nBelow: Sampling distribution of the Cohenâ€™s d statistic (standardized diff. in means). Simulated so that power = 0.27 and population d = 0.5.\n\n\n\n\n\nBelow: same thing, but after removing all d statistics that do not achieve statistical significance.\n\n\n\n\n\nUpshot: conditioning an unbiased estimator on p &lt; 0.05 creates a biased estimator. The lower the power, the greater the bias.\n\n\n8.8.3 The extreme example\n\n\n\n\n\n\n\n8.8.4 Can low power be assessed empirically?\nâ€œPost-hocâ€ power analysis describes performing a power analysis on a data set after having conducted a significance test.\nSometimes researchers will get a non-significant result, and suspect low power is the reason. So, they do a power analysis using the effect size and standard error from the data, and find low power.\nTHIS IS INVALID! If p &gt; 0.05, then post-hoc power must be lower than 50%.\nSo, â€œnon-significantâ€ results will always be identified as â€œlow powerâ€, post-hoc.\n\n\n8.8.5 Estimating power meta-analytically: R-Index\nR-Index: calculate observed power for each study. Compare average power to proportion of studies showing significance. Lower observed power could suggest missing studies.\ne.g.Â observed power = 0.6, 10 / 10 results significant; 6 / 10 expected if power = 0.6.\nIf a collection of studies all show significant results, then average observed power must be greater than 50%. But, it might not be much greater."
  },
  {
    "objectID": "Ch8_Mind_the_Gap.html#should-we-throw-away-non-significant-results",
    "href": "Ch8_Mind_the_Gap.html#should-we-throw-away-non-significant-results",
    "title": "8Â  Chapter 8: Minding the gap between science and statistics",
    "section": "8.9 Should we throw away non-significant results?",
    "text": "8.9 Should we throw away non-significant results?\nThere are statistical problems that arise when only reporting significant results.\nThere is also a scientific problem: are non-significant results really uninteresting?\nIf the question is worth asking (â€œdo I have evidence for this substantive hypothesis?â€), isnâ€™t the answer worth knowing?\nThe practice of throwing away non-significant results goes hand in hand with a false interpretation of NHST results: that non-significance implies â€œno effectâ€.\np-value &lt; 0.05 is commonly interpreted as evidence for an effect.\nBut, p-value &gt; 0.05 should not necessarily be interpreted as evidence for no effect.\n\n8.9.1 â€œThere was no effect (p&gt;0.05)â€\nExample: suppose three studies (A, B, and C) all aimed to estimate a standardized difference in means in terms of Cohenâ€™s d:\n\\[\nd= \\frac{\\bar{x}_1 - \\bar{x}_2}{s_p}\n\\]\nSuppose that for each study, a p-value is also computed, testing against the null of \\(H_0: \\mu_1-\\mu_2 = 0\\)\nA 95% CI for d is also constructed.\nIn this example, all CIs contain zero, so all p-values exceed 0.05. All three tests are consistent with â€œno effectâ€.\n\n\n\n\n\nHowever, the first CI is also consistent with a very large effect, which could be positive or negative. The third CI is consistent with no effect or a very small effect .\nBut - the p-values are the same! In all three cases, p = 0.51.\n\n\n8.9.2 Published example: knee surgery study\n\n\n\n\n\nThe article â€œArthroscopic partial meniscectomy versus placebo surgery for a degenerative meniscus tear: a 2-year follow-up of the randomized controlled trialâ€ assesses the effectiveness of a surgical procedure for treating a degenerative knee tear relative to a sham â€œplaceboâ€ surgery (!!!). https://ard.bmj.com/content/77/2/188\nThe article finds a non-significant difference between treatment and placebo, and interprets this as the treatment being â€œno betterâ€ than placebo. Author conclude there is â€œno evidenceâ€ in favor of the treatment.\nHowever, later in the paper the authors make a much stronger argument: that the 95% CI for the difference in means is fully below the minimum clinically meaningful difference, which they established a priori:\n\n\n\n\n\n\nNote the sharp difference between these arguments:\n\nâ€œSurgery was not effective because the difference between surgery and placebo was not statistically significant.â€\nâ€œSurgery was not effective because the 95% CI for the difference between surgery and placebo fell entirely below the minimum clinically significant difference.\n\n\nThe first argument says â€œthe estimated difference is no bigger than what would be expected by chance.â€ The second says â€œthe largest plausible value for the difference is still too small to be interesting.â€"
  },
  {
    "objectID": "Ch8_Mind_the_Gap.html#debates-over-statistical-signficance",
    "href": "Ch8_Mind_the_Gap.html#debates-over-statistical-signficance",
    "title": "8Â  Chapter 8: Minding the gap between science and statistics",
    "section": "8.10 Debates over statistical signficance",
    "text": "8.10 Debates over statistical signficance\nThe use of statistical significance has always been controversial.\nThree recent high profile papers have argued for some different viewpoints on statistical significance.\n\n8.10.1 â€œRedefine statistical significanceâ€\nâ€œRedefine Statistical Significanceâ€ (2017) calls for lowering the standard threshold for significance to p &lt; 0.005\nThe argument: p &lt; 0.05 is too easy to obtain from noise.\nThis paper proposes labeling p-values between 0.005 and 0.05 as â€œsuggestiveâ€, and p-values less than 0.005 as â€œsignificantâ€.\nAn exception: if the procedure is pre-registered, p &lt; 0.05 can be labeled â€œsignificantâ€. So a distinction is drawn between exploratory and confirmatory data analyses.\n\n\n8.10.2 â€œJustify your alphaâ€\nâ€œJustify Your Alphaâ€ (2017), written in response, calls for allowing flexibility in alpha levels rather than defaulting to p &lt; 0.05.\nThe argument: alpha (a.k.a. the significance level) sets a trade-off between Type I errors and Type II errors.\nSmaller values of alpha lower Type I error rates but increase Type II error rates, and vice versa.\nThe optimal trade-off will be different for different research fields. FDA drug trials should not use the same trade-off as exploratory research in brand new research fields.\n\n\n8.10.3 â€œAbandon statistical significanceâ€\nâ€œAbandon Statistical Significanceâ€ (2017) calls for the elimination of thresholds entirely.\nThe argument: â€œsignificanceâ€ is just a way of taking continuous phenomena (e.g.Â differences in means, probabilities, correlations) and forcing them into one of two categories.\nInstead, why not report the evidence on its own terms? No need to force it into an artificial and simplistic â€œeither / orâ€ distinction."
  },
  {
    "objectID": "Ch8_Mind_the_Gap.html#proposed-reforms",
    "href": "Ch8_Mind_the_Gap.html#proposed-reforms",
    "title": "8Â  Chapter 8: Minding the gap between science and statistics",
    "section": "8.11 Proposed reforms",
    "text": "8.11 Proposed reforms\n\n8.11.1 Some proposals for doing things differently\nPre-registration and registered reports: data analysis plans are stated ahead of time. This removes flexibility in analysis.\nWith registered reports, data analysis plans are peer reviewed, and papers can be accepted for publication before results are known. This removes publication bias.\n\n\n\n\n\nRewarding â€œopenâ€ practices.\nThe Association for Psychological Science now does this using badges:\n\n\n\n\n\nâ€œEquivalence testingâ€: an alternative to â€œacceptingâ€ a null that has not been rejected.\nIdea: establish a minimum effect size of interest (e.g.Â â€œweâ€™re not interested in this drug if it doesnâ€™t reduce blood pressure by at leastâ€¦â€)\nMake the null of the equivalence test be that the true effect is smaller than the minimum effect size of interest.\nIf the null is rejected, then observed results are â€œequivalentâ€ to the null insofar as they are too small to be interesting.\nVisualization of equivalence testing, using confidence intervals:\n\n\n\n\n\nNote that results can be both â€œnot significantly differentâ€ and â€œnot significantly equivalentâ€.\nThey can also be both â€œsignificantly equivalentâ€ and â€œsignificantly differentâ€!\nâ€œThe New Statisticsâ€ proposes that we emphasize confidence intervals over p-values, as they are easier to understand and less noisy (i.e.Â they donâ€™t change as much across repeated samples)\nThe Peer Reviewersâ€™ Openness Initiative calls on reviewers to require open data, open methods, and code that will reproduce analyses, so that reviewers can double check the analyses and results.\nThe GRIM test, SPRITE test, and Statcheck are algorithms that check for internal consistency of reported results. They provide a â€œsanity checkâ€ that can detect potentially p-hacked data analyses.\n\n\n8.11.2 Some closing remarks\nAs stated at the outset, there is great controversy over the appropriate use of statistical methods!\nSome wise words from participants in this controversy:\n\nâ€œWe often hear itâ€™s too easy to obtain small p-values, yet replication attempts find it difficult to get small p-values with preregistered results. This shows the problem isnâ€™t p-values but failing to adjust them for cherry picking, multiple testing, post- data subgroups and other biasing selection effects.â€\n\n-Deborah Mayo, â€œDonâ€™t throw out the error control baby with the bad statistics bathwaterâ€\n\nâ€œIt seems to me that statistics is often sold as a sort of alchemy that transmutes randomness into certainty, anâ€uncertainty launderingâ€ that begins with data and concludes with success as measured by statistical significance â€¦ the solution is not to reform p-values or to replace them with some other statistical summary or threshold, but rather to move toward a greater acceptance of uncertainty and embracing of variation.\n\nAndrew Gelman, â€œThe problems with p-values are not just p-valuesâ€"
  },
  {
    "objectID": "Ch9_Other_Topics.html#formal-model-selection-tools",
    "href": "Ch9_Other_Topics.html#formal-model-selection-tools",
    "title": "9Â  Brief looks at major topics we didnâ€™t cover",
    "section": "9.1 Formal model selection tools",
    "text": "9.1 Formal model selection tools\n\n9.1.1 AIC / BIC\n\n\n9.1.2 Backwards and forwards selection\n\n\n9.1.3 Penalized regression (LASSO and Ridge)"
  },
  {
    "objectID": "Ch9_Other_Topics.html#bayesian-statistics",
    "href": "Ch9_Other_Topics.html#bayesian-statistics",
    "title": "9Â  Brief looks at major topics we didnâ€™t cover",
    "section": "9.2 Bayesian statistics",
    "text": "9.2 Bayesian statistics\n\n9.2.1 Probability as rational degree of belief\n\n\n9.2.2 Priors\n\n\n9.2.3 Things Bayes permits that classical methods donâ€™t"
  },
  {
    "objectID": "Ch9_Other_Topics.html#non-parametric-methods",
    "href": "Ch9_Other_Topics.html#non-parametric-methods",
    "title": "9Â  Brief looks at major topics we didnâ€™t cover",
    "section": "9.3 Non-parametric methods",
    "text": "9.3 Non-parametric methods\n\n9.3.1 Alternatives for t-tests and ANOVA\n\n\n9.3.2 Alternatives for regression"
  },
  {
    "objectID": "Ch9_Other_Topics.html#meta-analysis",
    "href": "Ch9_Other_Topics.html#meta-analysis",
    "title": "9Â  Brief looks at major topics we didnâ€™t cover",
    "section": "9.4 Meta-analysis",
    "text": "9.4 Meta-analysis\n\n9.4.1 Combining lots of studies\n\n\n9.4.2 Heterogeneity\n\n\n9.4.3 Interpretability"
  },
  {
    "objectID": "Ch9_Other_Topics.html#regression-to-the-mean",
    "href": "Ch9_Other_Topics.html#regression-to-the-mean",
    "title": "9Â  Brief looks at major topics we didnâ€™t cover",
    "section": "9.5 Regression to the mean",
    "text": "9.5 Regression to the mean"
  }
]
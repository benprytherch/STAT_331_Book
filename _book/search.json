[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "STAT 331",
    "section": "",
    "text": "Introduction to STAT 331: Intermediate Applied Statistical Methods"
  },
  {
    "objectID": "index.html#purpose-and-intended-audience",
    "href": "index.html#purpose-and-intended-audience",
    "title": "STAT 331",
    "section": "Purpose and intended audience",
    "text": "Purpose and intended audience\nSTAT 331, as the title states, is an â€œappliedâ€ statistics course. It is intended for anyone who has taken at least one introductory level statistics course, and who wants to learn more about the use of statistical methods in quantitative research.\nIt covers many statistical tools that are usually considered too advanced for an introductory level class, but are nonetheless very popular. It also provides guidance on making data analysis decisions.\nMost assignments will involve looking up a published scientific paper for which the data are available and reproducing the main results. There are many worked examples that go through the statistical analyses in used in specific published papers.\nSTAT 331 doesnâ€™t require any coding; the software we use is jamovi, a GUI-based (meaning â€œgraphical user interfaceâ€, aka â€œpoint-and-clickâ€) statistical analysis package. Jamovi is built on to of R, and for those interested in using R it has the ability to display the R code it creates under the hood.\nSTAT 331 is not mathematically heavy in the traditional sense, but it isnâ€™t math-free either. My approach is to present mathematical formulas and expressions when they are necessary or at least helpful for understanding the statistical itâ€™s being covered. There are no mathematical character-building exercises or examples, and we wonâ€™t be computing things by hand - the software does all the computational work. Our job is to make sense of the results. And that usually requires looking at formulas and figuring out what they do. We will be constantly answering the question â€œwhat does this number mean?â€"
  },
  {
    "objectID": "index.html#structure-of-these-notes",
    "href": "index.html#structure-of-these-notes",
    "title": "STAT 331",
    "section": "Structure of these notes",
    "text": "Structure of these notes\nThese notes are broken up into 9 chapters (or â€œmodulesâ€):\n\nChapter 1: Review of classical inference\nChapter 2: Model building with linear regression\nChapter 3: Assessing and improving model fit\nChapter 4: ANOVA-based methods\nChapter 5: Analyzing categorical data\nChapter 6: Generalized Linear Models (GLMs)\nChapter 7: Mixed-effects models\nChapter 8: Minding the gap between science and statistics\nChapter 9: Brief looks at major topics we didnâ€™t cover\n\nYou can access chapters and subsections directly through the table of contents."
  },
  {
    "objectID": "index.html#really-good-online-books",
    "href": "index.html#really-good-online-books",
    "title": "STAT 331",
    "section": "Really good online books",
    "text": "Really good online books\nThese notes will frequently reference some other freely available online statistics books:\nLearning statistics with jamovi, by Danielle Navarro and David Foxcroft\nIntroduction to Regression Analysis in R, by Kayleigh Keller\nAnswering questions with data, by Matthew J. C. Crump, Danielle J. Navarro, and Jeffrey Suzuki\nStatistical Analysis with The General Linear Model, by Jeff Miller and Patricia Haden"
  },
  {
    "objectID": "Ch1_Review.html#module-1-overview",
    "href": "Ch1_Review.html#module-1-overview",
    "title": "1Â  Chapter 1: Review of classical inference",
    "section": "1.1 Module 1 overview",
    "text": "1.1 Module 1 overview\nâ€¢ These notes briefly cover material from your introductory statistics course that will be relevant in STAT 331.\nâ€¢ For a more in-depth review, consult the OpenIntro text, or just do an internet search."
  },
  {
    "objectID": "Ch1_Review.html#distributions",
    "href": "Ch1_Review.html#distributions",
    "title": "1Â  Chapter 1: Review of classical inference",
    "section": "1.2 Distributions",
    "text": "1.2 Distributions\nâ€¢ The term â€œdistributionâ€ will be used a lot.\nâ€¢ A distribution gives the values a variable takes on, and how often it takes them on.\nâ€¢ Examples: normal distribution, uniform distribution, distribution of exam scores, distribution of heightsâ€¦"
  },
  {
    "objectID": "Ch1_Review.html#commonly-used-statistics",
    "href": "Ch1_Review.html#commonly-used-statistics",
    "title": "1Â  Chapter 1: Review of classical inference",
    "section": "1.3 Commonly used statistics",
    "text": "1.3 Commonly used statistics\n\n1.3.1 Mean\nâ€¢ Mean and median identify the center of a data set or distribution.\nâ€¢ The mean of a variable \\(X\\) is denoted \\(\\bar{X}\\).\nâ€¢ To calculate the mean of a data set, add up all the values of a variable and divide by how many there are.\n\\[\n\\bar{X} = \\frac{\\sum^n_{i=1}x_i}{n}\n\\]\n\n\n1.3.2 Median\nâ€¢ Median is the â€œmiddleâ€ number in a data set. To find the median, put the data values in order from smallest to largest, and identify the number in the middle.\nâ€¢ If there are an even number of data points, the median is the average of the middle two numbers:\n\n\n\n\n\n\n\n1.3.3 Mean vs.Â Median\nâ€¢ In statistical inference (the process of generalizing from sample to population), we most often draw inference on the population mean.\nâ€¢ Sometimes, though, the median is a more sensible statistic than the mean.\nâ€¢ This is usually the case when we are studying a â€œskewedâ€ distribution.\nâ€¢ Skewed distributions are distributions that take on values that are extreme (or outlying) values.\nâ€¢ Example: income. Most households have incomes between $20,000/yr and $100,000/yr. A handful of households have incomes in the millions or billions of dollars per year.\nâ€¢ The mean is affected by outliers. The median is not. This is why we often hear about â€œmedian household incomeâ€ rather than â€œmean household incomeâ€.\n\n\n1.3.4 jamovi example: mean vs.Â median\nâ€¢ Try creating a skewed data set in jamovi, then analyzing it by selecting for mean and median in the Statistics drop down menu in Descriptives, found under Exploration in the Analysis tab\nâ€¢ To create a new dataset, enter data into the blank data table jamovi creates by default:\n\n\n\n\n\n\n\n\n\n\n\n\n1.3.5 Variance and standard deviation\nâ€¢ The center of a distribution is quantified by the mean or the median.\nâ€¢ The variability (i.e.Â spread) of a distribution is quantified by the standard deviation, which is closely related to the variance.\nâ€¢ The variance of a distribution or data set is denoted \\(s^2\\):\n\\[\ns^2 = \\frac{\\sum^n_{i=1}(x_i - \\bar{x})^2}{n-1}\n\\]\nâ€¢ The standard deviation is simply the square root of the variance\n\\[\ns = \\sqrt{\\frac{\\sum^n_{i=1}(x_i - \\bar{x})^2}{n-1}}\n\\]\nâ€¢ Think of this as the â€œstandardâ€ amount by which values deviate from their mean.\nâ€¢ We will most often look at standard deviation, because it is the more interpretable of the two statistics. It is in the same units as the original variable.\n\n\n1.3.6 The correlation coefficient\nâ€¢ The correlation coefficient, ğ‘Ÿ, quantifies the extent to which two variables (call them X and Y) move together:\n\\[\nr = (\\frac{1}{n-1})\\sum^n_{i=1}\\frac{(x_i - \\bar{x})(y_i-\\bar{y})}{s_xs_y} = (\\frac{1}{n-1})\\sum^n_{i=1}z_{x_i}z_{y_i}\n\\]\nâ€¢ Donâ€™t worry too much about the formula. The most important things to know are:\nâ€¢ When two variables move in the opposite direction (i.e.Â when one gets bigger, the other gets smaller), \\(r\\) is negative.\nâ€¢ When they move in the same direction, \\(r\\) is positive.\nâ€¢ \\(r = 0\\) means no correlation. \\(r =1\\) means perfect positive correlation. \\(r = -1\\) means perfect negative correlation."
  },
  {
    "objectID": "Ch1_Review.html#statistics-and-parameters",
    "href": "Ch1_Review.html#statistics-and-parameters",
    "title": "1Â  Chapter 1: Review of classical inference",
    "section": "1.4 Statistics and parameters",
    "text": "1.4 Statistics and parameters\nâ€¢ A statistic is any value calculated from data.\nâ€¢ A parameter is any value pertaining to a population.\nâ€¢ The values of unknown parameters are â€œestimatedâ€ using statistics.\nâ€¢ Example: a sample mean can be used to estimate a population mean. i.e.Â \\(\\bar{X}\\) estimates \\(\\mu\\)."
  },
  {
    "objectID": "Ch1_Review.html#sampling-distributions",
    "href": "Ch1_Review.html#sampling-distributions",
    "title": "1Â  Chapter 1: Review of classical inference",
    "section": "1.5 Sampling distributions",
    "text": "1.5 Sampling distributions\nâ€¢ A sampling distribution is the distribution of values a statistic takes on, under repeated sampling.\nâ€¢ For example, the Central Limit Theorem states that the sampling distribution of \\(\\bar{X}\\) will be normal, so long as the sample size (\\(n\\)) is large enough.\nâ€¢ Sampling distributions are important because most methods used in statistical inference invoke long run frequency properties.\nâ€¢ Example: if the sampling distribution of \\(\\bar{X}\\) is not very spread out, then the value of \\(\\bar{X}\\) should not change much if we take a new sample.\n\n1.5.1 Standard error\nâ€¢ Standard error is the standard deviation of a sampling distribution.\nâ€¢ In other words, it is the amount of variability in the values a statistic takes on under repeated sampling.\nâ€¢ So, if a statistic we calculate has a small standard error, we can infer that the value of that statistic is close to the value of the population parameter it is estimating. If it has a large standard error, its value might be very far away from the value of the parameter.\nâ€¢ Example: the standard error of \\(\\bar{X}\\) is \\(\\frac{s}{\\sqrt{n}}\\) i.e.Â \\(s_{\\bar{X}} = \\frac{s}{\\sqrt{n}}\\)\n\n\n1.5.2 Sampling dist. and standard error, visually\nâ€¢ On Canvas there is a Central Limit Theorem simulator.\nâ€¢ When the sample size is large, the distribution of the sample mean is not very spread out. In other words, its standard error is small.\nâ€¢ When the sample size is small, the standard error is large."
  },
  {
    "objectID": "Ch1_Review.html#confidence-intervals",
    "href": "Ch1_Review.html#confidence-intervals",
    "title": "1Â  Chapter 1: Review of classical inference",
    "section": "1.6 Confidence intervals",
    "text": "1.6 Confidence intervals\nâ€¢ A confidence interval (â€œCIâ€) is an interval (i.e.Â a left endpoint and a right endpoint) constructed around a statistic, when that statistic is being treated as an estimate for the value of an unknown parameter.\nâ€¢ Typical confidence intervals are constructed by adding a â€œmargin of errorâ€ to, and subtracting it from, an estimate:\nğ¶ğ¼ ğ‘“ğ‘œğ‘Ÿ ğ‘ğ‘ğ‘Ÿğ‘ğ‘šğ‘’ğ‘¡ğ‘’ğ‘Ÿ = ğ‘’ğ‘ ğ‘¡ğ‘–ğ‘šğ‘ğ‘¡ğ‘’ ğ‘œğ‘“ ğ‘ğ‘ğ‘Ÿğ‘ğ‘šğ‘’ğ‘¡ğ‘’ğ‘Ÿ Â± ğ‘šğ‘ğ‘Ÿğ‘”ğ‘–ğ‘› ğ‘œğ‘“ ğ‘’ğ‘Ÿğ‘Ÿğ‘œğ‘Ÿ\nâ€¢ Typical margins of error are calculated by multiplying the standard error of the estimate by a â€œcritical valueâ€. A critical value comes from a known distribution and is given by a confidence level.\nâ€¢ Example: a 95% CI for a population mean uses a critical value from the \\(t\\) distribution (we wonâ€™t cover why this is).\nğ¶ğ¼ ğ‘“ğ‘œğ‘Ÿ ğœ‡ = \\(\\bar{x}\\) Â± \\(ğ‘¡_{ğ‘ğ‘Ÿğ‘–ğ‘¡ğ‘–ğ‘ğ‘ğ‘™}\\) âˆ— \\(s_{\\bar{x}}\\)\nâ€¢ As long as \\(n\\) isnâ€™t tiny, the 95% critical value from a \\(t\\) distribution is approximately 2:\nğ¶ğ¼ ğ‘“ğ‘œğ‘Ÿ ğœ‡ â‰ˆ \\(\\bar{x}\\) Â± \\(2\\) âˆ— \\(s_{\\bar{x}}\\)\nâ€¢ Confidence intervals should capture the unknown parameter value being estimated. The confidence level gives how often the interval captures the parameter under repeated sampling.\nâ€¢ Example: 95% of all 95% CIs for \\(\\mu\\) capture \\(\\mu\\) .\nâ€¢ The confidence level can also be thought of as the success rate of the method being used.\nâ€¢ So, 95% confidence intervals have a 95% success rate in capturing the value of the unknown parameter.\nâ€¢ Just as with sampling distributions, we are invoking repeated sampling here. We say that a 95% CI can be trusted because it is created using a method that would â€œworkâ€ 95% of the time, if we were to keep taking new samples and keep constructing 95% CIs."
  },
  {
    "objectID": "Ch1_Review.html#confidence-intervals-quantify-uncertainty",
    "href": "Ch1_Review.html#confidence-intervals-quantify-uncertainty",
    "title": "1Â  Chapter 1: Review of classical inference",
    "section": "1.7 Confidence intervals quantify uncertainty",
    "text": "1.7 Confidence intervals quantify uncertainty\nâ€¢ The most important characteristic of a CI is its width.\nâ€¢ We are typically willing to believe that the unknown value of a parameter lies inside the confidence interval constructed from our data.\nâ€¢ If the confidence interval is wide, there is a lot of uncertainty as to the true value of the parameter.\nâ€¢ If the confidence interval is narrow, then our estimate for the value of the parameter is â€œpreciseâ€, in that it shouldnâ€™t be wrong by much.\nâ€¢ CIs are narrow when the sample size is large and / or the standard deviation of our data is small.\nâ€¢ CIs are wide with the sample size is small and / or the standard deviation of our data is large.\nâ€¢ IMPORTANT: CIs, like all inferential statistical methods, are created under assumptions. We make distributional assumptions about our data (e.g.Â normality). We assume our statistic is an unbiased estimate of the parameter, i.e.Â it will not systematically differ from the parameter value under repeated sampling."
  },
  {
    "objectID": "Ch1_Review.html#confidence-interval-simulation-apps",
    "href": "Ch1_Review.html#confidence-interval-simulation-apps",
    "title": "1Â  Chapter 1: Review of classical inference",
    "section": "1.8 Confidence interval simulation apps",
    "text": "1.8 Confidence interval simulation apps\nâ€¢ There is a confidence interval simulation app on Canvas, that demonstrates creating confidence intervals â€œunder repeated samplingâ€. This app is from Brown Universityâ€™s â€œSeeing theoryâ€ series .\nâ€¢ There is also a â€œsampling distribution and standard errorâ€ app on Canvas. It shows a population distribution for a normally distributed variable, a sample of data from that distribution, and the sampling distribution of the mean.\nâ€¢ This app also super-imposes the standard error in pink."
  },
  {
    "objectID": "Ch1_Review.html#confidence-intervals-in-jamovi",
    "href": "Ch1_Review.html#confidence-intervals-in-jamovi",
    "title": "1Â  Chapter 1: Review of classical inference",
    "section": "1.9 Confidence intervals in jamovi",
    "text": "1.9 Confidence intervals in jamovi\nâ€¢ Jamovi can create a 95% CI and any other summary statistics selected for using the Statistics menu of Descriptives\nâ€¢ For example, when producing summary statistics for a variable using the Statistics drop down menu in Descriptives, you will need to select Confidence Interval for Mean found under Mean Dispersion. Jamovi will report â€œ95% CI mean lower boundâ€ and â€œ95% CI mean upper boundâ€. These are the endpoints for the 95% CI for \\(\\bar{X}\\)."
  },
  {
    "objectID": "Ch1_Review.html#hypothesis-testing",
    "href": "Ch1_Review.html#hypothesis-testing",
    "title": "1Â  Chapter 1: Review of classical inference",
    "section": "1.10 Hypothesis testing",
    "text": "1.10 Hypothesis testing\nâ€¢ Hypothesis testing is an inferential method in which a null hypothesis (\\(H_0\\)) is â€œtestedâ€ against. If the data are in strong enough disagreement with \\(H_0\\) , then \\(H_0\\) is rejected.\nâ€¢ \\(H_0\\) typically represents the proposition that â€œthere is nothing of interest at the population levelâ€, or â€œthe proposed research hypothesis is not trueâ€.\nâ€¢ If \\(H_0\\) is rejected, then the result of the test is described as â€œstatistically significantâ€.\nâ€¢ Example: if we have data from a controlled experiment in which \\(\\mu_1\\) represents the population mean for the control group and \\(\\mu_2\\) represents the population mean for the treatment group, then we might test against the null hypothesis:\n\\[\nH_0: \\mu_1 = \\mu_2,\\text{which is equivalent to }H_0: \\mu_1 âˆ’ \\mu_2 = 0\n\\]\nâ€¢ If we reject \\(H_0\\), we say that the sample means, \\(\\bar{x}_1\\) and \\(\\bar{x}_2\\), are â€œsignificantly differentâ€. Or, equivalently, that \\(\\bar{x}_1 - \\bar{x}_2\\) is â€œsignficantly differentâ€ from zero."
  },
  {
    "objectID": "Ch1_Review.html#the-test-statistic",
    "href": "Ch1_Review.html#the-test-statistic",
    "title": "1Â  Chapter 1: Review of classical inference",
    "section": "1.11 The test statistic",
    "text": "1.11 The test statistic\nâ€¢ The strength of the evidence against \\(H_0\\) is quantified by a â€œtest statisticâ€, from which a â€œp-valueâ€ is calculated.\nâ€¢ Test statistics are set up so that, the more the inconsistent the data are with \\(H_0\\), the larger the test statistic will be.\nâ€¢ Example: when testing \\(H_0: \\mu_1 âˆ’ \\mu_2 = 0\\), we use the test statistic:\n\\[\nt = \\frac{\\bar{x}_1 - \\bar{x}_2}{s_{(\\bar{x}_1 - \\bar{x}_2)}} = \\frac{\\bar{x}_1 - \\bar{x}_2}{\\sqrt{\\frac{s_1^2}{n_1} + \\frac{s_2^2}{n_2}}} \\text{(where } s_{(\\bar{x}_1 - \\bar{x}_2)}\\text{ is the standard error of }\\bar{x}_1 - \\bar{x}_2)\n\\]"
  },
  {
    "objectID": "Ch1_Review.html#the-p-value",
    "href": "Ch1_Review.html#the-p-value",
    "title": "1Â  Chapter 1: Review of classical inference",
    "section": "1.12 The p-value",
    "text": "1.12 The p-value\nâ€¢ The p-value is defined as the probability of getting a test statistic at least as large as the one calculated, if we assume \\(H_0\\) is true.\nâ€¢ Visually, the p-value is the area in the tail of the sampling distribution of the test statistic under \\(H_0\\)\n\n\n\n\n\nâ€¢ If the p-value is less than the â€œlevel of significanceâ€ (\\(\\alpha\\)), then \\(H_0\\) is rejected.\nâ€¢ By far the most typical level of significance is \\(\\alpha = 0.05\\)\n\n\n\n\n\n\n1.12.1 Interpreting â€œstatistical significanceâ€\nâ€¢ â€œStatistically significantâ€ results are those that produce a small p-value.\nâ€¢ Small p-values result from data that would be unlikely to be obtained just by chance, if the null hypothesis were true.\nâ€¢ So, when you hear that results are â€œstatistically significantâ€, you can interpret this as meaning â€œthe data we obtained donâ€™t look like the kind of data weâ€™d expect to see just by chanceâ€.\n\n\n1.12.2 Cautions regarding â€œstatistical significanceâ€\nâ€¢ As with confidence intervals, hypothesis tests require assumptions.\nâ€¢ These will be covered in detail in the next module.\nâ€¢ The most important distinction to be made right now is the distinction between statistical significance and practical importance.\nâ€¢ Results can be statistically significant, but still seem weak or unimpressive by practical standards.\nâ€¢ Example: this is a statistically significant correlation:\n\n\n\n\n\n(\\(r = 0.22\\), p-value = 0.011)\nâ€¢ Example: this is a statistically significant difference in means:\n\n\n\n\n\n(\\(\\bar{x}_1 = 19.7\\), \\(\\bar{x}_2 = 22.7\\), p-value = 0.0013)\nâ€¢ The use of hypothesis testing is controversial.\nâ€¢ I personally do not like hypothesis testing, and I think that statistical significance is usually uninteresting.\nâ€¢ Weâ€™ll explore the debates surrounding statistical significance in module 2.\n\n\n1.12.3 Confidence intervals vs.Â p-values\nâ€¢ For now, weâ€™ll note that in many cases, confidence intervals can be used in place of p-values to perform a hypothesis test.\nâ€¢ If a 95% CI excludes the null value (typically zero), then \\(H_0\\) is rejected at the \\(\\alpha = 0.05\\) level of significance.\nâ€¢ The advantage of using a confidence interval rather than a p-value is that it is easier to make sense out of, and it quantifies uncertainty: the wider the CI, the more uncertainty there is regarding the value of the unknown parameter.\n\n\n1.12.4 Confidence intervals vs.Â p-values example\n\n\n\n\n\n\\(\\bar{x}_1 = 19.7\\), \\(\\bar{x}_2 = 22.7\\), p-value = 0.0013\n95% CI for \\(\\mu_1 - \\mu_2: (-4.72,-1.19)\\)\n\n\n\n\n\nâ€¢ Here we see 95% CIs for differences in means, along with their corresponding p-values.\nâ€¢ Note how much the p-values change for small changes in the CIs.\nâ€¢ Note also that different CIs can correspond to the same p-value."
  },
  {
    "objectID": "Ch1_Review.html#cis-and-p-values-in-jamovi",
    "href": "Ch1_Review.html#cis-and-p-values-in-jamovi",
    "title": "1Â  Chapter 1: Review of classical inference",
    "section": "1.13 CIs and p-values in jamovi",
    "text": "1.13 CIs and p-values in jamovi\nâ€¢ CIs and p-values can be calculated for a huge variety of statistics.\nâ€¢ For now, we will consider testing for a difference in means.\nâ€¢ In jamovi, select an Independent Samples T-Test from T-Tests under the Analyses tab\nâ€¢ Make Max_Temp_Challenge be the Dependent variable (the one containing measurements), and make Vaccine be the Grouping variable (the one identifying which group the measurement belongs to).\nâ€¢ Here is an example using the Vaccine data set. This example uses sheet 3 of the Excel file, titled â€œH3N2_Clinical_Maxâ€: Data will need to be prepared for test by swapping the rows so that Vaccine occurs first."
  },
  {
    "objectID": "Ch1_Review.html#cis-and-p-values-in-jmp",
    "href": "Ch1_Review.html#cis-and-p-values-in-jmp",
    "title": "1Â  Chapter 1: Review of classical inference",
    "section": "1.14 CIs and p-values in JMP",
    "text": "1.14 CIs and p-values in JMP\n\n\n\n\n\nâ€¢ Here, the p-value is \\(0.002\\)\nâ€¢ The 95% CI for the difference in population mean max_temp is \\((âˆ’2.07, âˆ’0.58)\\)\nâ€¢ The confidence interval excludes zero and \\(p &lt; 0.05\\), so the difference in sample means is statistically significant."
  },
  {
    "objectID": "Ch1_Review.html#data-format-in-jamovi",
    "href": "Ch1_Review.html#data-format-in-jamovi",
    "title": "1Â  Chapter 1: Review of classical inference",
    "section": "1.15 Data format in jamovi",
    "text": "1.15 Data format in jamovi\n\nA final note on data formatting: this data set is in \"long form\", meaning each row is a single observation and each column is a variable.\njamovi's Independent Samples T-test requires long form data.\n\n\n\nSometimes you'll have data in \"wide form\", where each column is a group, and the rows do not correspond to single observations\n\n\n\nExample: here's some fake data in wide form:\n\n\n\n\n\n\n\nFit Independent Samples T-test cannot be used to compare these means.Â  jamovi thinks there are 4 observations, each with a measurement on Var1 and Var 2.Â \n\n\n\n\n\n\n\nThis isn't what we want!\nTo transform the data into long form, we need to install the Rj â€“ Editor module which will allow us to run R-code in jamovi"
  },
  {
    "objectID": "Ch1_Review.html#installing-rj-in-jamovi",
    "href": "Ch1_Review.html#installing-rj-in-jamovi",
    "title": "1Â  Chapter 1: Review of classical inference",
    "section": "1.16 Installing Rj in jamovi",
    "text": "1.16 Installing Rj in jamovi\n\nNavigate to the Analyses tab\n\nClick on Modules in the top right of the jamovi window\nClick jamovi library\nScroll until you see \"Rj â€“ Editor to run R code\", click install\nYou should now see an R logo under the Analyses tab"
  },
  {
    "objectID": "Ch1_Review.html#wide-form-to-long-form-in-jamovi",
    "href": "Ch1_Review.html#wide-form-to-long-form-in-jamovi",
    "title": "1Â  Chapter 1: Review of classical inference",
    "section": "1.17 Wide form to long form in jamovi",
    "text": "1.17 Wide form to long form in jamovi\n\nTo switch from wide to long form, we will write a simple line of R-code using the Rj â€“ Editor module:Â \nClick on Rj, it will open an empty window where we can enter R - code\n\n\n\n\n\n\n\nThe code below transforms the data into long form by stacking the data within Var1 and Var2 into a new column Data and creates a new column Labels to identify if data is from Var1 or Var2.\nThe data will output a csv file which we can import from a new session of jamovi\n\n\n\nImporting the transformed csv file to a new jamovi window shows our transformed long form data table. Note: column names will need to be updated\nCompare the two:\n\nâ€œLong formâ€\n\n\n\n\n\nâ€œWide formâ€\n\n\n\n\n\n\nNow Independent Samples T-test can be used to compare means:"
  },
  {
    "objectID": "Ch2_Model_Building.html#outline-of-notes",
    "href": "Ch2_Model_Building.html#outline-of-notes",
    "title": "2Â  Chapter 2: Model building with linear regression",
    "section": "2.1 Outline of notes:",
    "text": "2.1 Outline of notes:\n\nThe linear regression equation\nRegression analysis in jamovi\nInterpreting regression results\nApplied example (â€œThe Binary Biasâ€)\nInteraction between variables, conceptually\nInteraction between variables in a regression model\nApplied example: arthritis treatment data\nCentering predictor variables\nStandardizing predictor variables"
  },
  {
    "objectID": "Ch2_Model_Building.html#the-linear-regression-equation",
    "href": "Ch2_Model_Building.html#the-linear-regression-equation",
    "title": "2Â  Chapter 2: Model building with linear regression",
    "section": "2.2 The linear regression equation",
    "text": "2.2 The linear regression equation\nLinear regression, in its simplest form, is a method for finding the â€œbest fittingâ€ line through a set of bivariate (two variable) data:\n\nWhat is meant by â€œbest fittingâ€ will be addressed shortly. For now, think of the line as showing the underlying linear trend through a set of data.\nThe vertical distance between each data point and the line is called a â€œresidualâ€. On the plot above, the red lines represent residuals. They quantify the amount by which a data point deviates from the underlying linear trend. Every point has a residual; the plot above only shows a few of them.\n\n2.2.1 The linear regression equation as a statistical model\nThe line that is drawn through data comes from a statistical model. A statistical model is a mathematical expression describing how data are generated. It has a fixed component and a random component. Think of the fixed component as describing the underlying relationship between variables, and the random component as describing any additional variability in data beyond what the fixed component describes.\nBelow is the standard linear regression model. The random component is represented by \\(``\\varepsilon_i\"\\). Everything from â€œ\\(\\beta_0\\)â€ up until â€œ\\(\\varepsilon_i\\)â€ is the fixed component.\n\\[\nY_i=\\beta_0+\\beta_1x_{1i} + \\beta_2x_{2i} + \\dots + \\beta_kx_{ki} + \\varepsilon_i \\\\\ni = 1, \\dots, n \\\\\n\\varepsilon_i \\sim \\text{Normal}(0,\\sigma^2)\n\\]\nHereâ€™s what each term represents:\n\n\\(i\\) is the index term. It counts through the data, starting at \\(i = 1\\) and going through \\(i=n\\), where \\(n\\) is the sample size.\n\\(Y_i\\) is the \\(i^{th}\\) value of the outcome variable. When written in upper-case, \\(Y_i\\) is treated as a random variable whose value has not been observed. When written lower-case, \\(y_i\\) represents an observed data value.\n\\(Y_i\\) is often referred to as the â€œresponseâ€ variable, or the â€œdependentâ€ variable. These notes will use the term â€œoutcomeâ€ variable. I prefer this term on the grounds that the others seem to imply causality: if \\(Y\\) is â€œrespondingâ€ to \\(x\\), or â€œdependentâ€ on \\(x\\), then it sounds like changing the value of \\(x\\) will induce a change in the value of \\(Y\\).\n\\(x_{1i}\\) is the \\(i^{th}\\) value of the first predictor (i.e.Â independent) variable. \\(x_{2i}\\) is the \\(i^{th}\\) value of the second predictor, etc. The \\(x's\\) are always written lower-case, and technically are assumed to be fixed values, either set prior to data collection or measured without error.\n\\(\\beta_1\\) is the slope (i.e.Â regression coefficient) for the first predictor variable. \\(\\beta_2\\) is the slope of the second predictor, etc. The \\(\\beta's\\) are parameters, meaning their values are treated as fixed (existing at the â€œpopulationâ€ level) but unknown.\nWe use data to calculate estimated values for the \\(\\beta's\\), and these estimates are written using hat notation. For example, \\(\\hat{\\beta_1}\\) is the estimated value for \\(\\beta_1\\).\n\\(\\varepsilon_i\\) is the \\(i^{th}\\) random error value. This is the amount by which \\(Y_i\\) differs from \\(\\beta_0+\\beta_1x_{1i} + \\beta_2x_{2i} + \\dots + \\beta_kx_{ki}\\), i.e.Â the fixed component of the model.\nThe amount by which \\(y_i\\) (the \\(i^{th}\\) observed value of \\(y\\)) differs from \\(\\hat{\\beta_0}+\\hat{\\beta_1}x_{1i} + \\hat{\\beta_2}x_{2i} + \\dots + \\hat{\\beta_k}x_{ki}\\) is called the \\(i^{th}\\) residual, which we can denote \\(e_i\\).\nThe errors are modeled as random values that are drawn from a normal distribution with mean zero and variance \\(\\sigma^2\\).\n\n\n\n\n\n\n\nNote\n\n\n\nThe errors in a regression model do not have to come from a normal distribution. This assumption is made in order to justify inferences about the coefficients; more on this soon.\n\n\nWhen a regression model has only one predictor variable, it is called a â€œsimpleâ€ regression model. If it has more than one predictor variable, it is called a â€œmultiple regressionâ€ model.\n\n\n2.2.2 Assumptions of the regression model\nThis model implies some assumptions:\n\nThe response variable \\(Y\\) is an additive, linear function of the predictors (the \\(x\\) variables)\nIf we fix the value(s) of the \\(x\\) variable(s), all values of \\(Y\\) will be normally distributed. In other words, the errors are normally distributed.\nThe errors have the same variance regardless of the values of the \\(x's\\). This variance is denoted \\(\\sigma^2\\)\n\n\n\n\n\n\n\nNote\n\n\n\nThe square root of the variance is the standard deviation, denoted \\(\\sigma\\). Standard deviation is expressed in the same units of the original variable, whereas variance is expressed in squared units.\nFor this reason, standard deviation is typically referred to when interpreting statistical results. Variance has desirable mathematical properties, and so is more often referred to in statistical theory\n\n\nVisually, this model treats values of Y as being generated randomly from normal distributions centered on the line:\n\n(figure derived from OpenStax Introductory Business Statistics, section 13.4)\nThe â€œerrorsâ€ are the distances between the line and the values generated from the normal distributions.\nThe errors are treated as random and uncorrelated: knowing the value of one error tells you nothing about the likely value of the next."
  },
  {
    "objectID": "Ch2_Model_Building.html#regression-analysis-in-jamovi",
    "href": "Ch2_Model_Building.html#regression-analysis-in-jamovi",
    "title": "2Â  Chapter 2: Model building with linear regression",
    "section": "2.3 Regression analysis in jamovi",
    "text": "2.3 Regression analysis in jamovi\n\n2.3.1 Simulating the regression model in jamovi\nWe noted earlier that a statistical model is data generating. It describes, mathematically, how values of the outcome variable \\(Y\\) can be created. Consider the â€œsimpleâ€ (single \\(x\\) ) regression model:\n\\[\nY_i=\\beta_0+\\beta_1x_{i} + \\varepsilon_i \\\\\n\\varepsilon_i \\sim \\text{Normal}(0,\\sigma^2)\n\\]\nIf we have values for \\(\\beta_0\\), \\(\\beta_1\\), and \\(\\sigma^2\\), when we can plug in values for \\(x_i\\) to generate values for \\(Y_i\\). Letâ€™s do this using jamovi.\nIn jamovi, we first create X values by double-clicking an empty column, choosing â€œNew Computed Variableâ€ then the \\(f_x\\) drop down menu and double click UNIF.\n\nHere, we are generating 100 random values from a \\(Uniform(0,100)\\) distribution. The uniform distribution is a distribution where all values are equally likely, so we should get an even spread of values between 0 and 100.\nTo simulate values of the response variable, weâ€™ll need to make up values for each parameter in the model. Say we want to generate values from this model:\n\\(Y_i=10+0.7x_i+\\varepsilon_i \\quad \\varepsilon_i \\sim Normal(0,8^2)\\)\nThis means weâ€™ve decided that \\(\\beta_0=10\\), \\(\\beta_1=0.7\\), and \\(\\sigma=8\\). And since weâ€™ve generated 100 values for \\(x_i\\), weâ€™ve also decided that \\(i=1\\dots 100\\)\nDouble click an empty column and choose â€œNew Computed Variableâ€:\n\nNow make the formula look like the right side of the regression equation from the previous slide:\n\nNow we can take a look using Scatterplot, a downloadable jamovi module. Click the icon of the plus sign labeled â€œModulesâ€ to bring up a list of available modules you can install. We will use many modules in STAT 331.\n\nAfter installation, Scatterplot is available under Exploration in the Analyses tab. You can assign the X and Y axis variables, and get a plot that looks something like this:\n\nThese are random data, so yours will look a little bit different. But the scales of the axes and vertical spread of the data should be similar.\nNext, weâ€™ll fit a regression model to this data. In practice, we do not know the values of the parameters in our model, so we estimate them using data. This is known as â€œfittingâ€ the model to the data. The point of this simulation is to look at what kind of results we get when fiting a regression model to fake data that was produced by a mechanism we fully understand."
  },
  {
    "objectID": "Ch2_Model_Building.html#fitting-a-regression-model-in-jamovi",
    "href": "Ch2_Model_Building.html#fitting-a-regression-model-in-jamovi",
    "title": "2Â  Chapter 2: Model building with linear regression",
    "section": "2.4 Fitting a regression model in jamovi",
    "text": "2.4 Fitting a regression model in jamovi\nWe can use Regression / Linear Regression to fit a â€œsimpleâ€ regression model, which is a regression model with just one predictor.\n\nThe response variable is â€œDependent Variableâ€.\nThe predictor variable goes under â€œCovariatesâ€.\n\nAfter selecting variables, model will automatically be fit, and output will be generated to the right under â€œResultsâ€.\n\nBased on these results, here is the estimated regression model:\n\\[\n\\hat{y_i}=\\hat{\\beta_0}+\\hat{\\beta_1}x_i=9.1+0.71x_i\n\\]\n\\[\n\\hat{\\sigma}=\\sqrt{MSE}=\\sqrt{64.8}=7.93\n\\]\nNote that it is standard to denote estimated values using â€œhatsâ€. The â€œestimateâ€ column is where we find the values for the estimated regression coefficients \\(\\hat{\\beta_0}\\) and \\(\\hat{\\beta_1}\\). â€œRMSEâ€ (â€œroot mean square errorâ€) is the estimated standard deviation of the errors, assumed to have come from a normal distribution. Compare these results to the values used to generate our fake data:\n\\[\n\\begin{align}\n&\\beta_0=10 &\\beta_1=0.7 \\quad &\\sigma=8 \\\\\n&\\hat{\\beta_0}=9.099 &\\hat{\\beta_1}=0.712 \\quad &\\hat{\\sigma}=7.93 \\\\\n&s_{\\hat{\\beta_0}}=1.776 &s_{\\hat{\\beta_1}}=0.034\n\\end{align}\n\\]"
  },
  {
    "objectID": "Ch2_Model_Building.html#the-r2-statistic",
    "href": "Ch2_Model_Building.html#the-r2-statistic",
    "title": "2Â  Chapter 2: Model building with linear regression",
    "section": "2.5 The \\(R^2\\) statistic",
    "text": "2.5 The \\(R^2\\) statistic\nNote that the output tells us \\(R^2=0.874\\). This is a statistic quantifying how well this model can â€œpredictâ€ the data used to fit it. It is found from the â€œsum of squaresâ€ values in the ANOVA table, Generically:\n\\[\nR^2=\\frac{\\text{model sum of squares}}{\\text{total sum of squares}}=\\frac{\\text{model sum of squares}}{\\text{model sum of squares + residual sum of squares}}\n\\]\n\n\n\n\n\n\nNote\n\n\n\nâ€œSums of squaresâ€ are used to quantify variance. You can think of this as being short for â€œsum of the squared distances between some values and their meanâ€. For example, the variance statistic is\n\\[\ns^2=\\frac{\\sum_{i=1}^n (y_i-\\bar{y})}{n-1}\\\n\\] The numerator, \\(\\sum_{i=1}^n (y_i-\\bar{y})\\), is a â€œsum of squaresâ€ - the sum of the squared deviations between all the values of \\(y_i\\) and their mean, \\(\\bar{y}\\).\n\n\nIn this example:\n\\[\nR^2=\\frac{29162}{29162+4211}=0.874\n\\]\nSo, in this case, \\(87.4\\%\\) of the total variance in \\(Y\\) can be accounted for using the values of \\(x\\). Hereâ€™s the data again, with the estimated regression line added:\n\n\nThe total variance in \\(Y\\) quantifies how much the data vary vertically around the horizontal red line, which is the mean of \\(Y\\).\nThe residual (or â€œerrorâ€) variance quantifies how much the data vary vertically around the regression line.\n\nHere, the data are relatively much closer to the regression line than to the the horizontal mean line, and so residual sum of squares is only a small portion of the total sum of squares, making \\(R^2\\) fairly large.\nAn alternative interpretation of \\(R^2\\) is that is quantifies the proportional decrease in residual variance when using the regression line rather than using only the mean of \\(Y\\).\nLooking at the plot above, you can imagine drawing vertical lines from each data point to the horizontal red line representing the mean of \\(Y\\). If you squared these lines and added them up, youâ€™d have the total sum of squares, which would also be the residual sum of squares if you were using only the mean of \\(Y\\) to calculate residuals. In this case, using the regression line instead of just the mean to calculate residuals would represent an \\(87.4\\%\\) decrease in residual sum of squares.\nSaid differently, \\(R^2\\) tells you how much better your predictions for \\(Y\\) would be if you use the regression line rather than only the mean.\n\n2.5.1 There is no â€œgoodâ€ or â€œbadâ€ value for \\(R^2\\)\nWhen residual variance in \\(Y\\) is larger, \\(R^2\\) is smaller. Visually, when the data are more spread out around the regression line, \\(R^2\\) is smaller. Is this â€œbadâ€? I want you to resist such an interpretation. A small \\(R^2\\) tells you that \\(Y\\) is being influenced by a lot more than just what is in your model. And this is often to be expected.\nFor instance, if Iâ€™m trying to predict how many tomatoes are produced per tomato plant in different parts of the country and my only predictor variable is average daily outdoor temperature, I should not expect a large \\(R^2\\). This is because there are many many more variables that influence how many tomatoes will grow (e.g.Â properties of soil, watering schedule, fertilizer, pestsâ€¦). But, a small \\(R^2\\) should not be interpreted as â€œaverage daily outdoor temperature doesnâ€™t matter when growing tomatoesâ€. It should be interpreted as â€œthere are way more other things that matter when growing tomatoes, and their combined influence is much greater than average daily outdoor temperature aloneâ€."
  },
  {
    "objectID": "Ch2_Model_Building.html#sums-of-squares-and-mean-squares",
    "href": "Ch2_Model_Building.html#sums-of-squares-and-mean-squares",
    "title": "2Â  Chapter 2: Model building with linear regression",
    "section": "2.6 Sums of squares and mean squares",
    "text": "2.6 Sums of squares and mean squares\nâ€¢ We will look at some formulas in this section. Some are based on sums of squares, which are reported in the ANOVA table.\nâ€¢ Total sum of squares quantifies total variability in \\(y\\):\n\\[\nSS_{Total} = \\sum^n_{i=1}(y_i - \\bar{y})^2\n\\]\nâ€¢ Note that this has nothing to do with the regression line, or the predictor variable. It quantifies variability in the response variable alone.\n\n2.6.1 Total sum of squares\nâ€¢ Visually, \\(SS_{Total}\\) is the sum of the squared vertical deviations between each data point and the mean of ğ‘¦, shown here as a horizontal line.\n\n\n\n\n\nâ€¢ The two blue lines drawn are two such instances of these deviations. If we drew these for every data point, squared them, and added them up, weâ€™d have \\(SS_{Total}\\)\nâ€¢ Again, note that this quantity has nothing to do with the regression line!\n\n\n2.6.2 Residual / Error sum of squares\nâ€¢ Error sum of squares quantifies total variability in ğ‘¦ around the regression line:\n\\[\nSS_{Error} = \\sum^n_{i=1}(y_i - \\hat{y}_i)^2 = \\sum^n_{i=1}[y_i - (\\hat{\\beta}_0 + \\hat{\\beta}_1)]^2\n\\]\nâ€¢ This is also known as the â€œsum of the squared residualsâ€, where a residual is the vertical distance between a data point and the regression line. The values of \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\) are chosen so as to minimize \\(SS_{Error}\\).\nâ€¢ In other words, the regression line drawn through the data produced a smaller \\(SS_{Error}\\) than any other line we could possibly draw.\nâ€¢ Visually, \\(SS_{Error}\\) is the sum of the squared vertical deviations between each data point and the regression line.\n\n\n\n\n\nâ€¢ Here, the blue lines are two instances of these deviations\nâ€¢ \\(SS_{Error}\\) will be larger when the data are more spread out around the line, and vice versa.\n\n\n2.6.3 Mean square error\nâ€¢ Error mean square (aka mean square error) is given by\n\\[\nMSE = MS_{Error} = \\frac{SS_{Error}}{n-k +1}\n\\]\nâ€¢ \\(k\\) is the number of predictor variables. Example: for simple regression, \\(k = 1\\), so \\(MSE = \\frac{SS_{Error}}{N-2}\\)\nâ€¢ As seen earlier, \\(\\sqrt{MSE}\\) is the estimate for the standard deviation of the residuals around the line: \\(\\hat{\\sigma} = \\sqrt{MSE}\\)\n\n\n2.6.4 Model sum of squares\nâ€¢ Model sum of squares (aka â€œregression sum of squaresâ€) is given by:\n\\[\nSS_{Model} = \\sum^n_{i=1}(\\hat{y}_i - \\bar{y})^2\n\\]\nâ€¢ This can be thought of as quantifying how much better the model is than \\(\\bar{y}\\) alone at accounting for variation in the values of \\(y\\).\nâ€¢ Visually, \\(SS_{Model}\\) is the sum of the squared vertical deviations between the regression line and the horizontal line, at each value of the predictor variable.\n\n\n\n\n\nâ€¢ Here, the blue lines are two instances of these deviations, associated with the circled blue data points.\n\n2.6.4.1 \\(SS_{Total} = SS_{Error} + SS_{Model}\\)\nâ€¢ Total sum of squares are equal to the sum of error sum of squares and model sum of squares.\nâ€¢ Note that this also implies:\n\\[\nSS_{Model} = SS_{Total} - SS_{Error} \\\\\nSS_{Error} = SS_{Total} - SS_{Model}\n\\]"
  },
  {
    "objectID": "Ch2_Model_Building.html#interpreting-regression-results",
    "href": "Ch2_Model_Building.html#interpreting-regression-results",
    "title": "2Â  Chapter 2: Model building with linear regression",
    "section": "2.7 Interpreting regression results",
    "text": "2.7 Interpreting regression results\nâ€¢ Here again is the estimated model from the previous example:\n\\[\n\\hat{y}_i = 9.1 + 0.71x_i\n\\]\nâ€¢ The intercept, \\(\\hat{\\beta}_0 = 9.1\\), gives the predicted value of the response variable (\\(y\\)) when the predictor variable (\\(x\\)) equals zero. This is not typically of practical interest.\nâ€¢ The slope, \\(\\hat{\\beta}_1 = 0.71\\), gives the predicted change in \\(y\\) for a one unit increase in \\(x\\).\n\n2.7.0.1 More on interpreting the slope\nâ€¢ The slope is often of practical interest. It tells us how much the response variable changes, on average, when the predictor variable increases by one unit.\nâ€¢ This interpretation is very common. It is also dangerous, because it is phrased in a way that suggests changes in ğ‘¥ cause changes in \\(y\\).\nâ€¢ Here is an alternate, non-causal sounding interpretation:\nIf we observe two values of \\(x\\) that are one unit apart, we estimate that their corresponding average \\(y\\) values will be \\(\\hat{\\beta}_1\\) units apart.\nâ€¢ Visually, we can choose two values of \\(x\\), go up to the line, and record the values of \\(y\\). The slope tells us how much these \\(y\\) values are expected to differ.\n\n\n\n\n\nâ€¢ Here, when we compare \\(x = 20\\) and \\(x = 80\\), we expect their corresponding \\(y\\) values to differ by:\n\\[\n(80 âˆ’ 20) âˆ— 0.725 = 43.5\n\\]\n\n\n2.7.0.2 Inference on the coefficients\nâ€¢ For each estimated coefficient (\\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_0\\)), jamovi reports a standard error, along with a t-test statistic and p-value testing the null that the parameter being estimated equals zero.\n\n\n\n\n\nâ€¢ Example: the above output shows the test of \\(H_0: \\beta_1 = 0\\)\n\\[\nt = \\frac{\\hat{\\beta}_1}{s_{\\hat{\\beta}_1}} = \\frac{0.712}{0.034} = 21.22\\\\\np-value &lt; 0.01 \\\\\n\\text{\"reject } H_0\"\n\\]\nâ€¢ We can use these results to create an approximate 95% confidence for \\(\\beta_1\\):\n\\[\n95\\% \\text{ CI for } \\beta_1 \\approx \\hat{\\beta}_1 \\pm 2* s_{\\hat{\\beta}_1} = 0.712 \\pm 2*0.034 = (0.645,0.779)\n\\]\nâ€¢ This is a very narrow interval, suggesting a â€œpreciseâ€ estimate of \\(\\beta_1\\)\nâ€¢ For both the hypothesis test and 95% CI, the results depend on how large the estimate of \\(\\beta_1\\) is, relative to its standard error.\nâ€¢ In this case, \\(\\hat{\\beta}_1\\) is very large relative to \\(s_{\\hat{\\beta}_1}\\), so the result is â€œhighly significantâ€ and the 95% CI is narrow.\n\n\n2.7.0.3 Formulas for \\(\\hat{\\beta}_1\\) and \\(s_{\\hat{\\beta}_1}\\)\nâ€¢ Now that youâ€™ve seen how \\(\\hat{\\beta}_1\\) and \\(s_{\\hat{\\beta}_1}\\) are used, here are their formulas:\n\\[\n\\hat{\\beta}_1 = r_{xy} * \\frac{s_y}{s_x} \\\\\ns_{\\hat{\\beta}_1} = \\sqrt{\\frac{MSE}{\\sum^n_{i=1}(x_i - \\bar{x})^2}} = \\sqrt{\\frac{1-R^2}{n-k-1}}*\\frac{s_y}{s_x}\n\\]\nâ€¢ \\(\\hat{\\beta}_1\\) is larger when the correlation between \\(x\\) and \\(y\\) is stronger, and when the variability in \\(y\\) is larger relative to the variability in \\(x\\).\nâ€¢\\(s_{\\hat{\\beta}_1}\\) is smaller when \\(R^2\\) is larger, when \\(n\\) is larger, and when the variability in \\(x\\) is larger relative to the variability in \\(y\\)."
  },
  {
    "objectID": "Ch2_Model_Building.html#applied-example-the-binary-bias",
    "href": "Ch2_Model_Building.html#applied-example-the-binary-bias",
    "title": "2Â  Chapter 2: Model building with linear regression",
    "section": "2.8 Applied example: â€œThe binary biasâ€",
    "text": "2.8 Applied example: â€œThe binary biasâ€\nâ€¢ There is a paper up on Canvas, titled â€œThe Binary Bias: A Systematic Distortion in the Integration of Informationâ€. This is a 2018 paper published in Psychological Science with open data.\nâ€¢ The overall hypothesis is that people tend to assess continuous information using binary thinking. The experiments all involve testing to see if participants will give higher or lower assessments of where an average lies, based on the â€œimbalanceâ€ of the data: i.e.Â the comparative frequency with which very low or very high values turn up.\nâ€¢ Here is the section of the paper describing the results of Study 1a:\n\n\n\n\n\nâ€¢ The data:\n\n\n\n\n\nâ€¢ Telling jamovi to fit the model:\n\n\n\n\n\nâ€¢ The model, before being fit to the data:\n\\[\nRecorded_i = \\beta_0 + \\beta_1Imbalance_i + \\beta_2Mode_i + \\beta_3First_i + \\beta_4Last_i + \\epsilon_i\n\\]\nâ€¢ The results in jamovi:\n\n\n\n\n\n\n\n\n\n\nâ€¢ Note that the estimated slope is being denoted as \\(b\\) rather than as \\(\\hat{\\beta}_1\\). \\(\\hat{\\sigma} = MSE\\). Note also how the 95% CI was created: \\(\\approx 4.62 Â± 2 âˆ— 0.63\\)\nâ€¢ The paper does not mention \\(R^2\\). We can see it in the jamovi output, and calculate it from the ANOVA table:\n\n\n\n\n\nâ€¢ \\(R^2 = 0.12\\). So, about 12% of the total variance in â€œRecordedâ€ is being â€œexplainedâ€ or â€œaccounted forâ€ in the model.\nINSERT IMAGE\nâ€¢ In multiple regression, each predictor is interpreted under the assumption that the values of all other predictors are held constant (i.e.Â â€œcontrolled forâ€).\nâ€¢ So, if we were to observe two participants who were equal in terms of â€œFirstâ€, â€œLastâ€, and â€œModeâ€, but were one unit apart in terms of â€œImbalanceâ€, we would expect their values for the response variable (â€œRecordedâ€) to differ by 4.62 units on average.\nâ€¢ Or: The predicted (or average) difference in Recorded associated with a one unit difference in Imbalance is 4.62 units, if all other predictors are held constant."
  },
  {
    "objectID": "Ch2_Model_Building.html#interaction",
    "href": "Ch2_Model_Building.html#interaction",
    "title": "2Â  Chapter 2: Model building with linear regression",
    "section": "2.9 Interaction",
    "text": "2.9 Interaction\nâ€¢ â€œInteractionâ€ is the phenomenon by which the association between a predictor variable and a response variable is itself dependent on the value of another predictor.\nâ€¢ Say we have response ğ‘¦, one predictor ğ‘¥1, and another predictor ğ‘¥2. We say that ğ‘¥1 and ğ‘¥2 interact if the amount of change in ğ‘¦ associated with a change in ğ‘¥1 is different for different values of ğ‘¥2, or vice versa.\nâ€¢ You can think of this as saying that the â€œslopeâ€ of ğ‘¥1 depends upon the value of ğ‘¥2.\nâ€¢ Another way of saying it: if the answer to the question:\nâ€œHow much does our estimate for ğ‘¦ change when ğ‘¥1 changes?â€\nis:\nâ€œIt depends on the value of ğ‘¥2â€,\nthen ğ‘¥1 and ğ‘¥2 interact.\n\n2.9.1 Interaction example\nâ€¢ Suppose a drug for treating rheumatoid arthritis is more effective at reducing inflammation for younger patients than it is for older patients.\nâ€¢ If we conduct an experiment in which inflammation is the response variable and the predictors are treatment group (drug vs.Â control) and age, then we expect treatment group and age to interact.\nâ€¢ This is different from saying that age and treatment both affect inflammation. It means that the extent to which treatment affects inflammation is different for patients of different age.\nâ€¢ Sometimes interaction is referred to as â€œmoderationâ€. This is common in the social and behavioral sciences, particularly Psychology.\nâ€¢ So, a â€œmoderatorâ€ variable is one that changes how the primary predictor of interest relates to the response.\nâ€¢ Example: suppose an experiment shows that subjects holding a pen with their teeth rate cartoons as funnier vs.Â subjects holding a pen with their lips.\nâ€¢ Suppose also that this â€œpen in teethâ€ effect disappears if subjects see a video camera in the room.\nâ€¢ In this case, the presence of the video camera â€œmoderatesâ€ the effect of the pen on cartoon ratings. In the language of interaction, the presence of the pen and the presence of the video camera â€œinteractâ€.\nThis is based on a real study that has generated controversy, see: \n\n\n2.9.2 Interaction, in the regression model\nâ€¢ Mathematically, we create an interaction variable by multiplying predictor variables by one another.\nâ€¢ So, if we want to allow ğ‘¥1 and ğ‘¥2 to interact, we simply make a new variable defined as ğ‘¥1 âˆ— ğ‘¥2.\nâ€¢ This interaction variable will be used as an additional predictor variable in the regression model:\nğ‘Œğ‘–= ğ›½0 + ğ›½1ğ‘¥1ğ‘– + ğ›½2ğ‘¥2ğ‘– + ğ›½3 ğ‘¥1ğ‘¥2 ğ‘– + ğœ€ğ‘–, ğœ€ğ‘–~ğ‘ğ‘œğ‘Ÿğ‘šğ‘ğ‘™(0, ğœ)\nâ€¢ The interaction coefficient is ğ›½3 in this model:\nğ‘Œğ‘– = ğ›½0 + ğ›½1ğ‘¥1ğ‘– + ğ›½2ğ‘¥2ğ‘– + ğ›½3 ğ‘¥1ğ‘¥2 ğ‘– + ğœ€ğ‘–\nğœ€ğ‘–~ğ‘ğ‘œğ‘Ÿğ‘šğ‘ğ‘™(0, ğœ)\nâ€¢ To interpret this, letâ€™s look at how it affects the coefficients (aka slopes) for ğ‘¥1 and ğ‘¥2.\nâ€¢ We can think of the â€œslopeâ€ of a predictor as everything it is being multiplied by.\nğ‘Œğ‘– = ğ›½0 + ğ›½1ğ‘¥1ğ‘– + ğ›½2ğ‘¥2ğ‘– + ğ›½3 ğ‘– + ğœ€ğ‘–\nâ€¢ Factoring out ğ‘¥1 from the above regression equation gives:\nğ‘Œğ‘– = ğ›½0 + (ğ›½1+ğ›½3ğ‘¥2ğ‘–)ğ‘¥1ğ‘– + ğ›½2ğ‘¥2ğ‘– + ğœ€ğ‘–\nâ€¢ Similarly, factoring out ğ‘¥2 gives:\nğ‘Œğ‘– = ğ›½0 + (ğ›½2+ğ›½3ğ‘¥1ğ‘–)ğ‘¥2ğ‘– + ğ›½1ğ‘¥1ğ‘– + ğœ€ğ‘–\nâ€¢ So, for this model, the â€œslopeâ€ of ğ‘¥1 is ğ›½1 + ğ›½3ğ‘¥2, and the â€œslopeâ€ of ğ‘¥2 is ğ›½2 + ğ›½3ğ‘¥1\nâ€¢ In other words, the slope of ğ‘¥1 depends on the value of ğ‘¥2, and vice versa. For different values of ğ‘¥2, the â€œpredicted change in ğ‘¦ for a one unit increase in ğ‘¥1â€ (i.e.Â the slope of ğ‘¥1) will be different.\nâ€¢ A simpler way of saying this is that, if two predictors interact, then the effect of one predictor on the response depends on the value of the other predictor.\n(This is a simpler interpretation, but also potentially misleading in that the term â€œeffectâ€ sounds causal. Nonetheless it is commonly used language when interpreting slopes.)"
  },
  {
    "objectID": "Ch2_Model_Building.html#jamovi-example-arthritis-data",
    "href": "Ch2_Model_Building.html#jamovi-example-arthritis-data",
    "title": "2Â  Chapter 2: Model building with linear regression",
    "section": "2.10 jamovi Example: Arthritis data",
    "text": "2.10 jamovi Example: Arthritis data\nâ€¢ The data set â€œarthritis_data.csvâ€ contains simulated data from a (fictional) Randomized Control Trial comparing treatments for inflammation from rheumatoid arthritis: a disease-modifying anti-rheumatic drug (DMARD) vs.Â a non-steroidal anti-inflammatory drug (NSAID)\nâ€¢ The variables are:\nâ€¢ Drug: indicator (0 / 1) variable; 1 = DMARD, 0 = NSAID â€¢ Before: Inflammation scan score prior to treatment (scale: 0 to 50) â€¢ After: Inflammation scan score six months after treatment â€¢ Difference: Difference in scores, before minus after. (Note that larger values\nBefore vs.Â after scatterplot\nâ€¢ We will fit some regression models using this data, but first letâ€™s take a look at our data using the Scatterplot module.\nâ€¢ Here is â€œafterâ€ vs.Â â€œbeforeâ€, with a linear regression line superimposed. Note that most patients had greater inflammation before than after treatment.\nDifferences by treatment type\nâ€¢ Note that the differences tend to be larger for the DMARD group: this corresponds to a greater reduction in inflammation.\nâ€¢ Use Descriptives to create the boxplot. Select difference as a Variable and Split by drug. Then select under Plots â€“ Box plot and Data Jittered to superimpose data.\nBefore vs.Â age scatterplot\nâ€¢ Here we see that age is positively correlated with inflammation before the drug trial.\nâ€¢ jamovi gives options to superimpose regression output on a scatterplot.\nâ€¢ jamovi can also plot â€œstandard errorâ€ bands around the line. These show the standard error for the average value of y (â€œbeforeâ€), given x (â€œageâ€).\nDifference vs.Â before\nWe donâ€™t see an association between amount of inflammation before treatment and reduction in inflammationâ€¦\nâ€¦ but maybe we do if we add in drug! Do â€œdrugâ€ and â€œbeforeâ€ interact here?\nDifference vs.Â age\nWe see a small negative correlation between difference and ageâ€¦\nâ€¦ but when we add drug, we see no correlation for the NSAID and clear negative correlation for the DMARD. Definite interaction!\nNow with model outputâ€¦\nâ€¢ The following slides show the plots again, plus the regression output JMP produces. First up, Difference vs.Â Drug, using t-test:\nâ€¢ And using regression:\nOMG! The estimated slope for â€œdrugâ€ is the same as the difference in means between the drugs!\nBefore vs.Â age\nDifference vs.Â age, with interaction\nâ€¢ Here, we are fitting the model:\nğ·ğ‘–ğ‘“ğ‘“ğ‘’ğ‘Ÿğ‘’ğ‘›ğ‘ğ‘’ğ‘– = ğ›½0 + ğ›½1ğ‘ğ‘”ğ‘’ğ‘– + ğ›½2ğ‘‘ğ‘Ÿğ‘¢ğ‘”ğ‘– + ğ›½3 ğ‘– + ğœ€ğ‘–\nâ€¢ To do this, create a new column in jamovi, defined as agedrug. May as well label it â€œagedrugâ€.\nDifference vs.Â age, with drug interaction\nâ€¢ Notice that the slope of â€œageâ€ by itself is for when drug = 0. The age*treatment interaction shows how much the slope of â€œageâ€ changes when treatment = 1.\nğ‘‘ğ‘–ğ‘“ğ‘“à·£ğ‘’ğ‘Ÿğ‘’ğ‘›ğ‘ğ‘’ = 2.0114 + 0.0125 + 15.1586 ğ‘‘ğ‘Ÿğ‘¢ğ‘” âˆ’0.2493(ğ‘ğ‘”ğ‘’ âˆ— ğ‘‘ğ‘Ÿğ‘¢ğ‘”)\nâ€¢ Slope of â€œageâ€ when drug = 1 is: 0.0125 âˆ’ 0.2493 = âˆ’0.2368\nDifference vs.Â age, with drug interaction\nâ€¢ Notice also that the slope of â€œageâ€ by itself is nowhere close to being statistically significant (the estimate is less than half the standard error), but the slope of the interaction is highly significant (the estimate is 6 times as large as the standard error)\nâ€¢ Now take a look at the plot. There is a clear negative correlation between age and difference when drug = 1. There is essentially none when drug = 0.\nDifference vs.Â age, with drug interaction\nâ€¢ Another way of thinking about this interaction:\nâ€¢ If we ask the question: â€œhow does inflammation reduction differ by age?â€, the answer is â€œit depends on which drug the patient took.â€\nâ€¢ Similarly, if we ask the question: â€œhow does inflammation reduction differ by drug?â€, the answer is â€œit depends on the\nâ€¢ In this example, we see a clear interaction between drug and age: the slope for age is negative for DMARD and flat for NSAID.\nâ€¢ Another way of thinking about this: DMARD appears to be more effective for younger patients than for older patients. NSAID appears to be equally effective regardless of age.\nâ€¢ HOWEVER â€“ this does NOT mean that treatment and age are correlated!\nâ€¢ This should make sense, after all patients were randomly assigned to one of the two drugs. If drug were correlated with age, there would be bias in this study. The whole point of randomization is to remove correlations!\nâ€¢ Just to confirm, here is the distribution of age, split by drug:\nâ€¢ Again, age and drug â€œinteractâ€ when it comes to their associations with the differences in inflammation scores: the association between â€œageâ€ and â€œdifferenceâ€ is different for the two different drugs.\nâ€¢ Similarly, the association between â€œdrugâ€ and â€œdifferenceâ€ is different for patients of different ages."
  },
  {
    "objectID": "Ch2_Model_Building.html#centering-predictor-variables",
    "href": "Ch2_Model_Building.html#centering-predictor-variables",
    "title": "2Â  Chapter 2: Model building with linear regression",
    "section": "2.11 Centering predictor variables",
    "text": "2.11 Centering predictor variables\nğ‘‘ğ‘–ğ‘“ğ‘“à·£ğ‘’ğ‘Ÿğ‘’ğ‘›ğ‘ğ‘’ = 2.0114 + 0.0125\n\n15.1586\n\nâˆ’ 0.2493(ğ‘ğ‘”ğ‘’ âˆ— ğ‘‘ğ‘Ÿğ‘¢ğ‘”)\nâ€¢ There is a serious challenge when interpreting the slope for â€œdrugâ€: this slope is only 15.1586 if ğ‘ğ‘”ğ‘’ = 0. But ğ‘ğ‘”ğ‘’ = 0 is not of interest.\nâ€¢ Plug in mean age (52.096) and see what happens to the slope for drug:\n15.1586 âˆ’ 0.2493 = 15.1586 âˆ’ 0.2493 âˆ— 52.096 = 2.171(ğ‘‘ğ‘Ÿğ‘¢ğ‘”) â€¢ So, for patients at mean age, the predicted difference in inflammation is 2.171 units greater under the DMARD than under the NSAID\nâ€¢ This process of estimating slopes at the average value of predictors can be done via â€œcenteringâ€\nâ€¢ Centering means subtracting the mean from a variableâ€™s distribution.\nâ€¢ In this example, ğ‘ğ‘’ğ‘›ğ‘¡ğ‘’ğ‘Ÿğ‘’ğ‘‘ ğ‘ğ‘”ğ‘’ = ğ‘ğ‘”ğ‘’ âˆ’ ğ‘ğ‘”ğ‘’ = ğ‘ğ‘”ğ‘’ â€“ 52.096\nâ€¢ This is very useful when using interaction terms in a regression model.\nâ€¢ To center age, create a new columns called â€œcentered ageâ€, defined as age â€“ VMEAN(age).\nâ€¢ Create another new column for â€œcentered drugâ€, defined as drug â€“ VMEAN(drug). Now create a final column defined as centered drug * centered age, call it â€œcentered age*drugâ€\nâ€¢ We canâ€™t use MEAN() to center variables in jamovi since it works across variables, one row at a time. What we want is to take the overall mean of a variable and subtract it from each measurement.\nâ€¢ So, use VMEAN() to center variables in jamovi.\nâ€¢ Compare the new results (on the left) to the results we saw when using the interaction term we created manually (on the right)\n\n2.11.1 Centered interaction in jamovi\nâ€¢ The â€œModel Fit Measuresâ€ are identical. Centering has no effect on ğ‘…2 or any sums of squares.\nâ€¢ Centering also had no effect on the slope for the interaction, or on its standard error. But, look at the individual â€œageâ€ and â€œdrugâ€ predictors.\nâ€¢ These slopes are different, because they are being calculated at the mean value of the other.\nâ€¢ Centering also reduces the standard\nâ€¢ Look at the slope for drug when using a centered interaction: itâ€™s the same value we calculated by plugging the mean of age into the interaction term in the non-centered model!\nâ€¢ The slope for age in the centered model is harder to interpret. It is calculated at the â€œaverageâ€ for drug, which doesnâ€™t make real world sense.\nOnly centering one predictor in jamovi\nâ€¢ It would be best if we could center â€œageâ€ but not drug.\nâ€¢ But we already created centered age when we created the centered interaction.\nâ€¢ Now we need to create the new interaction where only age is centered. Create a new column and define it as â€œdrug * centered ageâ€.\nOnly centering one predictor in JMP\nâ€¢ Here are the results. Compare them to the previous two versions:\nâ€¢ Only age centered:\nâ€¢ Age and drug centered in the interaction:\nâ€¢ Nothing centered:\nâ€¢ First, the slope for the interaction is the same in all three models.\nâ€¢ Second, the slope for drug is the same in both centered models, but different in the non-centered model. It is the centering of age in the interaction that changed the slope for drug.\nâ€¢ Third, the slope for age is the same in both models for which drug is not centered. This allows us to interpret it as before: slope for age is 0.0125 for the NSAID and is 0.0125 âˆ’ 0.2493 = âˆ’0.2368 for the DMARD\nâ€¢ Fourth, the standard error for drug is substantially smaller when age is centered in the interaction. This is typically the case when centering a continuous variable in an interaction.\nâ€¢ Finally, the intercept is different in all three models.\nâ€¢ Normally we donâ€™t care about the intercept, but centering allows the intercept to be meaningfully interpreted.\nâ€¢ Since\nğ¶ğ‘’ğ‘›ğ‘¡ğ‘’ğ‘Ÿ\nğ‘ğ‘”ğ‘’ = ğ‘ğ‘”ğ‘’ â€“ ğ‘ğ‘”ğ‘’, [ğ¶ğ‘’ğ‘›ğ‘¡ğ‘’ğ‘Ÿ]ğ‘ğ‘”ğ‘’ = 0 when ğ‘ğ‘”ğ‘’ = ğ‘ğ‘”ğ‘’\nâ€¢ Remember that the intercept is interpreted as the â€œpredicted value of the response when the predictors equal zeroâ€.\nâ€¢ So, when centering, the intercept is the predicted value of the response when the centered predictors equal their mean.\nâ€¢ Going back to our example, the predicted reduction in inflammation (before minus after) for a patient at the average age in our data set who got the NSAID is 2.665.\nâ€¢ The predicted reduction in inflammation for a patient at the average age who got the DMARD is 2.665 + 2.171 = 4.836"
  },
  {
    "objectID": "Ch2_Model_Building.html#standardizing-predictor-variables",
    "href": "Ch2_Model_Building.html#standardizing-predictor-variables",
    "title": "2Â  Chapter 2: Model building with linear regression",
    "section": "2.12 Standardizing predictor variables",
    "text": "2.12 Standardizing predictor variables\nâ€¢ We will briefly consider an extension on centering: standardizing.\nâ€¢ Recall from your introductory statistics course the standardization â€œzâ€ formula:\nğ‘§ = ğ‘¥âˆ’ğœ‡, or in words: ğ‘§ = ğ‘£ğ‘ğ‘™ğ‘¢ğ‘’ âˆ’ğ‘šğ‘’ğ‘ğ‘› ğœ ğ‘ ğ‘¡ğ‘ğ‘›ğ‘‘ğ‘ğ‘Ÿğ‘‘ ğ‘‘ğ‘’ğ‘£ğ‘–ğ‘ğ‘¡ğ‘–ğ‘œğ‘›\nâ€¢ To center a variable, we subtract the mean. To standardize, we subtract the mean and then divide by the standard deviation.\nWhy standardize?\nâ€¢ Just like with a centered variable, the mean of a standardized variable is zero. So, standardizing has all the same benefits as centering when it comes to interpretation of interactions.\nâ€¢ Standardizing has an additional potential benefit: the slope can be interpreted as the predicted change in Y for a one standard deviation increase in X (while holding all other predictors constant).\nâ€¢ Z values are interpreted as â€œnumber of standard deviations from the meanâ€. And so increasing Z by 1 implies increasing X by 1 standard deviation.\nâ€¢ To standardize in jamovi we will need to create a new column defined by (age â€“ VMEAN(age)) / VSTDEV(age). â€¢ We will also need to create a column for the new interaction and define it as drug * Standardized age\nâ€¢ Here are the results when age is standardized:\nâ€¢ Compare to the results when age is centered:\nâ€¢ Note that the intercepts are the same. In both cases, the predicted reduction in inflammation for a patient at average age getting NSAID is 2.665. Likewise, in both cases this prediction is 4.836 for DMARD.\nâ€¢ What has changed is the slope for terms involving age. Now, a one unit increase in standardized age is a one standard deviation increase in age.\nâ€¢ So, for NSAID, the predicted difference in inflammation reduction for two people whose ages are one standard deviation apart is 0.14. For DMARD, it is 0.14 â€“ 2.79 = -2.64.\nâ€¢ How much is a standard deviation? We can look it upâ€¦\nInteraction and centering / standardizing summary\nâ€¢ This has been a long example. I encourage you to load up the data yourself and play around with it in jamovi. At the minimum, make sure you can re-create the results in these notes.\nâ€¢ The most important take-aways:\nâ€¢ Interaction terms allow the slope of predictor to change for different values of another predictor.\nâ€¢ Centering helps make regression coefficients (slopes and intercepts) more interpretable. Standardizing allows you to interpret them in terms of one standard deviation changes, rather than â€œone unitâ€ changes."
  },
  {
    "objectID": "Ch3_Model_Fit.html#part-1-assumptions-and-assumption-violations",
    "href": "Ch3_Model_Fit.html#part-1-assumptions-and-assumption-violations",
    "title": "3Â  Chapter 3: Assessing and improving model fit",
    "section": "3.1 Part 1: assumptions and assumption violations",
    "text": "3.1 Part 1: assumptions and assumption violations"
  },
  {
    "objectID": "Ch3_Model_Fit.html#outline-of-notes",
    "href": "Ch3_Model_Fit.html#outline-of-notes",
    "title": "3Â  Chapter 3: Assessing and improving model fit",
    "section": "3.2 Outline of notes:",
    "text": "3.2 Outline of notes:\n\nRegression assumptions\nLinearity\nNormality of residuals\nHomogeneity of variance\nInfluential observations\n(Multi)collinearity\n\n\n3.2.1 Violating model assumptions\nâ€¢ The previous notes outlined the fundamentals of specifying, fitting, and interpreting a linear regression model.\nâ€¢ These notes focus on the assumptions that our models are based upon, and how we check to see if they are being violated.\nâ€¢ If model assumptions are violated, DONâ€™T PANIC! There is nearly always a remedy. In these notes we will look at how to spot violations of assumptions. In the next set of notes we will look at ways to remedy these violations, and how to decide if they pose a serious problem to the usefulness of the model.\n\n\n3.2.2 The regression model and what it assumes\nâ€¢ Once again, here is the regression model:\n\\[\nY_i = \\beta_0 + \\beta_1x_{1i} + \\beta_2x_{2i} + \\dots + \\beta_px_{pi} + \\epsilon_i, \\text{ where } \\epsilon_i \\sim Normal(0,\\sigma)\n\\]\n\nThis assumes:\n\nThat the response variable is a linear (straight line) function of the predictor variables\nThat the residuals will be normally distributed\nThat the standard deviation of the residuals does not vary\nThat the residuals are independent\n\n\n\n\n3.2.3 Linearity\nâ€¢ Remember the â€œsimpleâ€ (i.e.Â single predictor) regression model\n\\[\nY_i = \\beta_0 + \\beta_1x_{i} + \\epsilon_i, \\text{ where } \\epsilon_i \\sim Normal(0,\\sigma)\n\\]\nâ€¢ This is linear in that it fits a straight line to the two-dimensional data.\nâ€¢ A two-predictor model would fit a flat plane to the three-dimensional data, and so on\nâ€¢ Hereâ€™s a bad idea: fitting a linear model to non-linear data!\n\n\n\n\n\n\n\n3.2.4 Diagnosing non-linearity\nâ€¢ When running â€œLinear Regressionâ€ in jamovi, a â€œresiduals by predictedâ€ plot can be created by selecting â€œResidual plotsâ€ under â€œAssumption Checksâ€\nâ€¢ The residuals are the differences between each observed values of the response variable and the value that the model predicts:\n\\[\nresidual_i = y_i - \\hat{y}_i = y_i - (\\hat{\\beta}_0 + \\hat{\\beta}_1x_1 + \\hat\\beta_2x_2 + \\dots)\n\\]\n\n\n\n\n\nâ€¢ For simple regression, this plot just looks like the regression plot with the line turned horizontally.\nâ€¢ For multiple regression, there is no (two dimensional) â€œregression plotâ€, so the residual plot will be very useful!\n\n\n\n\n\nâ€¢ In this example, there is clear curvature in the data. A straight line model is not appropriate.\nâ€¢ Hereâ€™s an example of what a linear relationship might look like:\n\n\n\n\n\nâ€¢ When there is non-linearity, you will see the residuals mostly on one side of zero, then on the other size of zero, then back again, and so on.\nâ€¢ When there is linearity, the residuals should randomly fall on either side of zero.\n\n\n\n\n\n\n\n3.2.5 What to look for in a residual plot\nâ€¢ We will look at many more examples of residual plots in these notes.\n\nWe want a residual plot that appears to agree with the model assumptions:\n\nStraight line relationship between the predictors and response\nNormally distributed random residuals around this line\nEqual variance in residuals across line\n\n\n\n\n3.2.6 Normality of residuals\nâ€¢ The â€œerror termâ€ in a regression model is that \\(+ \\epsilon_i\\) on the end\nâ€¢ When we write \\(\\epsilon_i \\sim Normal(0, \\sigma)\\), we are saying that the errors (aka residuals) are normally distributed, with mean zero and some standard deviation \\(\\sigma\\).\nâ€¢ This can be assessed visually: run the regression plot the residuals to see if they appear roughly normally distributed.\n\n3.2.6.1 The QQ plot\nâ€¢ When fitting a model using â€œLinear Regressionâ€ in jamovi, there is an option to save residuals. This will create a new column with a residual for each row.\nâ€¢ The option is under the last drop-down menu, under â€œSaveâ€.\n\n\n\n\n\nâ€¢ To create a plot of the residuals, select â€œQ-Q plot of residualsâ€ under â€œAssumption Checksâ€ in â€œLinear Regressionâ€\nâ€¢ The Normal Quantile plot is also known as the QQ plot, for â€œquantile quantileâ€.\nâ€¢ It is easier to assess normality with a QQ plot than with a histogram.\n\n\n\n\n\n\n\n3.2.6.2 Assessing normality with a QQ plot\nâ€¢ On Canvas under Simulations there is a â€œQQ plot generatorâ€ app.\nâ€¢ This app allows you to manipulate a distribution, sample from it, and then view a histogram next to a QQ plot. It is meant to give you an idea of how QQ plots work.\nâ€¢ By default, it draws data from a normal distribution. But, you can add â€œskewednessâ€ or â€œpeakednessâ€ (aka kurtosis) to make the distribution non-normal. You can also adjust sample size.\n\nâ€¢ A QQ plot shows you how much the distribution of your data â€œagreeâ€ with a normal distribution.\n\n\n\n\n\nâ€¢ The horizonal axis gives the distribution data would follow if it were perfectly normal.\nâ€¢ The vertical axis gives the distribution your data actually follows.\nâ€¢ The diagonal line shows perfect agreement between the two.\n\n\n\n\n\n\n\n\n\n\nâ€¢ The big advantage of the QQ plot vs.Â the histogram is that very often data that come from a normal distribution donâ€™t look normal, especially if the sample size is small.\nâ€¢ In this case, the histogram isnâ€™t clearly normal. But, on the QQ plot the data are close to the line.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nâ€¢ Notice that the data still veer from the diagonal line to some extent, even though we know for a fact they came from a normal distribution.\n\n\n3.2.6.3 Limitations of QQ plots\nâ€¢ As you can see from the app, sometimes data that come from a normal distribution donâ€™t sit right on the line.\nâ€¢ Sometimes data that come from a skewed distribution look similar to data that come from a normal distribution\nâ€¢ Itâ€™s easier to assess normality when sample sizes are larger.\nâ€¢ As it turns out, the assumption of normality is not vital to the validity of a regression model. If the QQ plot is vague, youâ€™re probably fine. We only worry when we see extreme non-normality.\n\n\n3.2.6.4 Tests for normality (not recommended)\nâ€¢ There are statistical tests, such as â€œShapiro-Wilksâ€ or â€œKolmgorov-Smirnovâ€, for which the null hypothesis is that the data come from some specified distribution, like the normal.\nâ€¢ Rejecting this null means that the data â€œsignificantlyâ€ disagree with the assumption of normality.\nâ€¢ I do not recommend using this test. The problem is that, when the sample size is large enough, even small deviations from normality will be statistically significant. But small deviations from normality are OK. Only major deviations are concerning.\n\n\n\n3.2.7 The homogeneity of variance assumption\nâ€¢ Back to the error term:\n\\(\\epsilon_i \\sim Normal(0, \\sigma)\\)\nâ€¢ Notice that \\(\\sigma\\) is just one number. This suggests that the standard deviation of the residuals should be the same across all values of predictor variables. In other words, there is homogeneity of variance.\nâ€¢ Another name for this is â€œhomoscedasticityâ€. If this assumption is violated, then we have â€œheteroscedasticityâ€.\n\n3.2.7.1 Heterogeneity of variance\nâ€¢ To show heterogeneity of variance, Iâ€™ll simulate data that is a function of an X variable, plus random values from a normal distribution with standard deviation equal to X.\nâ€¢ Thus, the standard deviation of residuals will get bigger as X gets bigger:\n\n\n\n\n\n\n\n3.2.7.2 The residuals vs fitted plot\nâ€¢ Here is the regression plot and residual plot when this simulated variable (called â€œWâ€ here) is the response and X is the predictor:\n\nâ€¢ Notice that the residuals are more spread out for larger X\nâ€¢ We also see â€œheavy tailsâ€ when plotting the residuals with a histogram and QQ plot:\n\n\n\n\n\nâ€¢ Heavy tails refers to a distribution with outliers on both ends.\nâ€¢ This shows up on the QQ plot as the residuals being too flat in the middle and then curving out on both ends.\n\n\n\n3.2.8 Influential observations (outliers)\nâ€¢ Outliers in regression can be seen on a residual plot, or on a QQ plot, or on just a regular plot of the data.\nâ€¢ Example: the Florida election data.\n\n\n\n\n\nâ€¢ Outliers can â€œpullâ€ on the regression line, especially if they are far away from the mean of the predictor(s).\nâ€¢ There are many statistics that assess influence. jamovi will calculate one of the most popular: a Cookâ€™s Distance\n\n\n3.2.9 Cookâ€™s Distances\nâ€¢ As we saw in the Florida election example, removing the outlier (Palm Beach County) had a substantial effect on the regression results.\nâ€¢ The logic behind Cookâ€™s Distance is to quantify what happens to the regression model when a single observation is removed. This is sometimes referred to as a â€œleave one outâ€ method.\nâ€¢ Cookâ€™s Distances quantify how much the predicted values of the response variable change when an observation is removed.\nâ€¢ Recall that, in simple regression, the predicted values are the values on the regression line.\nâ€¢ It is hard to interpret the actual values for Cookâ€™s Distances. Values greater than 1 are often considered â€œinfluentialâ€.\nâ€¢ The formula shows that it is based on the sum of the differences in predicted values between a model with the data point included and a model with it removed:\n\\[\n\\text{Cook's Distance for data point } \"i\" = D_i = \\frac{\\sum^n_{j=1}(\\hat{y}_i - \\hat{y}_{j(i)})^2}{MSE * p}\n\\]\nâ€¢ Where \\(\\hat{y}_{j(i)}\\) is the predicted value of the response variable when the model is re- fit with the \\(i^{th}\\) data point removed, and \\(p\\) is the number of predictor variables in\nâ€¢ A Cookâ€™s Distance is calculated for every data point. The option to do this in jamovi is under â€œSaveâ€ in â€œLinear Regressionâ€. This creates a new column with a Cookâ€™s distance for each row.\nâ€¢ The saved Cookâ€™s Distances can then be plotted. Any possibly influential points will usually stand out clearly from the rest.\n\n\n\n\n\nâ€¢ To quickly narrow in on the influential counties, we can filter out all the small Cookâ€™s distances.\nâ€¢ After implementing the filter, we can see that all rows which do not meet the criteria of the filter are excluded.\n\n\n\n\n\nâ€¢ Only rows 13 and 50 should be highlighted for Dade and Palmbeach county which have cookâ€™s distances of 1.983 and 3.786.\n\n\n3.2.10 (Multi)collinearity\nâ€¢ In regression analysis, we want our predictor variables to be correlated with the response variable.\nâ€¢ But we donâ€™t want our predictor variables to be (highly) correlated with one another!\nâ€¢ When two predictor variables are highly correlated, we say our model has â€œcollinearity.â€\nâ€¢ When more than two predictor variables are mutually highly correlated, we say our model as â€œmulticollinearityâ€.\n\n\n3.2.11 Why donâ€™t we want correlated predictors?\nâ€¢ To understand why we donâ€™t want correlated predictors, recall that multiple regression models estimate the association between each individual predictor variable and the response, \\(\\textit{while holding all other predictor variables constant}\\).\nâ€¢ This can be thought of as asking â€œwhat is the difference in the response variable when we observe data points that have different values of one predictor but the same values of the other predictors?â€\nâ€¢ If \\(Y\\) is the response and \\(x_1\\) and \\(x_2\\) are predictors, we want to know how different \\(Y\\) is when \\(x_1\\) values differ but \\(x_2\\) values are the same, or vice versa.\nâ€¢ But, if \\(x_1\\) and \\(x_2\\) are highly correlated, then we donâ€™t get to observe cases where values of one variable differ substantially while values of the other are the same! Consider instances of no correlation vs.Â heavy correlation:\n\n\n\n\n\nâ€¢ When \\(x_1\\) and \\(x_2\\) are uncorrelated, we see lots of instances of \\(x_1\\) values differing a lot when \\(x_2\\) values are equal.\n\n\n\n\n\nâ€¢ When \\(x_1\\) and \\(x_2\\) are highly correlated, we never see instances where \\(x_2\\) values are equal but \\(x_1\\) values are highly correlated.\n\n\n\n\n\nâ€¢ The upshot is that, when \\(x_1\\) and \\(x_2\\) are highly correlated, the regression procedure has a difficult time distinguishing between the â€œeffectâ€ of \\(x_1\\) on \\(Y\\) and the â€œeffectâ€ of \\(x_2\\) on \\(Y\\).\nâ€¢ Extreme example: if we had degrees Celsius and degrees Fahrenheit as predictors in the same model, it would be impossible to tell their effects apart. You canâ€™t change Celsius while holding Fahrenheit constant!\nâ€¢ The practical consequence is that the standard errors for the slopes of (multi)collinear predictors are much larger than they would be if it were not for the (multi)collinearity.\nâ€¢ If two or more predictors are perfectly correlated (\\(r=1\\)), then the model cannot be fit and jamovi produces an error:\n\n\n\n\n\nâ€¢ Here, \\(X1\\) and \\(X2\\) are perfectly correlated. jamovi cannot estimate a slope for \\(X2\\).\n\n\n3.2.12 Collinearity example: Florida election data\nâ€¢ In the Florida election data, we used total votes for each county as our predictor variable.\nâ€¢ There is another variable called â€œTotal_Regâ€. This is the total number of registered voters in each county.\nâ€¢ Unsurprisingly, Total_Votes and Total_Reg are highly correlated:\n\n\n\n\n\nâ€¢ If we run two separate simple regression models, we get very similar results:\n\\[\nBuchanin_i = \\beta_0 + \\beta_1Total\\_Votes_i + \\epsilon_i\n\\]\n\n\n\n\n\n\\[\nBuchanin_i = \\beta_0 + \\beta_1Total\\_Reg_i + \\epsilon_i\n\\]\n\n\n\n\n\nâ€¢ But look what happens if we use Total_Votes and Total_Reg as predictors in the same model:\n\\[\nBuchanin_i = \\beta_0 + \\beta_1Total\\_Votes_i + \\beta_2Total\\_Reg_i + \\epsilon_i\n\\]\n\n\n\n\n\nâ€¢ Two important things to note:\nâ€¢ P-values on slopes are much larger than for the individual models\nâ€¢ \\(R^2\\) is larger than on either individual model!\nâ€¢ Looking at the estimates and standard errors in all three models, we see that the standard errors are much larger in the multiple regression model. These estimates are â€œunstableâ€ â€“ their values will change a lot if the data change a little.\n\n\n\n\n\nâ€¢ We know the association between Total_Reg are Buchanan is actually positive. But with so high a standard error, the slope for Total_Reg turned out negative!\n\n3.2.12.1 Variance inflation factor (VIF)\nâ€¢ (Multi)collinearity can be assessed using a â€œVariance Inflation Factorâ€, or VIF. A VIF is calculated for the \\(j^{th}\\) predictor variable as:\n\\[\nVIF_j = \\frac{1}{1-R^2_j}\n\\]\nâ€¢ Where \\(R^2_j\\) is the \\(R^2\\) from a regression model with predictor \\(j\\) as the response variable and all other predictors still as predictors.\nâ€¢ In the Florida election example, the VIF for Total_Votes can be found using the \\(R^2\\) for the model:\n\\[\nTotal\\_Votes_i = \\beta_0 + \\beta_1Total\\_Reg_i + \\epsilon_i\n\\]\n\n\n\n\n\n\nâ€¢ This \\(R^2\\) is huge! Plugging it into the formula:\n\\[\nVIF_{Total\\_Votes}=\\frac{1}{1-0.997} = 333.33\n\\]\nâ€¢ Thankfully we donâ€™t have to do this by hand. In jamovi, under â€œLinear Regressionâ€ select â€œCollinearity statisticsâ€:\n\n\n\n\n\nâ€¢ VIF &gt; 10 typically is considered large (note that this would imply \\(R^2 = 0.9\\) between predictor variables).\nâ€¢ The most obvious thing to do in the presence of (multi)collinearity is to remove one or more correlated predictor variable. From a scientific standpoint, you also may not want highly correlated predictors in the same model.\n\n\n3.2.12.2 When should we worry about (multi)collinearity?\nâ€¢ (Multi)collinearity is a potentially huge problem if the goal of the regression model is to interpret the estimated slopes.\nâ€¢ This is because it increases the standard error of these slopes, making their values less reliable. Some people say it makes slopes â€œunstableâ€.\nâ€¢ It may also complicate the interpretation of slopes: you are trying to statistically â€œhold constantâ€ a predictor variable that doesnâ€™t naturally stay constant when the other predictor varies. This isnâ€™t necessarily a problem, but it is something to be aware of.\nâ€¢ However, (multi)collinearity does not negatively impact the predicted values themselves. Remember that it didnâ€™t hurt the \\(R^2\\) value in the Florida election example. \\(R^2\\) tells you how good your predictions are.\nâ€¢ So, if the model is only for predicting, you probably donâ€™t need to worry about using correlated predictor variables. Just beware when interpreting the slopes."
  },
  {
    "objectID": "Ch3_Model_Fit.html#part-2-improving-models",
    "href": "Ch3_Model_Fit.html#part-2-improving-models",
    "title": "3Â  Chapter 3: Assessing and improving model fit",
    "section": "3.3 Part 2: Improving models",
    "text": "3.3 Part 2: Improving models\n\n3.3.1 Summary of part 1\n\nIn the last set of notes, we looked at some things that can go wrong in regression modeling, including:\n\n-    Non-linear relationships between predictor(s) and response\n\n-   Non-normality of residuals\n\n-   Non-constant (heterogeneous) variance of residuals\n\n-   Influential outliers\n\n-   Multicollinearity\nâ€¢ In these notes, weâ€™ll look at some tools available for dealing with these problems.\n\n\n3.3.2 Transforming variables\nâ€¢ Recall the regression model:\n\\[\nY_i = \\beta_0 + \\beta_1x_{1i} + \\beta_2x_{2i} + \\dots + \\beta_px_{pi} + \\epsilon_i, \\text{ where } \\epsilon_i \\sim Normal(0,\\sigma)\n\\]\nâ€¢ Sometimes we can correct violations of model assumptions by applying a mathematical transformation to the response or predictor variables.\nâ€¢ The most common transformation in Statistics is the log transformation:\n\\[\nln(x) = log_e(x)\n\\]\n\n\n3.3.3 Log transformation\nâ€¢ \\(ln(ğ‘¥)\\) is the inverse function of \\(e^ğ‘¥\\), where \\(ğ‘’ = 2.718 \\dots\\)\nâ€¢ In other words, \\(ln(e^x) = x\\)\nâ€¢ Example: \\(ğ‘’^3 = 20.086; ln(20.086) = 3\\)\nâ€¢ So, the natural log of \\(x\\) is the number you would have to raise \\(e\\) to so that youâ€™d get \\(x\\).\nâ€¢ Note: in statistics, when we say â€œlogâ€, we usually mean â€œnatural logâ€. It turns out that the distinction is not very important. Iâ€™ll say â€œlog transformâ€\n\n3.3.3.1 Why log transform?\n\nThere are two main reasons for log transforming a variable:\n\nTo correct for skew in data or residuals\nTo interpret increases in a variable as multiplicative rather than additive.\n\n\nâ€¢ Both can be understood by recognizing an important property of logarithms; they â€œturn addition into multiplicationâ€\n\\[\nlog(ğ´) + log(ğµ) = log(ğ´ğµ)\n\\]\nâ€¢ In this sense, logarithms turn addition into multiplication.\nâ€¢ Example: suppose we have data for a skewed variable \\(X_1\\):\n\n\n\n\n\nâ€¢ Now we define \\(x_2 = ln(x_1)\\):\n\n\n\n\n\nâ€¢ This is a toy â€œdata setâ€. I chose \\(x_1\\) so that \\(x_2 = ln(x_1)\\) would just be the integers \\(1\\) through \\(10\\).\nâ€¢ Note: there is no more skew.\nâ€¢ Also note: increasing \\(x_2\\) by one unit results in multiplying \\(x_1\\) by \\(e\\). Addition in \\(x_2 = ln(x_1)\\) is the same thing as multiplication in \\(x_1\\).\n\n\n3.3.3.2 Same again, with log base 2\nâ€¢ Even simpler: define \\(x_2\\) as log base \\(2\\) of \\(x_1\\), i.e.Â \\(log_2(x_1)\\)\n\n\n\n\n\nâ€¢ Now increasing \\(x_2\\) by one unit is equivalent to multiplying \\(x_1\\) by \\(2\\). Addition in \\(x_2 = log_2(x_1)\\) is the same thing as multiplication in \\(x_1\\).\n\n\n3.3.3.3 Log transforming right-skewed data\n\nSkewed data can be bad for regression, in that it can lead to:\n\n-   Non-linear relationship between X and Y\n\n-   Influential outliers\n\n-   Non-normal residuals\n\n-   Non-constant variance in residuals\nâ€¢ So a simple log transformation can sometimes go a long way toward making the regression model fit the better!\nâ€¢ It is most common to log transform a response variable, because assumptions about residuals apply to \\(Y\\), not \\(X\\).\nâ€¢ But if \\(X\\) is skewed, the model can benefit from a log transformation of \\(X\\).\nâ€¢ Bear in mind that log transformation will affect the interpretation of slope coefficients!\nâ€¢ If \\(X\\) is log transformed, then a one unit increase in \\(ln(ğ‘‹)\\) corresponds to multiplying \\(X\\) by \\(e \\approx 2.72\\). So the slope for \\(ln(ğ‘‹)\\) tells you how much \\(Y\\) increases when \\(X\\) is multiplied by \\(2.72\\). Or, even better, use log base \\(2\\) and the slope will give how much \\(Y\\) changes when \\(X\\) is doubled.\nâ€¢ If \\(Y\\) is log transformed, then the interpretations of slopes get more complicated. Hereâ€™s the math, with the error term omitted for convenience:\n\\[\nln(y_i) = \\beta_0 + \\beta_1X_i\n\\]\n\\[\n\\therefore y_i = e^{\\beta_0 + \\beta_1X_i}\n\\]\nâ€¢ Increase \\(X\\) by \\(1 \\dots\\)\n\\[\ny_i^* = e^{\\beta_0 + \\beta_1(X_i + 1)} = e^{\\beta_0 + \\beta_1X_i} \\cdot e^{\\beta_1}\n\\]\nâ€¢ So, when \\(Y\\) is log transformed, a one unit increase in \\(X\\) multiplies predicted \\(Y\\) by \\(e^{\\beta_1}\\)\n\n\n3.3.3.4 Interpreting slope as a % change in outcome\nâ€¢ Recall the heights vs.Â wages data from group project 1. The paper reported this estimated model:\n\\[\nln(\\text{wage}) = \\hat{\\beta}_0 + 0.002\\text{(Adult Height)} + 0.027\\text{(Youth Height)} + 0.024\\text{(Age)}\n\\]\nâ€¢ So, when comparing two adults \\(1\\) inch apart in height but with the same youth height and age predicted wage is multiplied by \\(e^{0.027} = 1.027\\) for the taller adult.\nâ€¢ Multiplying by \\(1.027\\) can be thought of as increasing by \\(2.7\\%\\)\n\n\n3.3.3.5 Log transformation applied example\nâ€¢ Here is the percent change formula:\n\\[\n\\%\\text{ change (from A to B)} = \\frac{B-A}{A}*100\\%\n\\]\nâ€¢ If B is \\(1.027*\\)A, then\n\\[\n\\%\\text{ change} = \\frac{1.027*A - A}{A}*100 = \\frac{0.027A}{A}*100 = 2.7\\%\n\\]\nâ€¢ So, when comparing two adults 1 inch apart in height but with the same youth height and age, predicted wage is \\(2.7\\%\\) higher for the taller adult.\n\n\n3.3.3.6 Log transformation in \\(Y\\) vs.Â in \\(X\\)\nâ€¢ Remember that log transformation â€œturns addition into multiplicationâ€. So, to keep track of how log transforming \\(Y\\) vs.Â log transforming \\(X\\) affects your model:\n\\[\n\\text{log}(Y_i) = \\beta_0 + \\beta_1X_i + \\epsilon_i\n\\]\n\\[\n\\text{vs.}\n\\]\n\\[\nY_i = \\beta_0 + \\beta_1\\text{log}(X_i) + \\epsilon_i\n\\]\nâ€¢ If you log transform \\(Y\\) but not \\(X\\), your model estimates the multiplicative change in predicted \\(Y\\) for an additive change in \\(X\\).\nâ€¢ If you log transform \\(X\\) but not \\(Y\\), your model estimates the additive change in predicted \\(Y\\) for a multiplicative change in \\(X\\).\n\n\n\n3.3.4 Non-linearity\nâ€¢ Sometimes data show obvious curvature, in the sense that \\(Y\\) is clearly not a straight line function of \\(X\\).\nâ€¢ This will be visible on a plot of \\(Y\\) vs.Â \\(X\\). It will also be visible on a residuals vs.Â predicted values plot after running a regression.\nâ€¢ If there is curvature in the relationship between \\(Y\\) and \\(X\\), then it might be sensible to add a polynomial \\(X\\) term:\n\\[\nY_i = \\beta_0 + \\beta_1x_i + \\beta_2x_i^2 + \\epsilon_i\n\\]\n\n3.3.4.1 â€œPolynomialâ€ review\nâ€¢ A â€œpolynomialâ€ expression is typically one in which variables are included at different powers. For example, a generic third degree polynomial equation might look like:\n\\[\ny = a + bx + cx^2 + dx^3\n\\]\nâ€¢ A â€œsecond degreeâ€ polynomial is one in which an \\(x\\) and \\(x^2\\) term are both included. This is by far the most common type of polynomial seen in regression models.\n\n\n3.3.4.2 \\(2^{nd}\\) degree and \\(3^{rd}\\) degree polynomials\nâ€¢ \\(2^{nd}\\) degree polynomials are often called â€œquadraticâ€. \\(3^{rd}\\) degree polynomials are often called â€œcubicâ€. Here are visual examples of simulated quadratic and cubic relationships between \\(Y\\) and \\(X\\):\n\n\n\n\n\n\n\n3.3.4.3 Curvature in residuals\nâ€¢ Here is regression output comparing a linear model to a quadratic model when the relationship between \\(Y\\) and \\(X\\) is quadratic:\n\n\n\n\n\n\n\n3.3.4.4 Example: Florida election data\nâ€¢ Here is what the Florida election data look like with the Palm Beach County outlier removed, along with regression results for the simple linear regression model:\n\\[\nBuchanan_i = \\beta_0 + \\beta_1Total\\_Votes_i + \\epsilon_i\n\\]\n\n\n\n\n\nâ€¢ Now we will fit a quadratic polynomial model to the same data:\n\\[\nBuchanan_i = \\beta_0 + \\beta_1Total\\_Votes_i + \\beta_2Total\\_Votes^2_i + \\epsilon_i\n\\]\nâ€¢ To create this polynomial predictor in jamovi we first need to install the GAMLj module. Then, select â€œGeneralized Linear Modelsâ€ and last our â€œDependent Variableâ€ and â€œCovariatesâ€.\nâ€¢ Under the â€œModelâ€ drop down menu, click on Total_Votes in the â€œComponentsâ€ table.\nâ€¢ An up and down arrow will appear with the degree of Total_Votes, click the up arrow so the 1 changes to 2. Then click the right arrow to add Total_Votes2 to â€œModel Termsâ€\n\n\n\n\n\n\\[\nBuchanan_i = \\beta_0 + \\beta_1Total\\_Votes_i + \\beta_2Total\\_Votes^2_i + \\epsilon_i\n\\]\n\n\n\n\n\nâ€¢ This is better, but we still see curvature in the residual plot.\nâ€¢ Letâ€™s try a cubic model:\n\\[\nBuchanan_i = \\beta_0 + \\beta_1Total\\_Votes_i + \\beta_2Total\\_Votes^2_i +\\beta_3Total\\_Votes_i^3 + \\epsilon_i\n\\]\n\n\n\n\n\n\\[\nBuchanan_i = \\beta_0 + \\beta_1Total\\_Votes_i + \\beta_2Total\\_Votes^2_i +\\beta_3Total\\_Votes_i^3 + \\epsilon_i\n\\]\n\n\n\n\n\n\n\n3.3.4.5 Example: Florida election data: check note\nâ€¢ Itâ€™s debatable whether this is much better. For one, the \\(Total\\_Votes^2\\) term is non-significant.\nâ€¢ But think back to multicollinearity. Each polynomial term will be correlated with the other terms â€“ after all, \\(Total\\_Votes\\), \\(Total\\_Votes^2\\), and \\(Total\\_Votes^3\\) must all be correlated.\nâ€¢ Note that jamovi will not produce VIFs in GLM. Intuitively we know these will be highly correlated with each other.\nâ€¢ It turns out that centering helps in polynomial models:\n\n\n\n\n\nâ€¢ By default jamovi centers polynomials which makes their standard errors smaller than if they were not centered. But, these estimates are not interpretable; you cannot hold \\(Total\\_Votes^2\\) constant while increasing \\(Total\\_Votes\\).\nâ€¢ In this example, the two counties with the highest total votes are heavily pulling on the regression line.\n\n\n\n\n\n\n\n\n3.3.5 Over-fitting\nâ€¢ This model might be â€œover-fitâ€.\nâ€¢ Over-fitting is when a model fits the data so well that it ends up fitting random variation that is not of interest.\n\n\n\n\n\nâ€¢ Imagine if the two data points on the right had slightly higher vote counts for Buchanan. Or if the next two to their left had slightly lower vote counts. The curve would look very different.\nâ€¢ At this point, we might just be modelling noise.\nâ€¢ Here is an extreme example of over-fitting: fitting a â€œsmootherâ€ curve to data and giving it permission to move dramatically up and down through the data.\n\n\n\n\n\nâ€¢ This line fits the data very well, but does it represent the general trend between total votes and votes for Buchanan? Definitely not!\nâ€¢ (Side note: â€œsmoothersâ€ are great tools for visualizing and summarizing data, but they can be extremely sensitive to degree of smoothing. We wonâ€™t use them in STAT 331)\nâ€¢ Compare the over-fit model to the linear model.\n\n\n\n\n\nâ€¢ The linear model may be missing out on some curvature. But it might also make better predictions.\nâ€¢ If we were to observe a new county with \\(450,000\\) total votes, would we be better guessing that votes for Buchanan fall on the highly curved line or on the straight line?\n\n\n3.3.6 Back to basics: is the model sensible?\n\nBack to basics: regression models are typically used for two purposes:\n\nPredicting values of the response variable, using the predictor variables. This is done by plugging values for the predictor variables into the estimated model.\nEstimating the association between each individual predictor variable and the outcome while statistically holding other predictor variables constant. This is done by interpreted estimated slope coefficients.\n\n\n\n3.3.6.1 If you just want to make predictions\nâ€¢ \\(R^2\\) is the easiest to understand statistic for assessing how well your model makes predictions. The closer to \\(1\\), the better.\nâ€¢ Multicollinearity isnâ€™t an issue. It doesnâ€™t affect predicted values.\nâ€¢ BUT â€“ beware of overfitting! The more complex your model, the more risk you take of modeling noise instead of signal.\nâ€¢ Also, be aware that \\(R^2\\) can never go down when adding predictors. You can add complete nonsense predictor variables, and the worst that will happen to \\(R^2\\) is that it stays the same.\n\n\n3.3.6.2 If you want to interpret slopes\nâ€¢ Always remember that each slope is interpreted under the assumption that all other predictor variables are being held constant, i.e.Â â€œcontrolled forâ€.\nâ€¢ The more predictor variables in the model, the less sense this will make.\nâ€¢ Example: wage vs.Â height study:\n\n\n\n\n\nâ€¢ In model 4, the estimated slope for youth height can be interpreted as:\nâ€œThe predicted difference in ln(wage) for two people one inch apart in youth height, but equal in adult height, whose mothers have the same # of years of schooling and are both in either skilled fields or work or not, whose fathers have the same # of years of schooling and are both in either skilled fields or work or not, and who have the same number of siblings.â€\n\n\n\n\n\nâ€¢ Maybe this is the best way to think about the association between youth height and wages. But it is fairly complicated.\nâ€¢ If youâ€™re going to try to make â€œreal worldâ€ sense out of regression results, your model should be informed by theory.\nâ€¢ This is necessarily subjective! You have to choose which variables you think are important. You have to think about what makes sense.\n\nThis might require:\n\nLog transforming a variable solely because you like the multiplicative interpretation better than the additive interpretation.\nKeeping a variable in a model even though it isnâ€™t statistically significant.\nRemoving a variable you are interested in, because it doesnâ€™t make sense to â€œhold it constantâ€ when estimating slopes for other variables.\n\n\n\n\n3.3.6.3 Is the model missing something important?\nâ€¢ There is another variable in the Florida election data set that could be worth including: â€œReg_Reformâ€: the total number of voters registered with the Reform Party. Pat Buchanan was the Reform Party candidate. Letâ€™s add it to the model:\n\\[\nBuchanan_i = \\beta_0 + \\beta_1Total\\_Votes_i + \\beta_2Reg\\_Reform_i + \\epsilon_i\n\\]\n\n\n\n\n\nâ€¢ This residual plot looks great!\n\n\n\n\n\nâ€¢ It turns out that the curvature in the previous residual plots went away when adding an important variable to the model. No need to mess with polynomials after all!\nâ€¢ Also, the \\(R^2\\) is roughly the same as in the cubic model using only total votes as a predictor.\nâ€¢ So we have roughly equal fit, without having to worry about overfitting, and without having to give up interpretability of the slopes.\nâ€¢ One downside: there is some collinearity. Look at the VIFs.\n\n\n\n\n\nâ€¢ VIF of about \\(5\\) implies \\(\\frac{1}{1-R^2}\\approx 5\\) when using the \\(R^2\\) from:\n\\[\nTotal\\_Votes_i = \\beta_0 + \\beta_1Reg\\_Reform_i + \\epsilon_i\n\\]\nâ€¢ So, this \\(R^2\\) is about \\(1 âˆ’ \\frac{1}{5} = 0.8\\). And so \\(r = \\sqrt{0.8} =0.89\\) . These predictors are strongly correlated.\n\nâ€¢ Note also that total votes is not significant.\nâ€¢ But: the slope for \\(Reg\\_Reform\\) has a nice interpretation:\nWhen comparing two counties with the same number of votes cast in the election, a county with an additional registered Reform Party member is estimated to have \\(2.24\\) additional votes, on average, for Pat Buchanan.\nâ€¢ Should total votes be taken out of the model? This is a subjective decision.\n\n\n3.3.6.4 What would you like to â€œcontrolâ€ for?\nâ€¢ In regression analysis, we usually emphasize (correctly) that correlation does not imply causation.\nâ€¢ However, if you have knowledge or beliefs about causal direction, you should take these into account when choosing your variables!\nâ€¢ Example: in the rheumatoid arthritis study, we were looking at the effect of drugs on inflammation level. In particular, we were comparing how effective they were at reducing inflammation. Suppose we also asked patients to rate their mobility level (RA tends to reduce mobility).\nâ€¢ Our model might be:\n\\[\nDifference_i = \\beta_0 + \\beta_1age_i + \\beta_2drug_i + \\beta_3(age*drug)_i + \\beta_4mobility_i + \\epsilon_i\n\\]\nâ€¢ Now, when interpreting the previous slopes, I am comparing average reduction in inflammation (â€œdifferenceâ€, the response variable), between two people who have the same mobility level. But if they have the same mobility level, then they will have more similar inflammation levels than if we allowed mobility level to differ.\nâ€¢ In other words, because the drug reduces inflammation and improves mobility, â€œcontrollingâ€ for mobility will make it look like the drugs are less effective than they really are.\n\n\n3.3.6.5 Beware the â€œkitchen sinkâ€ approach\nâ€¢ Thereâ€™s an old saying: â€œtaking everything but the kitchen sinkâ€.\nâ€¢ It can be tempting to toss everything but the kitchen sink into a regression model, especially when you have loads of variables that all seem like theyâ€™d be associated with the response.\nâ€¢ But beware! Adding in one predictor can have a dramatic effect on the slopes of other predictors, as well as on their standard errors.\nâ€¢ It really really really matters that each slope is estimated as though all other predictors are held constant. This can reveal otherwise unseen effects, but it can also obscure otherwise obvious effects or induce apparent effects that arenâ€™t real. There is no substitute for scientific reasoning when choosing a model.\n\n\n3.3.6.6 The model is simpler than whatâ€™s being modeled\nâ€¢ Letâ€™s take a step back and ask: why are we fitting data to models?\nâ€¢ Well, we are interested in the real world. And the real world in incredibly complicated. Maybe incomprehensibly complicated.\nâ€¢ So, we simplify things using models. We hope that the model captures the essence of what we care about in the real world. But we know it is a simplification; perhaps an extreme simplification.\nâ€¢ â€œAll models are wrong; some are usefulâ€ â€“ George Box\nâ€¢ Consider how the regression model describes where data comes from:\n\\[\nY_i = \\beta_0 + \\beta_1x_{i} + \\epsilon_i, \\text{ where } \\epsilon_i \\sim Normal(0,\\sigma)\n\\]\nâ€¢ This says that to get data, we pick a value for X, go to a line, then randomly draw a value from a normal distribution and add this to the value on the line. And thatâ€™s where data comes from!\nâ€¢ Except, thatâ€™s not where data comes from. This is a model. It is a simplification of reality. We use these models because we think they will help us answer questions we care about (e.g.Â make predictions, identify associations between variables). Donâ€™t forget that the model is not the thing itself."
  },
  {
    "objectID": "Ch4_ANOVA.html#what-is-anova",
    "href": "Ch4_ANOVA.html#what-is-anova",
    "title": "4Â  Chapter 4: ANOVA-based methods",
    "section": "4.1 What is ANOVA?",
    "text": "4.1 What is ANOVA?\nâ€¢ ANOVA stands for â€œanalysis of varianceâ€\nâ€¢ ANOVA is regression with categorical predictors. Thatâ€™s it.\n\n4.1.1 ANOVA is regression presented differently\nâ€¢ OK, thereâ€™s more to say about ANOVA than just â€œregression with categorical predictors.â€\nâ€¢ ANOVA is typically used to analyze data from experiments. In experiments, the categorical predictors are usually groups to which experimental units (aka subjects) are assigned. â€¢ ANOVA tends to focus on comparing means of different groups to one another. â€¢ Although ANOVA is â€œjustâ€ regression, there are conventions for reporting ANOVA results that are simpler and cleaner than what weâ€™ve seen for regression.\n\n\n4.1.2 Indicator variables\nâ€¢ Weâ€™ve seen how to incorporate a categorical predictor into a regression model when the predictor takes on two values. We create an â€œindicatorâ€ (aka â€œdummyâ€ aka â€œbinaryâ€) variable that takes on the values 0 or 1. For example:\n\\[\nY_i=\\beta_0+\\beta_1x_{1i} + \\epsilon_i\n\\]\n\\[\nx_i = 1 \\text{ if \"treatment;\"} x_i = 0 \\text{ if \"control\"}\n\\]\nâ€¢ Here, \\(\\hat{y}_i = \\hat{\\beta}_0\\) for control, and \\(\\hat{y}_i = \\hat{\\beta}_0 + \\hat{\\beta}_1\\) for treatment.\nâ€¢ What if our categorical predictor takes on more than two categories? Suppose we have three groups: treatment 1, treatment 2, and control. We can add another indicator:\n\\[\nY_i=\\beta_0+\\beta_1x_{1i} + \\beta_2x_{2i} + \\epsilon_i\n\\]\n\\[\nx_{1i} = 1 \\text{ if treatment }1; x_{1i} = 0 \\text{ otherwise} \\\\\nx_{2i} = 1 \\text{ if treatment }2; x_{1i} = 0 \\text{ otherwise}\n\\]\nâ€¢ \\(\\hat{y}_i = \\hat{\\beta}_0\\) for control, \\(\\hat{y}_i = \\hat{\\beta}_0 + \\hat{\\beta}_1\\) for treatment 1, and \\(\\hat{y}_i = \\hat{\\beta}_0 + \\hat{\\beta}_1 + \\hat{\\beta}_2\\) for treatment 2.\nâ€¢ Control is serving as the â€œbaselineâ€ category, represented by the intercept."
  },
  {
    "objectID": "Ch4_ANOVA.html#one-way-anova",
    "href": "Ch4_ANOVA.html#one-way-anova",
    "title": "4Â  Chapter 4: ANOVA-based methods",
    "section": "4.2 One-Way ANOVA",
    "text": "4.2 One-Way ANOVA\nâ€¢ One-way ANOVA models have a single categorical predictor variable. They can be written as:\n\\[\nY_i = \\mu + \\alpha_j + \\epsilon_{ij}, \\text{ where } \\epsilon_{ij} \\sim Normal(0,\\sigma^2) \\text{ and } i = 1, \\dots ,n_j, j = 1, \\dots, p\n\\]\nâ€¢ Where there are â€œ\\(p\\)â€ groups (i.e.Â the categorical predictor takes on â€œ\\(p\\)â€ values), â€œ$n_j$â€ is the sample size of the \\(j^{th}\\) group, and \\(y_{ij}\\) is the \\(i^{th}\\) observation in the \\(j^{th}\\) group.\nâ€¢ Here, \\(\\mu\\) is the overall mean and \\(\\alpha_j\\) is the deviation of the \\(j^{th}\\) group mean from the overall mean.\nâ€¢ Suppose we have 4 groups. The ANOVA model is:\n\\[\nY_i = \\mu + \\alpha_j + \\epsilon_{ij}, \\text{ where } \\epsilon_{ij} \\sim Normal(0,\\sigma^2) \\text{ and } i = 1, \\dots ,n_j, j = 1, \\dots, 4\n\\]\nâ€¢ Written as a regression model instead:\n\\[\nY_i = \\beta_0 +\\beta_1Group1 + \\beta_2Group2 + \\beta_3Group3 + \\epsilon_{i}, \\text{ where } \\epsilon_{i} \\sim Normal(0,\\sigma^2) \\text{ and } i = 1, \\dots ,n\n\\]\nâ€¢ Here, the â€œGroupâ€ predictors are \\(0\\) / \\(1\\) indicator variables. For group \\(4\\), the mean of \\(y\\) is the intercept, \\(\\beta_0\\)."
  },
  {
    "objectID": "Ch4_ANOVA.html#factorial-anova",
    "href": "Ch4_ANOVA.html#factorial-anova",
    "title": "4Â  Chapter 4: ANOVA-based methods",
    "section": "4.3 Factorial ANOVA",
    "text": "4.3 Factorial ANOVA\nâ€¢ If we have more than one categorical predictor variable, and interactions between the predictors, we have a factorial ANOVA model. Generically:\n\\[\nY_{ijk} = \\mu + \\alpha_j + \\beta_k + (\\alpha\\beta)_{jk} + \\epsilon_{ijk}, \\text{ where } \\epsilon_{ijk} \\sim Normal(0,\\sigma^2) \\\\\n\\text{ and } i = 1, \\dots ,n_{jk}, j = 1, \\dots, p, k = 1, \\dots,q\n\\]\nâ€¢ Now predictor \\(1\\) is \\(\\alpha\\) and predictor \\(2\\) is \\(\\beta\\) and they interact. There are â€œ$p$â€ groups for predictor \\(1\\) and â€œ$q$â€ groups for predictor \\(2\\).\nâ€¢ The subscripts start getting messy pretty fast.\n\n4.3.1 Factorial ANOVA example\nâ€¢ Hereâ€™s an example of a â€œ5x2 factorialâ€ ANOVA, meaning that one variable has 5 groups and the other has 2.\nâ€¢ The study is on memory: how many words, on average, do people recall when given certain processing tasks?\nâ€¢ This example is where the graph on our Canvas home page comes from. It uses data simulated to mimic data from a 1974 Hans Eysenck study.\n\n100 subjects were split into 5 recall groups:\n\nâ€œCountingâ€: subjects counted how many letters were in each presented word\nâ€œRhymingâ€ subjects thought of words that rhymed with each presented word\nâ€œAdjectiveâ€: subjects thought of an adjective that could be used to modify each presented word\nâ€œImageryâ€: subjects were told to form vivid images of each word\nâ€œIntentionalâ€: subjects were told to memorize the word for later recall\n\n\nâ€¢ Counting and rhyming are lower level processing tasks, so the hypothesis was that this group would recall fewer words than the others. Subjects were also classified as â€œyoungâ€ or â€œoldâ€.\nâ€¢ Hereâ€™s the ANOVA model:\n\\[\nRecall_{ijk} = \\mu + Group_j + Age_k + (Group \\cdot Age)_{jk} + \\epsilon_{ijk}, \\text{ where } \\epsilon_{ijk} \\sim Normal(0,\\sigma^2) \\\\\n\\text{ and } j = 1, \\dots ,5, k = 1, 2\n\\]\nâ€¢ Jamovi has an â€œANOVAâ€ function and a â€œregressionâ€ function. Weâ€™ll use both the analyze these data.\n\n\n\n\n\nâ€¢ First weâ€™ll use ANOVA, using items recalled as the response (dependent) variable, and recall condition and age as factors. Note that jamovi automatically includes their interaction.\n\n\n\n\n\nâ€¢ I am usually not interested in the sums of squares or mean squares in the ANOVA table, as these are not interpretable. We do have p-values for each â€œmain effectâ€ along with the interaction. More importantly, we have effect size statistics: eta-squared (\\(\\eta^2\\)) and partial eta-squared (\\(\\eta^2p\\))\n\n\n4.3.2 ANOVA effect sizes: \\(\\eta^2\\) and partial-\\(\\eta^2\\)\nâ€¢ \\(\\eta^2\\) is akin to \\(R^2\\) for one factor (main effect or interaction). It gives the proportion of variance in the response attributable to the factor in question:\n\\[\n\\eta^2 = \\frac{SS_{factor}}{SS_{total}}\n\\]\n\n\n\n\n\nâ€¢ Here, we see that recall condition explains the most variance by far, followed by age, followed by their interaction.\nâ€¢ \\(\\eta^2p\\) (partial eta-squared) is like \\(\\eta^2\\), but with the variance accounted for by the other factors removed from the denominator:\n\\[\n\\eta^2p = \\frac{SS_{factor}}{SS_{factor} + SS_{residuals}}\n\\] \nâ€¢ Example: for recall condition,\n\\[\n\\eta^2p = \\frac{1515}{1515 + 722} = 0.677, \\text{ and } \\eta^2 = \\frac{1515}{1515 + 240 + 190 + 722} = 0.568\n\\]\nâ€¢ I personally prefer \\(\\eta^2\\), as their sum cannot exceed \\(1\\). Some prefer \\(\\eta^2p\\), because it quantifies the â€œeffectâ€ of a factor relative to remaining unexplained variance,\n\n\n4.3.3 The means / interaction plot\nâ€¢ We can make a nice plot under â€œestimated marginal meansâ€:\n\n\n\n\n\nâ€¢ This plot shows every combination of means across recall condition and age. It also has 95% CIs around each mean, and raw data displayed.\n\n\n\n\n\nâ€¢ This is sometimes called a â€œmeans plotâ€ or an â€œinteraction plotâ€. The interaction is represented by non-parallel lines, e.g.Â a positive change going from â€œImageryâ€ to â€œIntentionâ€ for young people, but a negative change for old people.\nâ€¢ The previous plot showed means for recall condition, with separate lines for age. If we flip the order of the variables, we get this:\n\n\n\n\n\nâ€¢ Here we see that mean items recalled for young people is substantially higher than for old people in the last three conditions, but differs only slightly in the first two conditions.\n\n\n4.3.4 ANOVA diagnostics\nâ€¢ We can also run diagnostic tests, under â€œAssumption checksâ€:\n\n\n\n\n\nâ€¢ The QQ plot looks pretty good.\n\n\n\n\n\nâ€¢ I personally do not recommend paying attention to Leveneâ€™s â€œhomogeneity of variancesâ€ test, nor to the Shapiro-Wilk normality tests\n\n\n\n\n\nâ€¢ These tests use null hypotheses of â€œpopulation variance is the same in all groupsâ€, or â€œthe residuals were drawn from a normal distributionâ€. As I donâ€™t think these model assumptions could be literally true, I am not interested in whether they can be rejected by the data.\nâ€¢ A â€œsignificantâ€ violation of modeling assumptions does not imply a consequential violation. In particular, if sample size is large, trivial violations of assumption\n\n\n4.3.5 Doing all of this as regression\nâ€¢ The beginning of these slides claimed that ANOVA is just regression with categorical predictors. Letâ€™s see what our results look like if we use jamoviâ€™s â€œlinear regressionâ€ function rather than â€œANOVAâ€.\n\n\n\n\n\nâ€¢ The predictor variables are entered as â€œfactorsâ€ here, rather than as â€œcovariatesâ€, to ensure they are treated as categorical rather than as quantitative variables.\n\n\n\n\n\nâ€¢ Finally, the interaction must be specified under â€œmodel builderâ€; jamovi does not create regression interactions by default.\nâ€¢ There are five recall condition groups, giving four indicator variables, all compared against the baseline group â€œcountingâ€, whose mean is represented by the intercept.\nâ€¢ There are two â€œageâ€ groups; â€œyoungâ€ is the baseline group.\n\n\n\n\n\nâ€¢ So the intercept of \\(6.500\\) is the mean words recalled for a young person in the â€œcountingâ€ condition.\nâ€¢ The first indicator under RecallCondition is â€œRhyming â€“ Countingâ€. Its slope of \\(1.1\\) is the difference in mean recall for these two groups, when Age = Young\nâ€¢ The interaction slope for â€œRhyming â€“ Countingâ€ is \\(-1.2\\).\nâ€¢ So, for Age = Old, the difference in mean recall is \\(1.1 +(-1.2) = âˆ’0.1\\)\nâ€¢ The interaction terms are all indicators, that â€œturn onâ€ when Age = Old, and â€œturn offâ€ when Age = Young.\nâ€¢ Notice that the slope for Age (â€œOld â€“ Youngâ€) is 0.5, suggesting greater recall for older participants.\nâ€¢ However, the interaction slopes are all larger negative values.\nâ€¢ So, when RecallCondition = Counting, the mean items recalled for Age = Old is larger than for Age = Young. But for all other recall conditions, the interaction slopes turn this negative, and mean items recalled is larger for younger participants.\n\n4.3.5.1 Main regression results\nâ€¢ â€œEstimated Marginal Meansâ€ under regression will produce a similar plot to the one made under ANOVA, just without the connecting lines and raw data:\n\n\n\n\n\nâ€¢ If you look carefully, you should be able to see how the regression results correspond to this plot. For instance, we see that the only condition where Old &gt; Young is Counting.\n\n\n\n4.3.5.2 Residual plot\nâ€¢ We can get a plot of residuals vs.Â fitted values.\n\n\n\n\n\nâ€¢ Notice that the residuals are all vertically stacked? This is to be expected when predictor variables are categorical.\nâ€¢ In this case, there are 5x2 = 10 possible combinations of groups that participants could be assigned to. And so there are only 10 possible â€œfittedâ€ (i.e.Â predicted) values that the model can produce."
  },
  {
    "objectID": "Ch4_ANOVA.html#ancova",
    "href": "Ch4_ANOVA.html#ancova",
    "title": "4Â  Chapter 4: ANOVA-based methods",
    "section": "4.4 ANCOVA",
    "text": "4.4 ANCOVA\nâ€¢ ANCOVA (analysis of covariance) is ANOVA with an additional continuous predictor variable.\nâ€¢ Typically, this additional continuous predictor is not of primary interest; the primary interest is still comparing group means.\nâ€¢ The continuous predictor is often thought of as a â€œcovariateâ€ â€“ a variable that should be accounted for when drawing inference on the other variables.\nâ€¢ A common use of ANCOVA is for modeling an outcome when â€œbaselineâ€ or â€œpre-studyâ€ or â€œpre-testâ€ scores are available.\nâ€¢ For instance, consider testing different educational models on different sections of a class. Some get traditional lecture, some are completely â€œflippedâ€, and some are a combination of the two.\nâ€¢ In this study, a preliminary quiz is given on the first day of class. Score on the preliminary quiz will be the covariate. Score on an end of semester quiz (â€œpostâ€) will be the response variable.\n\\[\nPost_{ij} = \\mu + type_j + \\beta(pre_{ij} - \\overline{pre}) + \\epsilon_{ij}\n\\]\nâ€¢ Here, \\(j\\) goes from \\(1\\) to \\(3\\), for the three teaching types being compared.\nâ€¢ The pre-test predictor is centered (note that \\(\\overline{pre}\\) is mean for pretests).\nâ€¢ Imagine we believe that the mean differences between teaching types will be larger for students with lower pretest scores. To account for this possibility, let type and (centered) pretest interact:\n\\[\nCourse\\_avg_{ij} = \\mu + type_j + \\beta(pre_{ij} - \\bar{pre}) + \\gamma_j(type_j)(pre_j - \\overline{pre}) + \\epsilon_{ij}\n\\]\n\n4.4.0.1 Looking at the data\nâ€¢ The data file is called â€œtest_pretestâ€. Here is the formatting:\n\n\n\n\n\nâ€¢ This plot was made using Analyses / Exploration / Scatterplot. Density curves are there, just for fun:\n\n\n\n\n\n\n\n\n\n\nâ€¢ The three lines look pretty close to parallel, so there either isnâ€™t an obvious interaction here, or itâ€™s small.\n\n\n\n\n\n\n\n4.4.0.2 Analysis using ANCOVA\nâ€¢ Notice that the effect size for pre- score is far greater than the effect size for type, or for the interaction. This is not surprising.\n\n\n\n\n\nâ€¢ Type is still significant; itâ€™s \\(\\eta^2p\\) value is much larger than its \\(\\eta^2\\) value.\nâ€¢ Careful interpreting the â€œEstimated marginal meansâ€ plot â€“ it takes into account only â€œpostâ€ scores!\n\n\n4.4.0.3 Analysis using regression w/ indicators\nâ€¢ For the regression analysis, weâ€™ll use two indicator variables. We donâ€™t have to do it this way; if we just include â€œtypeâ€, jamovi will create the indicators for us, using the first class type listed in the data.\n\n\n\n\n\nâ€¢ Notice that the interaction estimates are small relative to their standard errors (thus producing large p- values).\n\n\n\n\n\nâ€¢ According to this regression model, there is not a â€œsignificantâ€ interaction.\n\n\n4.4.0.4 Analysis using regression w/factor\nâ€¢ Hereâ€™s what happens if we put in â€œtypeâ€ as a factor variable rather than making our own indicators. The results are the same.\n\n\n\n\n\nâ€¢ The QQ plot and residual plot both look great\n\n\n\n\n\n\n\n4.4.1 Analyzing paired data: ANCOVA vs ANOVA\nâ€¢ Another approach we could take would be to compute the differences in the two scores for each person, then do a regular ANOVA or regression analysis on those.\n\n\n\n\n\nâ€¢ We see that there are significant differences in mean test score change between teaching types ($F=8.94, p&lt;0.001$)\n\n\n\n\n\n\n\n\n\n\n\n4.4.1.1 Analyze differences, regression approach\nâ€¢ This agrees with the ANOVA results from the previous slide: we have a statistically significant difference in mean test score differences (post â€“ pre) when comparing â€œcomboâ€ to â€œflippedâ€ or â€œtraditionalâ€. And \\(R^2 = \\eta^2\\)!\n\n\n\n\n\nâ€¢ It turns out here that taking the post â€“ pre differences first and then comparing mean differences across teaching types produces similar results to predicting post-test scores using pre-test and teaching type as predictors.\nâ€¢ But, these methods are not answering the exact same question. Using pre-test as a covariate, we answer the question â€œwhat difference do I expect in post-test scores when comparing two students with the same pre- test score but different teaching typesâ€?\nâ€¢ When differencing first and then doing the analysis, we answer the question â€œwhat differences in the mean post-pre score change do I expect when comparing class typesâ€?\nâ€¢ These question sound similar, but they arenâ€™t the same! Whether to use ANCOVA or do the differencing first is a matter of subjective judgement, and the experts donâ€™t all agree (see â€œLordâ€™s Paradoxâ€ for more fun on this)."
  },
  {
    "objectID": "Ch5_Categorical.html#confidence-intervals-and-hypothesis-tests-for-proportions",
    "href": "Ch5_Categorical.html#confidence-intervals-and-hypothesis-tests-for-proportions",
    "title": "5Â  Chapter 5: Analyzing categorical data",
    "section": "5.1 Confidence intervals and hypothesis tests for proportions",
    "text": "5.1 Confidence intervals and hypothesis tests for proportions\nâ€¢ At the beginning of the class we reviewed confidence intervals and hypothesis tests for means. These methods can also be used for proportions\nâ€¢ A proportion is just a special kind of mean, where the data are all ones and zeros.\nâ€¢ Example: 8 out of 10 people say â€œyesâ€ to the question â€œIs politics too polarized?â€ Let yes = 1 and no = 0. Average is:\nğ‘¥Ò§\n1 + 1 + 1 + 1 + 1 + 1 + 1 + 1 + 0 + 0 8 = =\n= 0.8\nâ€¢ We make confidence intervals and perform hypothesis tests for proportions using the exact same methods used for means. Only differences are:\nâ€¢ Sampling distribution of proportions follow z, rather than t (the difference is usually trivial)\nâ€¢ We denote the population proportion ğœ‹ and the sample proportion\nğœ‹à·œ\nâ€¢ Standard error of a sample proportion is ğ‘ ğœ‹à· =\nâ€¢ Here are results from a recent political poll:\nâ€¢ Say we want to make a confidence interval for the proportion of registered voters who say that â€œthings in the country are going in the right directionâ€. Denote this population proportion ğœ‹.\nâ€¢ 95% CI for ğœ‹:\nğœ‹à·œ\nÂ± ğ‘§ğ‘ğ‘Ÿğ‘–ğ‘¡ğ‘–ğ‘ğ‘ğ‘™\nâˆ— ğ‘ ğœ‹à·\n= 0.39 Â± 1.96 âˆ—\n= 0.39 Â± 0.022 = (0.368, 0.412)\nâ€¢ In jamovi, enter a column of frequencies for each value of the response variable, then use Frequencies / N â€“ Outcomes or Frequencies / 2 Outcomes:\n\n5.1.1 Toy example: skin cream and rashes\nâ€¢ Here is an example from the 2013 paper â€œMotivated Reasoning and Enlightened Self Governmentâ€, by Kahan et. al.:\nâ€¢ Putting the data into jamovi:\nâ€¢ Analyzing the data using Frequencies / Independent Samples: â€¢ Rash is the response variable â€¢ Skin cream is the predictor variable â€¢ Frequency tells how often each combination occurred. (note: if you had raw data where each row was a single response, you would not use Freq)\nâ€¢ Results! Thereâ€™s a lot in hereâ€¦\nâ€¢ Split bar plot: displays each cell in the contingency table as a bar:\nâ€¢ Here we can easily see that the largest number of people were those who got the skin cream and whose rash got better.\nâ€¢ But, we can also see that rashes got better at a higher rate for those who did not get the skin cream.\nâ€¢ Weâ€™ll cover the chi-square results soon. Right now, letâ€™s have jamovi directly compare proportions, using Frequencies / Independent Samples:\nâ€¢ Here, jamovi quantifies what we saw in the split bar plot: that the proportion of those who got better without the skin cream is greater than the proportion of those who got better with the skin cream:\nâˆ’ ğ‘ƒ ğµğ‘’ğ‘¡ğ‘¡ğ‘’ğ‘Ÿ ğ‘ğ‘œ = âˆ’0.0876\nâ€¢ We also see a 95% CI for this difference, which is fairly wide and just barely excludes zero\nâ€¢ And we see the p-value testing against:\nğ»0: ğœ‹\nâˆ’ ğœ‹ = 0\nâ€¢ The two-sided p-value is 0.047, so this result is just barely significant. Woohoo!"
  },
  {
    "objectID": "Ch5_Categorical.html#relative-risk",
    "href": "Ch5_Categorical.html#relative-risk",
    "title": "5Â  Chapter 5: Analyzing categorical data",
    "section": "5.2 Relative risk",
    "text": "5.2 Relative risk\nâ€¢ Instead of knowing the difference in proportions / probabilities, we may want to know their ratio. This would tell us how many times larger one is than the other.\nâ€¢ This is quantified by the â€œrelative riskâ€, a.k.a. â€œrisk ratioâ€ :\nğ‘…ğ‘… =\nğ‘ƒğ‘Ÿğ‘œğ‘ğ‘œğ‘Ÿğ‘¡ğ‘–ğ‘œğ‘› ğ´\nğ‘ƒğ‘Ÿğ‘œğ‘ğ‘œğ‘Ÿğ‘¡ğ‘–ğ‘œğ‘› ğµ\nâ€¢ The phrase â€œriskâ€ is used because this method is popular for comparing the risk of a negative outcome under two conditions (e.g.Â treatment and\n\n5.2.1 Relative Risk in jamovi\nâ€¢ â€œRelative Riskâ€ is an option under Comparative Measures. jamovi will give conditional probabilities based on the order of the data. Here, rash getting worse is Better is selected:\nğ‘ƒ ğµğ‘’ğ‘¡ğ‘¡ğ‘’ğ‘Ÿ ğ‘Œğ‘’ğ‘  /ğ‘ƒ ğµğ‘’ğ‘¡ğ‘¡ğ‘’ğ‘Ÿ ğ‘ğ‘œ\nğ‘ƒ ğ‘Œğ‘’ğ‘  ğµğ‘’ğ‘¡ğ‘¡ğ‘’ğ‘Ÿ /ğ‘ƒ ğ‘Œğ‘’ğ‘  ğ‘Šğ‘œğ‘Ÿğ‘ ğ‘’\n20\nğ‘ƒ ğµğ‘’ğ‘¡ğ‘¡ğ‘’ğ‘Ÿ ğ‘Œğ‘’ğ‘  /ğ‘ƒ ğµğ‘’ğ‘¡ğ‘¡ğ‘’ğ‘Ÿ ğ‘ğ‘œ â€¢ We see here that ğ‘…ğ‘… = 0.895.\nNotice the 95% CI is not very wide.\nâ€¢ We could also flip this ratio:\nğ‘ƒ ğµğ‘’ğ‘¡ğ‘¡ğ‘’ğ‘Ÿ ğ‘ğ‘œ /ğ‘ƒ ğµğ‘’ğ‘¡ğ‘¡ğ‘’ğ‘Ÿ ğ‘Œğ‘’ğ‘  â€¢ Now everything is reciprocated, e.g.Â 1\n0.809\n= 1.236\nğ‘ƒ ğµğ‘’ğ‘¡ğ‘¡ğ‘’ğ‘Ÿ ğ‘Œğ‘’ğ‘  /ğ‘ƒ ğµğ‘’ğ‘¡ğ‘¡ğ‘’ğ‘Ÿ ğ‘ğ‘œ\nâ€¢ Notice that, for â€œBetterâ€, RR is small but (just barely) statistically significant, because the CI does not contain 1. â€¢ For â€œWorseâ€, RR is large but (just barely) not statistically significant, because the CI contains 1.\nâ€¢ There is another very popular kind of ratio called an â€œodds ratioâ€, which we will consider more when we cover logistic regression."
  },
  {
    "objectID": "Ch5_Categorical.html#the-chi-square-test",
    "href": "Ch5_Categorical.html#the-chi-square-test",
    "title": "5Â  Chapter 5: Analyzing categorical data",
    "section": "5.3 The chi-square test",
    "text": "5.3 The chi-square test\nâ€¢ The chi-square (ğœ’2) test is a popular hypothesis test for comparing observed frequencies to expected frequencies under a null hypothesis.\nâ€¢ The null is typically that of â€œindependenceâ€ between two categorical variables, meaning the probability an observation falls into a category for one variable does not depend on its category for the other variable\n(As a side note, ğœ’2 is a distribution that is used for many purposes, including modeling distributions of variances. A statistic that is distributed chi-square is not necessarily being used in the context of the chi-square test outlined here)\nâ€¢ Staying with the skin cream example, here is the contingency table as jamovi initially reports it: â€¢ There is a lot being shown here. Top rows are observed frequencies, what jamovi calls â€œobservedâ€.\nâ€¢ The two middle rows are column % and row %.\nâ€¢ Notice that the column %â€™s sum to 100% down the columns, and the row\nâ€¢ We can have jamovi display the components of a chi-square test.\nâ€¢ The first of these are the expected counts under the null hypothesis of independence.\nâ€¢ To see how these â€œexpectedâ€ counts would suggest independence, consider the relative risk:\nğ‘…ğ‘… =\nğ‘ƒ(ğ‘Šğ‘œğ‘Ÿğ‘ ğ‘’|ğ‘ğ‘œ)\nğ‘ƒ(ğ‘Šğ‘œğ‘Ÿğ‘ ğ‘’|ğ‘Œğ‘’ğ‘ )\n28.8451 28.8451+99.1549\n28.8451 = 128\n0.2254 = = 1 0.2254\nâ€¢ The chi-square statistic compares the expected frequencies under the null (which we denote E) to the observed frequencies in the data (which we denote O).\nğœ’2 = à·\n2\nğ¸\nâ€¢ This is a general formula that can be used when you have one variable, two variable, three variables, etc.\nâ€¢ Most popular use is for two variables, as in this skin cream example.\nâ€¢ Just to verify the math, hereâ€™s the chi-square calculation for the upper left cell of the table:\nğ‘‚ âˆ’ ğ¸ 2 à· = ğ¸\n107 âˆ’ 99.2 2 + 99.2\n223 âˆ’ 230.8 2 + 230.8\n75 âˆ’ 67.2 2 + 67.2\n21 âˆ’ 28.8 2\n28.8\n= 3.94\nâ€¢ And here is the jamovi output showing the full chi-square test:\n(â€œPearsonâ€ chi-square is the classic chi-square test)\nâ€¢ Verifying that this chi-square statistic is indeed the sum of the chi-square values for each of the four cells:\n2 ğœ’2 = à· ğ¸\n= 0.6207 + 2.1336 + 0.2666 + 0.9165 = 3.937\nâ€¢ The p-value is found using the appropriate degrees of freedom for the chi-square distribution. We wonâ€™t cover this part.\nâ€¢ In this case, we see that the chi-square test just barely meets the standard for statistical significance (ğ‘ = 0.0472).\nâ€¢ This is in line with the other methods we used to analyze these data.\nâ€¢ Recall that the test for a difference in proportions was barely significant, and the risk ratios were just on either side of significance, depending on whether â€œrash got worseâ€ or â€œrash got betterâ€ was used as the outcome variable.\nâ€¢ The chi-square test can be used when we have frequencies for a single variable. All we have to do is specify expected counts or probabilities.\nâ€¢ Going back to the polling data, we can select Frequencies / N - Outcomes, and then make Answer â€œVariableâ€ and Frequency â€œCountsâ€.\nâ€¢ Suppose the null hypothesis is that equal numbers of voters feel the country is on the right vs.Â wrong track. Enter 0.5 for hypothesized probability:\nâ€¢ Here we get a very large chi-square statistic and a very small p-value.\nâ€¢ No surprise; the frequencies were very different!\nMy opinion on these\nâ€¢ I personally dislike statistical tests, and I really dislike tests that donâ€™t incorporate an interpretable statistic.\nâ€¢ So, I am not a big fan of this chi-square test. To compare rates for categorical variables, I prefer a 95% CI around either a relative risk or a difference in proportions, whichever seems more meaning for the question at hand.\nâ€¢ Note that a â€œchi-square testâ€ is quite general; it refers to any test whose test statistic follows a chi-square distribution. So you may see â€œchi-square testsâ€ that are not being used to test against a null of independence for categorical variables.\nâ€¢ The methods weâ€™ve covered in these notes are useful for analyzing fairly simple categorical data.\nâ€¢ In the next set of notes, we will look at logistic regression, which is a way to use regression modeling to predict the outcome of a categorical variable."
  },
  {
    "objectID": "Ch6_GLMs.html#part-1-logistic-regression",
    "href": "Ch6_GLMs.html#part-1-logistic-regression",
    "title": "6Â  Generalized Linear Models (GLMs)",
    "section": "6.1 Part 1: Logistic regression",
    "text": "6.1 Part 1: Logistic regression\nâ€¢ All of the regression methods weâ€™ve seen have involved models in which the response variable is normally distributed, given values for the predictor variables\nâ€¢ In other words, the residuals have been modeled as normal.\nâ€¢ What if we have a different kind of response variable? In particular, consider a binary response variable. Maybe the outcomes are â€œyesâ€ and â€œnoâ€, or â€œsuccessâ€ and â€œfailureâ€, or â€œpresentâ€ and â€œabsentâ€.\nâ€¢ Logistic regression is a type of â€œgeneralized linear modelâ€ (GLM) that works well for modeling binary outcome data.\nâ€¢ Before we get into logistic regression, though, letâ€™s see what happens if we use standard regression (sometimes called â€œordinary least squaresâ€, or OLS regression) with a binary response.\nâ€¢ Weâ€™ll use simulated data corresponding to a study of sexual harassment reporting at a university. (Brooks and Perot â€œReporting Sexual Harassment: Exploring a Predictive Modelâ€ (1991)).\nâ€¢ Here is data on whether or not sexual harassment at a university was reported, using the offensiveness of the behavior as a predictor variable: â€¢ Data points are â€œjitteredâ€ so that they donâ€™t fall right on top of one another.\nâ€¢ Suppose we want to predict the value of â€œReportâ€, using â€œOffensBehâ€.\nâ€¢ Here is the linear regression line. In this picture, the response variable takes on the values 0 and 1, and the data are not jittered. â€¢ The predicted value of â€œReportâ€ can be thought of as the predicted probability that Report=1 (for reported behavior)\nâ€¢ Note that this line can go below zero and above one. We donâ€™t want to predict probability greater than 1! A straight line is not great here. Logistic\n\n6.1.1 The logistic regression model\nAnother way of writing a linear regression model\nâ€¢ By now we are well familiar with the linear regression model:\nğ‘¦ğ‘– = ğ›½0 + ğ›½1ğ‘‹1ğ‘– + ğ›½2ğ‘‹2ğ‘– + â‹¯ + ğ›½ğ‘ğ‘‹ğ‘ğ‘–, +ğœ€ğ‘– ğœ€ğ‘–~ğ‘ğ‘œğ‘Ÿğ‘šğ‘ğ‘™(0, ğœ)\nâ€¢ Here is an equivalent way of writing it:\nğ‘¦ğ‘–~ğ‘ğ‘œğ‘Ÿğ‘šğ‘ğ‘™ ğœ‡ğ‘– = ğ›½0 + ğ›½1ğ‘‹1ğ‘– + ğ›½2ğ‘‹2ğ‘– + â‹¯ + ğ›½ğ‘ğ‘‹ğ‘ğ‘–\nâ€¢ In other words, the response variable is normally distributed with some mean ğœ‡, and the value of ğœ‡ is determined by the predictor (ğ‘‹) variables.\nWriting a logistic regression model\nâ€¢ We will take this approach to writing the logistic regression model.\nâ€¢ What we want is a regression equation that looks like this:\nğ‘¦ğ‘– = ğ›½0 + ğ›½1ğ‘‹1ğ‘– + ğ›½2ğ‘‹2ğ‘– + â‹¯ + ğ›½ğ‘ğ‘‹ğ‘ğ‘–\nBut that will work when ğ‘¦ğ‘– does not follow a normal distribution.\nâ€¢ Response variable ğ‘¦ takes on the values 0 and 1.\nâ€¢ Denote the probability that ğ‘¦ = 1 as ğœ‹.\nâ€¢ This can be written ğ‘¦~ğµğ‘’ğ‘Ÿğ‘›ğ‘œğ‘¢ğ‘™ğ‘™ğ‘–\n(The Bernoulli distribution is a distribution of 1â€™s and 0â€™s, where the probability of 1 is ğœ‹ and the probability of 0 is 1 âˆ’ ğœ‹)\nâ€¢ We will use regression to model ğœ‹, the probability that ğ‘¦ = 1. This is often thought of as the probability of a â€œsuccessâ€.\nâ€¢ If we wanted, we could use this model:\nğ‘¦ğ‘–~ğµğ‘’ğ‘Ÿğ‘›ğ‘œğ‘¢ğ‘™ğ‘™ğ‘– ğœ‹ğ‘– ğœ‹ğ‘– = ğ›½0 + ğ›½1ğ‘‹1ğ‘– + ğ›½2ğ‘‹2ğ‘– + â‹¯ + ğ›½ğ‘ğ‘‹ğ‘ğ‘–\nâ€¢ The Bernoulli distribution is a distribution of 1â€™s and 0â€™s, where the probability of 1 is ğœ‹ and the probability of 0 is 1 âˆ’ ğœ‹.\nâ€¢ The standard deviation of a Bernoulli distribution is ğœ‹(1 âˆ’ ğœ‹ ). So, ğœ‹ is the only parameter for this distribution. This is different from the normal distribution, which has two parameters ğœ‡ and ğœ.\nâ€¢ But, as we saw in the opening example, a linear model for probability can have serious deficiencies.\nâ€¢ So, instead of a linear model for probability ğœ‹, weâ€™ll make a linear model for a function of ğœ‹, so that the variable on the left hand side of the equation is linearly related to the variable(s) on the right.\nâ€¢ In logistic regression, we use the â€œlogitâ€ function, also known as â€œlog oddsâ€\nğ‘™ğ‘œğ‘”ğ‘–ğ‘¡\n= ln\n= â€œğ‘™ğ‘œğ‘” ğ‘œğ‘‘ğ‘‘ğ‘ â€\nâ€¢ Applied to our sexual harassment example, we would like to predict the probability that harassing behavior is reported. This probability is denoted ğœ‹\nâ€¢ Plugging this into the logit formula:\nğ‘™ğ‘œğ‘”ğ‘–ğ‘¡\n= ln\n= â€œğ‘™ğ‘œğ‘” ğ‘œğ‘‘ğ‘‘ğ‘ â€ ğ‘œğ‘“ ğ‘Ÿğ‘’ğ‘ğ‘œğ‘Ÿğ‘¡ğ‘–ğ‘›ğ‘”\nâ€¢ This will be our response variable for logistic regression.\nâ€¢ Logit vs.Â probability, visually:\n\n\n6.1.2 Odds\nâ€¢ To understand logistic regression, youâ€™ll need to understand odds.\nâ€¢ In casual English, â€œoddsâ€ and â€œprobabilityâ€ are often used interchangeably.\nâ€¢ In statistics, they are not the same thing. Odds tell you how likely one outcome is compared to another.\nâ€¢ For instance, you might hear that a football team has been given â€œ3 to 2â€ odds of winning a game. This means that their probability of winning is 3Î¤2 = 1.5 times as big as their probability of losing. Or, that theyâ€™d be expected to win 3 times for every 2 times they lost.\nâ€¢ Formally, consider some outcome A, where the probability of A occurring is written as â€œğ‘ƒ(ğ´)â€. In this case,\nğ‘œğ‘‘ğ‘‘ğ‘ \nğ‘ƒ(ğ´)\n1 âˆ’ ğ‘ƒ(ğ´)\nğ‘ğ‘Ÿğ‘œğ‘ğ‘ğ‘ğ‘–ğ‘™ğ‘–ğ‘¡ğ‘¦ ğ´ ğ‘œğ‘ğ‘ğ‘¢ğ‘Ÿğ‘ \nğ‘ğ‘Ÿğ‘œğ‘ğ‘ğ‘ğ‘–ğ‘™ğ‘–ğ‘¡ğ‘¦ ğ´ ğ‘‘ğ‘œğ‘’ğ‘  ğ‘›ğ‘œğ‘¡ ğ‘œğ‘ğ‘ğ‘¢ğ‘Ÿ\nâ€¢ This is the ratio of the probability A occurs to the probability A does not occur.\nâ€¢ Some probabilities and their associated odds:\nâ€¢ Think of odds(A) as â€œhow many times will A occur for every time A does not occur?â€\nâ€¢ Sometimes we add â€œto 1â€ to an odds statement, e.g.Â â€œodds of 4 to 1â€ means â€œthis outcomes occurs 4 times for every 1 time\nBack to the logistic regression model\nâ€¢ The response variable for logistic regression, again, is:\nğ‘™ğ‘œğ‘”ğ‘–ğ‘¡\n= ln\n= â€œğ‘™ğ‘œğ‘” ğ‘œğ‘‘ğ‘‘ğ‘ â€\nâ€¢ So, the full logistic regression model is :\nğ‘¦ğ‘–~ğµğ‘’ğ‘Ÿğ‘›ğ‘œğ‘¢ğ‘™ğ‘™ğ‘–\nâ€¢ We are not actually interested in log odds; we only make this conversion\nfor mathematical convenience. So, once we have ğ‘™ğ‘œğ‘”ğ‘–ğ‘¡ ğœ‹à·œğ‘– back via the inverse logit function:\n, we can get\nğ‘™ğ‘œğ‘”ğ‘–ğ‘¡âˆ’1\nâ€¢ In the context of the logistic regression model:\n\n\n6.1.3 jamovi example\nâ€¢ Applying this to the harassment data, we use Linear Models / Generalized Linear Models in jamovi and select Logistic under Categorical dependent variable.\nâ€¢ Note that â€œTarget Levelâ€ defaults to zero. Changing it to 1 makes sense in this case; we want to predict ğ‘ƒ(ğ‘…ğ‘’ğ‘ğ‘œğ‘Ÿğ‘¡ğ‘’ğ‘‘).\nâ€¢ jamovi also produces a Loglikelihood ratio test, but we will just focus on â€œParameter Estimatesâ€:\nâ€¢ Here is our estimated model:\nğ‘™ğ‘œğ‘” ğ‘œğ‘‘ğ‘‘ğ‘  = âˆ’1.7976 + 0.4869 âˆ— ğ‘‚ğ‘“ğ‘“ğ‘’ğ‘›ğ‘ ğµğ‘’â„\nâ€¢ Plugging in large and small values for OffensBeh:\nğ‘™ğ‘œğ‘” ğ‘œğ‘‘ğ‘‘ğ‘  ğ‘…ğ‘’ğ‘ğ‘œğ‘Ÿğ‘¡ğ‘’ğ‘‘ = âˆ’1.7976 + 0.4869 âˆ— 1 = âˆ’1.3107 ğ‘™ğ‘œğ‘” ğ‘œğ‘‘ğ‘‘ğ‘  ğ‘…ğ‘’ğ‘ğ‘œğ‘Ÿğ‘¡ğ‘’ğ‘‘ = âˆ’1.7976 + 0.4869 âˆ— 8 = 2.0976\nğœ‹à·œ|ğ‘‚ğ‘“ğ‘“ğ‘’ğ‘›ğµğ‘’â„ = 1:\nğ‘’âˆ’1.3107 1 + ğ‘’âˆ’1.3107 =\n0.2696 = 0.21 1.2696\nğœ‹à·œ|ğ‘‚ğ‘“ğ‘“ğ‘’ğ‘›ğµğ‘’â„ = 1:\nğ‘’2.0976 1 + ğ‘’2.0976 =\n8.1466 = 0.89 9.1466\nâ€¢ The slope coefficient is directly interpreted as change in log odds for a one unit increase in the predictor. â€œLog oddsâ€ are not of direct interest.\nâ€¢ Exponentiating both sides of the equation gives straight odds\nğ‘œğ‘‘ğ‘‘ğ‘  =\n= ğ‘’ğ›½0+ğ›½1ğ‘‹1ğ‘–+ğ›½2ğ‘‹2ğ‘–+â‹¯+ğ›½ğ‘ğ‘‹ğ‘ğ‘–\nâ€¢ This means that, for a one unit increase in ğ‘‹1, odds are multiplied by ğ‘’ğ›½1\nâ€¢ For the harassment data:\nğ‘™ğ‘œğ‘” ğ‘œğ‘‘ğ‘‘ğ‘  = âˆ’1.7976 + 0.4869 âˆ— ğ‘‚ğ‘“ğ‘“ğ‘’ğ‘›ğ‘ ğµğ‘’â„\nâ€¢ ğ›½áˆ˜ = 0.4869. So, for a one unit increase in OffensBeh, predicted odds of reporting are multiplied by ğ‘’0.4869 = 1.627\nâ€¢ In other words, there is about a 63% increase in odds of reporting when OffensBeh increases by one. NOTE: odds are not probabilities!\nâ€¢ Comparing probabilities and odds from this model:\nOffensBeh P(Report) Odds(Report) 1 0.212 0.27 2 0.305 0.44 3 0.417 0.71 4 0.537 1.16 5 0.654 1.89 6 0.755 3.08 7 0.834 5.01 8 0.891 8.15 9 0.930 13.26 10 0.956 21.57"
  },
  {
    "objectID": "Ch6_GLMs.html#part-2-poisson-and-negative-binomial-regression",
    "href": "Ch6_GLMs.html#part-2-poisson-and-negative-binomial-regression",
    "title": "6Â  Generalized Linear Models (GLMs)",
    "section": "6.2 Part 2: Poisson and negative binomial regression",
    "text": "6.2 Part 2: Poisson and negative binomial regression\nWeâ€™ll now look at two other popular GLMs: Poisson (â€œpwa-sawnâ€ roughly) and negative binomial.\nâ€¢ These are used for modeling count data, which can be extended to how often a categorical variable takes on some value. Thus Poisson regression can be used to model contingency table data.\n\n6.2.1 The Poisson distribution\nâ€¢ The Poisson distribution is a discrete probability distribution. A Poisson distributed variable takes on only positive integer values. The integer is referred to as â€œcountâ€ or â€œ# of eventsâ€.\nâ€¢ The Poisson distribution has a single parameter, ğœ† (â€œlambdaâ€), which is sometimes called the â€œrateâ€ parameter.\nâ€¢ ğœ† is both the mean and the variance of a Poisson distribution\nâ€¢ The probability function for the Poisson is: ğœ†ğ‘˜\nğ‘ƒ\n= ğ‘’ğœ†ğ‘˜! 4\nVisualizing the Poisson distribution\nğœ† = 1\nğœ† = 2\nğœ† = 5\nğœ† = 10\n\n\n6.2.2 The structure of a GLM\nâ€¢ In logistic regression, the response variable was ğ‘™ğ‘œğ‘”ğ‘–ğ‘¡(ğœ‹) = ln\nâ€¢ In Poisson regression, the response variable is ln(ğœ†)\nâ€¢ The reasoning will be that the natural log allows the estimated rate to be modeled as a linear function of some predictor variables.\nâ€¢ This is how GLMs work: they allow us to use non-normal response variables by expressing a function of their mean as a linear function of the predictors.\nâ€¢ A GLM has three parts:\n\nA response variable with some distribution\nA â€œlink functionâ€, ğ‘”(âˆ™), that is applied to the mean of the response variable.\nA linear expression of the predictor variables: ğ›½0 + ğ›½1ğ‘‹2 + ğ›½2ğ‘‹2 + â‹¯\n\nGLM examples\nâ€¢ Logistic regression uses ğ‘¦~ğµğ‘’ğ‘Ÿğ‘›ğ‘œğ‘¢ğ‘™ğ‘™ğ‘–(ğœ‹) as the response variable and\nğ‘”\n= ln\nas the link function.\nâ€¢ Poisson regression uses ğ‘¦~ğ‘ƒğ‘œğ‘–ğ‘ ğ‘ ğ‘œğ‘›(ğœ†) as the response variable and ğ‘” = ln(ğœ†) as the link function.\nâ€¢ Ordinary least squares (OLS) regression can also be considered a special case of a GLM. It uses ğ‘¦~ğ‘ğ‘œğ‘Ÿğ‘šğ‘ğ‘™(ğœ‡, ğœ) and the response variable and the identity function, ğ‘” = ğœ‡ as the link.\nâ€¢ Itâ€™s worth briefly noting that the mathematical method used to come up with parameter estimates for GLMs is not â€œleast squaresâ€. So, we are not getting our ğ›½áˆ˜ğ‘  by minimizing sums of squared residuals.\nâ€¢ Instead, the estimation procedure we use is called â€œmaximum likelihoodâ€. This method finds the values of the parameter estimates that maximize (i.e.Â make as large as possible for a given set of data) something called â€œthe likelihood functionâ€.\nâ€¢ The likelihood function takes a fixed set of data and an assumed distribution (e.g.Â normal), and gives the â€œprobability of the dataâ€, given\nâ€¢ So, if we have:\nğ‘Œğ‘–~ğ‘ğ‘œğ‘Ÿğ‘šğ‘ğ‘™\nâ€¢ Then the likelihood function is:\nâ€¢ Values of ğ›½áˆ˜ , ğ›½áˆ˜ , ğœà·œ are found that maximize this function, i.e.Â that maximize the probability of the data, given the parameter estimates.\n\n\n6.2.3 Poisson regression example\nâ€¢ Weâ€™ll use some General Social Survey data for this example.\nâ€¢ Poisson is good for modeling count data, so weâ€™ll use a response variable that takes the form of counts.\nâ€¢ For this example, the goal will be to look at the relationship (if any) between the number of sibling a person has, and the number of children that person has.\nâ€¢ Our question will be: do people with more siblings tend to have more children? And if so, can we quantify the relationship?\nâ€¢ It would be wise to collect data on covariates that we expect will also be related to the number of children someone has.\nâ€¢ An obvious one is age. Older people will have more children than younger people.\nâ€¢ We might also want to control for â€œcultureâ€. If people from different cultural backgrounds tend to have more or fewer children, then this would definitely induce a relationship between # of siblings and # of children.\nâ€¢ There are lots of possible ways to try account for cultural background.\nâ€¢ So, the variables will be:\nâ€¢ # of children â€¢ # of siblings â€¢ Age â€¢ Frequency of attending religious services\nâ€¢ Weâ€™ll just look at 2018 data. The GSS lets us choose any years we want, going back to 1972.\nâ€¢ This data set is on Canvas, as GSS_Children_Siblings.jmp\nâ€¢ First thing to do is plot our variables.\nâ€¢ Yikes! Thereâ€™s some cleaning to do. The instances of 98 siblings are not real data points.\nâ€¢ GSS data explorer website lets us look in detail at each variable. Here is part of the coding for â€œSIBSâ€:\nâ€¢ And hereâ€™s the coding for the variable CHILDS.\nâ€¢ So, CHILDS = 9 is a value for missing data. These should also be excluded.\nâ€¢ This should feel familiar â€“ remember how messy the NLSY data was in the heights analysis?\nâ€¢ Keep in mind that data in public databases often have idiosyncrasies like this.\nâ€¢ Here are the row selection options that will select all rows with invalid responses.\nâ€¢ Once selected, they can be excluded.\nâ€¢ Here is the distribution of CHILDS. â€¢ This looks a lot like a Poisson distribution. Hooray!\nâ€¢ To run the regression, use Linear Models / Generalized Linear Models and choose Poisson(overdispersion) for Frequencies.\nâ€¢ Here are results for a simple model, where # of siblings is the sole predictor of # of children. Weâ€™ll just look at the parameter estimates and the overdispersion statistic:\nâ€¢ Letting\nğœ†áˆ˜\nrepresent the predicted mean # of children, we have:\nln = 0.375 + 0.0631(ğ‘†ğ¼ğµğ‘†)\nâ€¢ You might not be surprised to learn that, due to the log link, the exponentiated slope is interpreted as the multiplicative change in the estimated value of the response variable, given a one unit increase in the predictor variable. ğ‘’0.0627 = 1.065\nâ€¢ So, increasing # of siblings by one is associated with an 6.5% increase in # of children.\nâ€¢ We can see that this is statistically significant, but it is also small.\nâ€¢ It is also not obvious that % change is the best way to quantify this. Maybe an OLS model would have been more interpretable.\nâ€¢ It might be more desirable to relate an additive change in siblings to an additive change in children.\nâ€¢ Downside is that # of children is not normally distributed.\nâ€¢ As is often the case, we are trading some interpretability for a better\nâ€¢ The Poisson distribution makes a strong assumption: the mean should be equal to the variance.\nâ€¢ Often, we observe real data in which the variance is greater than the mean.\nâ€¢ This is referred to as â€œoverdispersionâ€.\nâ€¢ If mean = variance, this should be equal to 1. But it rarely is.\nâ€¢ If there is strong overdispersion, a negative binomial model will fit better.\nâ€¢ The overdispersion statistic is the ratio of the Pearson chi-square statistic to its degrees of freedom..\nâ€¢ For these data, it turns out that # of siblings, age, frequency of attending religious services, and the interactions between age and the other two variables are all statistically significant and all improve model fit. Here are the parameter estimate results for this model:\nâ€¢ The other predictor variables and the interactions can be interpreted in the usual ways.\nâ€¢ One thing to notice is that the estimate for SIBS has not changed much. So, while the other covariates and interactions matter, they donâ€™t substantially change our interpretation of the SIBS predictor.\n\n\n6.2.4 Negative binomial regression\nâ€¢ There is also still some overdispersion, though less than there was before:\nâ€¢ Remember that the Poisson distribution only has one parameter. This limits its flexibility.\nâ€¢ The negative binomial distribution is similar to the Poisson distribution, but it is more flexible, and may be a better choice in the presence of overdispersion.\nâ€¢ The negative binomial distribution is also a distribution for count data. It is interpreted as giving the number of â€œsuccessâ€ before a certain number of â€œfailuresâ€ occur.\nâ€¢ There are two parameters: ğ‘, the probability of success, and ğ‘Ÿ, the number of failures at which counting stops.\nâ€¢ The mean of the negative binomial distribution is ğ‘Ÿ\n= ğ‘Ÿ âˆ— ğ‘œğ‘‘ğ‘‘ğ‘ (ğ‘ ğ‘¢ğ‘ğ‘ğ‘’ğ‘ ğ‘ )\nâ€¢ Example: suppose ğ‘ = 0.8 and ğ‘Ÿ = 2. We expect 2âˆ—0.8\n= 8 success before\nâ€¢ If a variable ğ‘¦ is distributed negative binomial, we denote it:\nğ‘¦~ğ‘ğµ(ğ‘Ÿ, ğ‘)\nâ€¢ The mean of the negative binomial distribution is ğ‘Ÿ\n= ğ‘Ÿ âˆ— ğ‘œğ‘‘ğ‘‘ğ‘ (ğ‘ ğ‘¢ğ‘ğ‘ğ‘’ğ‘ ğ‘ )\nâ€¢ Example: suppose ğ‘ = 0.8 and ğ‘Ÿ = 2. We expect 2âˆ—0.8\n= 8 success before\nwe observe two failures.\nâ€¢ Or suppose ğ‘ = 0.5 and ğ‘Ÿ = 1. We expect 1âˆ—0.5\n= 1 success before we\nobserve 1 failure.\nâ€¢ For practical purposes, negative binomial regression will show better fit than Poisson regression in the presence of overdispersion.\nâ€¢ The tradeoff is that the interpretation is less generally applicable. It might not make sense to think of your count variable as # of successes for a certain # of failures.\nâ€¢ As with Poisson regression, negative binomial regression uses a GLM with a log link.\nNegative binomial example\nâ€¢ Here is where you select negative binomial regression in jamovi:\nâ€¢ Under â€œGeneralized Linear Modelsâ€, under â€œFrequenciesâ€ choose â€œNegative Binomialâ€.\nNegative binomial results\nâ€¢ These results are awfully similar to the Poisson results.\nâ€¢ It is often the case that different statistical methods designed for the same purpose will with similar results."
  },
  {
    "objectID": "Ch7_Mixed.html#test-for-screen-reader",
    "href": "Ch7_Mixed.html#test-for-screen-reader",
    "title": "7Â  Chapter 7: Mixed-effects models",
    "section": "7.1 Test for screen reader:",
    "text": "7.1 Test for screen reader:\n\n\n\n\n    \n        \n        \n        Results\n        \n\n\n    \n\n\n\nFixed Effect Omnibus tests\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nF\nNum df\nDen df\np\n\n\n\n\nSoil\n\n1.769\n\n1\n\n8.000\n\n0.220\n\n\n\n\nNote. Satterthwaite method for degrees of freedom"
  },
  {
    "objectID": "Ch7_Mixed.html#part-1-repeated-measures",
    "href": "Ch7_Mixed.html#part-1-repeated-measures",
    "title": "7Â  Chapter 7: Mixed-effects models",
    "section": "7.2 Part 1: Repeated measures",
    "text": "7.2 Part 1: Repeated measures\nâ€¢ This module covers data analysis methods for â€œrepeated measuresâ€ data.\nâ€¢ â€œRepeated measuresâ€ refers to measuring the same subjects (people, trees, dogs, cities, cells, widgets, etc) more than once.\nâ€¢ Studies that use repeated measures are often referred to as â€œwithin subjectsâ€ studies. The idea is that multiple measurements within the same subject will be compared.\nâ€¢ Mixed models are popular tools for analyzing repeated measures data.\nâ€¢ We will see these models formally in the next notes.\nâ€¢ These notes introduce the problems and opportunities that arise from the use of repeated measures data.\n\n7.2.1 The independence assumption\nâ€¢ We have looked at model assumptions in this class, e.g.\nğœ€ğ‘–~ğ‘ğ‘œğ‘Ÿğ‘šğ‘ğ‘™(0, ğœ)\nâ€¢ There is one that has been left out. The complete way to write this statement is:\niid ğœ€ğ‘– ~\nğ‘ğ‘œğ‘Ÿğ‘šğ‘ğ‘™(0, ğœ)\nâ€¢ The â€œiidâ€ means â€œindependent and identically distributed.\niid ğœ€ğ‘– ~\nğ‘ğ‘œğ‘Ÿğ‘šğ‘ğ‘™(0, ğœ) says that ğœ€ğ‘– takes on values that are independently\nsampled from the same normal distribution.\nâ€¢ â€œindependently sampledâ€ means that the value of the next data point is not dependent upon the value of the previous data point.\nâ€¢ Every method we have used assumes independence of the response variable, conditional on the predictor(s).\nâ€¢ But, if the same subjects are measured multiple times, then the multiple observations on each subject will not be independent.\nâ€¢ Example: suppose we take a random sample of 10 people, have each solve a maze, and record how much time it takes each person to solve it. Call this time ğ‘‡.\nâ€¢ We might model the times as i.i.d. normal:\nğ‘–ğ‘–ğ‘‘ ğ‘‡ğ‘– ~\nğ‘ğ‘œğ‘Ÿğ‘šğ‘ğ‘™(ğœ‡, ğœ)\nâ€¢ This means that, given a mean and standard deviation, each observed time ğ‘‡ğ‘– is uncorrelated with any previous times. This should make sense: there is no reason that the 3rd personâ€™s maze time should depend upon the 2nd personâ€™s maze time.\nâ€¢ But, if we take repeated measures on each subject, then our data will not be independent.\nâ€¢ For instance, suppose we have 10 observations on maze completion from 2 people, where the first 5 come from one person and the last 5 come from the other.\nâ€¢ Then I would expect correlation (non-independence) between the first 5 maze times, and between the last 5 times. Observation 5 should be closer to observations 1-4 than it is to observations 6-10.\n\n\n7.2.2 Cheating with repeated measures\nâ€¢ We will look at a fake data simulation of repeated measures data.\nâ€¢ In this simulation, suppose we have cows infected with the Staph bacteria.\nâ€¢ We are comparing two treatments for Staph. We have 6 cows, and each treatment is randomly assigned to 3 cows. We will do a t-test to compare mean infection levels (quantified on some arbitrary scale).\nâ€¢ We will also assume that the two treatments have an identical effect. Therefore the null hypothesis of no difference in means is true.\nâ€¢ The file cows_ttest.jmp has some fake data, simulated as ğ‘¦ğ‘–~ğ‘ğ‘œğ‘Ÿğ‘šğ‘ğ‘™(12,4) for both groups:\nâ€¢ Not surprisingly, the t-test gives no significant difference in means:\n95% ğ¶ğ¼ ğ‘“ğ‘œğ‘Ÿ ğœ‡1 âˆ’ ğœ‡2 =\n, pâˆ’value = 0.321\nâ€¢ Now suppose we measure infection level for each cow seven times after receiving treatment.\nâ€¢ The file cows_repeated_ttest.csv contains data simulating this scenario.\nâ€¢ For each cow, there are 7 observations. These all average out to the one observation per cow from the last data set.\nâ€¢ But check out these t-test results!\nâ€¢ Notice where it says degrees of freedom = 29.2. If your degrees of freedom exceeds your sample size, something is wrong.\nâ€¢ Comparing the two t-tests:\nâ€¢ The mean difference is nearly the same for both.\nâ€¢ The standard error for the repeated measures data is much smaller.\nâ€¢ Remember that the t-test is just a simple regression model:\nğ‘–ğ‘–ğ‘‘ ğ‘¦ğ‘– = ğ›½0 + ğ›½1ğºğ‘Ÿğ‘œğ‘¢ğ‘ğ‘– + ğœ€ğ‘–, ğœ€ğ‘– ~\nğ‘ğ‘œğ‘Ÿğ‘šğ‘ğ‘™\nâ€¢ In this case, ğ›½áˆ˜ is the estimated mean difference in infection levels between the two groups.\nâ€¢ Thereâ€™s nothing wrong with this estimate on its own. However its standard error is computing using an incorrect assumption: that all of the observations within each group are independent.\nEffective sample size\nâ€¢ In this case, the apparent sample size is ğ‘› = 42, or ğ‘› = 21 per group.\nâ€¢ But the â€œeffectiveâ€ sample size is ğ‘› = 6, or ğ‘› = 3 per group.\nâ€¢ Taking lots of observations from each animal and doing a t-test on all the data amounts to falsely inflating the sample size without having to actually collect more data.\nâ€¢ Main lesson: observations that are correlated should not be treated as independent!\n\n\n7.2.3 Cheating ourselves with repeated measures\nâ€¢ Treating repeated measures as independent can fool us into thinking weâ€™ve discovered an association when really there is none.\nâ€¢ It can also do the opposite: it can fool us into thinking we see no association when really there is one.\nâ€¢ This will be illustrated with another toy example. In this case, we will again consider collecting data on infection level in cows under two treatments. This time, weâ€™ll imagine collecting repeated observations over the course of a week.\nâ€¢ Here are the data in â€œwide formâ€. Treatment group B is highlighted to distinguish it from treatment group A.\nâ€¢ Two things to note:\no There is a lot of variability across cows. o Every cowâ€™s infection rating goes down over time\nâ€¢ Here are the data in â€œlong formâ€.\nâ€¢ Long form is needed for most analyses.\nâ€¢ We can use jamoviâ€™s Rj-code editor to wide form to long form. (Though using Excel might be easier) â€¢ The following code will convert the data from wide to long form. Note you will need to open the data in a new session of jamovi and rename the Day and Infection_level columns. 23\nConverting from wide to long form\nâ€¢ We must now transform the Day column of the longform data. We want the variable Day to take on nominal integer values\nâ€¢ Double click the Day column and in the Data menu select Transform / Using Transform / Create New Transformation.\nâ€¢ Here is a basic plot of infection rates across time.\nâ€¢ There appears to be a small downward trend, and lots of noise in the data.\nâ€¢ But we can account for this noise! It is due to different cows having different overall infection rates.\nâ€¢ Here is a similar plot, with the Y axis grouped by treatment.\nâ€¢ This shows a small decrease for treatment B.\nâ€¢ But, we are still treating these observations as though they are independent.\nâ€¢ They are not independent; they are\nâ€¢ Here is the same plot, but with â€œcow_IDâ€ as the overlay variable.\nâ€¢ Each cow now gets its own line. The downward trend for B is clear.\nâ€¢ Also, it is now clear that most of the variation in infection rate was due to differences between cows. Once this is accounted for, variability is low.\nAnalyzing the data\nâ€¢ These plots illustrate the idea behind accounting for repeated measures.\nâ€¢ In this module, we will learn how to incorporate repeated measures in statistical models.\nâ€¢ For now, letâ€™s look at some different way of analyzing the data that we just plotted.\nâ€¢ We can run regression models where infection rate is the response variable and some combination of treatment and day are the predictors.\nâ€¢ We know both variables matter, and that they interact â€“ the â€œeffectâ€ of day on infection level depends on treatment, and the â€œeffectâ€ of treatment on infection level depends on day.\nâ€¢ The next slide shows output for fitting this model:\nğ¼ğ‘›ğ‘“ğ‘’ğ‘ğ‘¡ğ‘–ğ‘œğ‘› ğ‘™ğ‘’ğ‘£ğ‘’ğ‘™ğ‘– = ğ›½0 + ğ›½1ğ‘‡ğ‘Ÿğ‘’ğ‘ğ‘¡ğ‘šğ‘’ğ‘›ğ‘¡ + ğ›½2ğ·ğ‘ğ‘¦ + ğ›½3ğ‘‡ğ‘Ÿğ‘’ğ‘ğ‘¡ğ‘šğ‘’ğ‘›ğ‘¡ âˆ— ğ·ğ‘ğ‘¦ + ğœ€ğ‘–\nğ‘–ğ‘–ğ‘‘ ğœ€ğ‘– ~\nğ‘ğ‘œğ‘Ÿğ‘šğ‘ğ‘™ 29\nâ€¢ ğ‘…2 is small, suggesting this model does not explain much variability in infection rates.\nâ€¢ Remember, we know that a lot of variability is due to differences between cows. This is all getting â€œabsorbedâ€ by the error term.\nâ€¢ Large error variance gives small ğ‘…2 and large standard errors for slopes\nAnalyzing the data, accounting for cows\nâ€¢ Here are results from analyzing these data using a mixed model. This mixed model accounts for differences between cows, and the fact that repeated measures taken on each cow are correlated.\nâ€¢ We will cover the details of how mixed models work in the next set of notes. For now, note the much larger ğ‘…2 = 0.83\nSide by side comparison\nâ€¢ The mixed model (on the bottom) gives smaller standard errors for Day and the interaction.\nâ€¢ Note what didnâ€™t change: the parameter estimates!\nChallenges from repeated measures data\nâ€¢ Repeated measures data can be challenging to analyze, if the design gets complicated. We will see some study designs in which it isnâ€™t immediately obvious how to set up an appropriate model.\nâ€¢ Repeated measures data can also be analyzed incorrectly (assuming independence) to artificially increase apparent sample size and get strong looking results that are not valid.\nOpportunities from repeated measures data\nâ€¢ Repeated measures data can also be very useful!\nâ€¢ â€œWithin subjectâ€ designs, in which subjects are measured repeatedly across time and / or conditions, can greatly enhance the precision and power of our inferences.\nâ€¢ This is because variability that would normally be accounted for by the error term can instead be attributed to overall differences between the subjects from whom repeated measurements were taken.\nâ€¢ We will get into the details of mixed modeling for repeated measures data"
  },
  {
    "objectID": "Ch7_Mixed.html#part-2-random-and-fixed-effects",
    "href": "Ch7_Mixed.html#part-2-random-and-fixed-effects",
    "title": "7Â  Chapter 7: Mixed-effects models",
    "section": "7.3 Part 2: Random and fixed effects",
    "text": "7.3 Part 2: Random and fixed effects\nâ€¢ The â€œmixedâ€ in â€œmixed modelsâ€ refers to a mix of random effects and fixed effects.\nâ€¢ All of the predictor variables weâ€™ve seen all semester have been â€œfixed effectsâ€. What this means will only make sense in comparison to a new kind of predictor: a â€œrandom effectâ€.\nSTAT 331\n\n7.3.1 Random effects\nâ€¢ Every predictor variable weâ€™ve seen this semester has been one for which we estimate and interpret a slope coefficient.\nâ€¢ Sometimes, though, we want our model to account for a variable that is important for explaining variability in the response variable, but for which we are not interested in estimating coefficients.\nâ€¢ A random effect (or random factor) will accomplish this. Random factors take on values that are treated as having been drawn at random from a larger population of possible values that might be different if we take a new sample. These random factors have coefficients (aka slopes)\nâ€¢ In the last set of notes, we imagined measuring the same cows over and over again. â€œCowâ€ should probably be treated as a random factor in these cases. The cows themselves were drawn from a larger population; we would not get the same cows again in a new study.\nâ€¢ Also, we wanted to account for differences in the mean infection levels for each individual cow, so that we could estimate standard errors appropriate to our study designs. And these mean infection levels should also be treated as random, in that they came from some population of possible mean infection levels.\nâ€¢ There are many interpretations of a â€œrandom effectâ€, and they arenâ€™t always helpful. From a 2005 paper by Andrew Gelman: 1. Fixed effects are constant across individuals, and random effects vary.\n\nEffects are fixed if they are interesting in themselves or random if there is interest in the underlying population.\nWhen a sample exhausts the population, the corresponding variable is fixed; when the sample is a small (i.e., negligible) part of the population the corresponding variable is randomâ€.\nâ€œIf an effect is assumed to be a realized value of a random variable, it is called\n\nâ€¢ So, interpretations of what random vs.Â fixed effects â€œreally meanâ€ will vary.\nâ€¢ But, formally, it isnâ€™t so ambiguous. â€œFixedâ€ effects are treated as having â€œfixedâ€ coefficients whose values we estimate and draw inference on (e.g.Â with confidence intervals and hypothesis tests).\nâ€¢ â€œRandomâ€ effects are treated as having â€œrandomâ€ coefficients drawn from some distribution. We wonâ€™t estimate individual coefficients, but we will estimate the variance of the distribution from which they came.\nSubjects and Nesting\nâ€¢ Our main motivation right now is to have a method of accounting for repeated measurements on the same subjects.\nâ€¢ So, in this class, we will look at mixed models for which the random effect is â€œsubjectâ€. In other words, we will make models that account for differences between subjects measured multiple times.\nâ€¢ In a study in which subjects are assigned to groups, each subject is assigned to one group.\nâ€¢ From a modeling perspective, subject is â€œnestedâ€ within group.\nâ€¢ In general, one variable is nested within the other if values of the nested variable only occur in certain categories of the variable it is nested within.\nâ€¢ In this case, cows are nesting within treatments because each cow is measures only in one treatment.\nâ€¢ A non-nested design would have each cow measured under each value of the other predictors.\nâ€¢ In other words, each cow would be measured under both treatment.\nâ€¢ Another example: say we are doing an education research study and we randomly sample 4 districts in a state, then 3 schools in each district, then 7 classrooms in each school\nâ€¢ In this case, classroom is nested within school, and school is nested within district.\nâ€¢ This is because each classroom exists in only one school, and each school exists in only one district.\nâ€¢ The notation for â€œsubject nested within groupâ€ is ğ‘ ğ‘¢ğ‘ğ‘—ğ‘’ğ‘ğ‘¡(ğ‘”ğ‘Ÿğ‘œğ‘¢ğ‘)\n\n\n7.3.2 The mixed effects model\nâ€¢ A mixed effects model contains both random and fixed effects.\nâ€¢ Applying this to the cows example, in which cows are assigned to one of two treatment groups and measured daily for seven days:\nğ¼ğ‘›ğ‘“ğ‘’ğ‘ğ‘¡ğ‘–ğ‘œğ‘› ğ‘Ÿğ‘ğ‘¡ğ‘’ğ‘– = ğ›½0 + ğ›½1ğ‘‡ğ‘Ÿğ‘’ğ‘ğ‘¡ğ‘šğ‘’ğ‘›ğ‘¡ğ‘– + ğ›½2ğ·ğ‘ğ‘¦ğ‘– + ğ›½3ğ‘‡ğ‘Ÿğ‘’ğ‘ğ‘¡ğ‘šğ‘’ğ‘›ğ‘¡ğ‘– âˆ— ğ·ğ‘ğ‘¦ğ‘–\n\nğ›¼ğ‘—ğ¶ğ‘œğ‘¤ğ‘—\n\nğ›¼ğ‘—~ğ‘ğ‘œğ‘Ÿğ‘šğ‘ğ‘™\n\nğœ€ğ‘–\n\n, ğœ€ğ‘–\n~ğ‘ğ‘œğ‘Ÿğ‘šğ‘ğ‘™(0, ğœ2)\nâ€¢ Donâ€™t worry too much about the notation details. The most important part is that â€œCowâ€ is random, while â€œTreatmentâ€ and â€œDayâ€ are fixed.\nâ€¢ We will not estimate coefficients for the different cows. This model treats each cow as having its own â€œrandom interceptâ€, meaning that the intercept, ğ›½0, gets adjusted by some amount for each individual cow.\nâ€¢ As we saw in the last set of notes, the purpose of this is to let the model estimate the effects of the â€œTreatmentâ€ and â€œDayâ€ variables, while taking accounts of the fact that different cows will have different overall mean infection rates.\n\n\n7.3.3 The mixed effects model in jamovi\nâ€¢ To fit this model in jamovi, use â€œLinear Modelsâ€, select â€œMixed Modelâ€, and then add the fixed predictors as factors and covariates and random predictors as cluster variables:\nâ€¢ Note that Cow_ID is the random effect. Treatment, Day, and their interaction are fixed effects.\nâ€¢ Note also jamovi will also require you to specify the random effects.\nâ€¢ jamovi gives the usual parameter estimates output, as well as a residual plot:\nâ€¢ Notice that â€œCow_IDâ€ is not listed under parameter estimates; only fixed coefficients are estimated. â€¢ Each cow has its own random coefficient, modeled as having been drawn from a normal distribution. 17\nSlope coding in â€œMixed Modelâ€\nâ€¢ Notice the categorical predictor â€œTreatmentâ€ lists â€œEffectâ€ as â€œA â€“ (B, A)â€. jamovi is coding this slope as the difference between Treatment A and the â€œmeanâ€ of Treatments A and B.\nâ€¢ So, if we were looking at Treatment B, weâ€™d apply a slope of -0.513.\n\n\n7.3.4 Applied example: â€œThe liking gapâ€\nâ€¢ The journal Psychological Science published many studies in which data are publicly available.\nâ€¢ The remaining slides reproduce results from a recent study published in Psychological Science on the difference between how much individuals â€œlikeâ€ other people and how much they perceive others â€œlikeâ€ them.\nâ€¢ The paper is available at: https://doi.org/10.1177%2F0956797618783714\nâ€¢ The basic setup is that volunteers were paired up (each pair is called a â€œdyadâ€) and directed to have a conversation for five minutes\nâ€¢ After this, participants rated their partners on some survey questions that the authors take as a measure of liking the other person.\nâ€¢ Participants also rated how much they thought they were liked.\nâ€¢ The study is looking for a â€œgapâ€ (i.e.Â difference) between volunteersâ€™ self-perception of how much their partners liked them, and how much their partners actually liked them.\nThe liking gap example\nâ€¢ From the results section of the paper:\nğ¿ğ‘–ğ‘˜ğ‘–ğ‘›ğ‘” ğ¼ğ‘›ğ‘‘ğ‘’ğ‘¥ğ‘– = ğ›½0 + ğ›½1ğ‘ ğ‘’ğ‘™ğ‘“ ğ‘œğ‘¡â„ğ‘’ğ‘Ÿ + ğ›½2ğ·ğ‘ğ‘¦ + ğ›¼ğ‘—ğ‘ğ‘–ğ‘‘ğ‘— + ğ›¾ğ‘˜ğ‘‘ğ‘–ğ‘‘ğ‘˜ + ğœ€ğ‘–\nğ›¼ğ‘—(ğ‘˜)~ğ‘ğ‘œğ‘Ÿğ‘šğ‘ğ‘™\n, ğ›¾ğ‘˜\n~ğ‘ğ‘œğ‘Ÿğ‘šğ‘ğ‘™\n, ğœ€ğ‘–\n~ğ‘ğ‘œğ‘Ÿğ‘šğ‘ğ‘™(0, ğœ2)\nâ€¢ The model statement starts looking complicated. We have two different random effects, one nested in the other.\nâ€¢ The ğ›¼â€™s and ğ›¾â€™s are modeled as random values.\nâ€¢ We have two fixed effects: self_other and Day. Their coefficients will\nğ¿ğ‘–ğ‘˜ğ‘–ğ‘›ğ‘” ğ¼ğ‘›ğ‘‘ğ‘’ğ‘¥ğ‘– = ğ›½0 + ğ›½1ğ‘ ğ‘’ğ‘™ğ‘“ ğ‘œğ‘¡â„ğ‘’ğ‘Ÿ + ğ›¼ğ‘—ğ‘ğ‘–ğ‘‘ğ‘— + ğ›¾ğ‘˜ğ‘‘ğ‘–ğ‘‘ğ‘˜ + ğœ€ğ‘–\nğ›¼ğ‘—(ğ‘˜)~ğ‘ğ‘œğ‘Ÿğ‘šğ‘ğ‘™\n, ğ›¾ğ‘˜\n~ğ‘ğ‘œğ‘Ÿğ‘šğ‘ğ‘™\n, ğœ€ğ‘–\n~ğ‘ğ‘œğ‘Ÿğ‘šğ‘ğ‘™(0, ğœ2)\nâ€¢ The random effect variances, ğœ2 and ğœ2, are estimated in the mixed ğ›¼ ğ›¾ model. These variances could be interpreted, but we will stick to interpreted the fixed effects in this class.\nAdding an interactionâ€¦\nâ€¢ The next section of the paper looks at personality variables as predictors:\nIn jamovi\nâ€¢ Weâ€™ll stop here. The paper goes on through many additional studies, and the data are all available via the supplemental materials link."
  },
  {
    "objectID": "Ch7_Mixed.html#part-3-worked-example",
    "href": "Ch7_Mixed.html#part-3-worked-example",
    "title": "7Â  Chapter 7: Mixed-effects models",
    "section": "7.4 Part 3: Worked example",
    "text": "7.4 Part 3: Worked example\nâ€¢ This set of notes on mixed models covers the analysis from a 2017 Psychological Science paper by Goudeau and Croizet, titled\nâ€œHidden Advantages and Disadvantages of Social Class: How Classroom Settings Reproduce Social Inequality by Staging Unfair Comparisonâ€\nâ€¢ This paper presents three studies on one topic. These notes cover studies 1 and 3; study 2 is reserved for a homework assignment.\nâ€¢ This study was performed in France. The data are available at https://osf.io/rkj7y/ and the data file has a codebook sheet that defines the variables:\nâ€¢ In each study, 6th grade students are given a challenging reading comprehension assignment.\nâ€¢ Performance on the assignment is the dependent variable.\nâ€¢ The researchers investigate whether studentsâ€™ performance is associated with their awareness of their classmatesâ€™ performance, and whether this association can be moderated by studentsâ€™ awareness of the different levels of preparation given to different students.\nPaper abstract\n\n7.4.1 Study 1\nThe variables used in this study are:\nâ€¢ Performance (response) â€¢ Visibility condition (fixed effect) â€¢ Social class (fixed effect) â€¢ Visibility X Social class interaction (fixed effect) â€¢ School (random effect) â€¢ Classroom, nested within School (random effect)\nStudy 1 model\nğ‘ƒğ¸ğ‘…ğ¹ğ‘– = ğ›½0 + ğ›½1ğ‘‹1 + ğ›½2ğ‘ƒğ¶ğ‘† + ğ›½3ğ‘‹1 âˆ— ğ‘ƒğ¶ğ‘† + ğ›¼ğ‘— ğ‘˜ ğ¶ğ¿ğ´ğ‘†ğ‘†ğ¸ğ‘— ğ‘˜ + ğ›¾ğ‘–ğ¸ğ‘‡ğ´ğµğ¿ğ¼ğ‘†ğ‘†ğ¸ğ‘€ğ¸ğ‘ğ‘‡ğ‘˜ + ğœ€ğ‘–\nğ›¼ğ‘—~ğ‘ğ‘œğ‘Ÿğ‘šğ‘ğ‘™\n, ğ›¾ğ‘–\n~ğ‘ğ‘œğ‘Ÿğ‘šğ‘ğ‘™\n, ğœ€ğ‘–\n~ğ‘ğ‘œğ‘Ÿğ‘šğ‘ğ‘™(0, ğœ2)\nâ€¢ Refer to the codebook for variable definitions\nâ€¢ Note that in this study, subjects (students) are only measured once. The repeated measurements are on classrooms and schools.\nâ€¢ Also note that PCS compares working class (PCS=1) to upper class (PCS=3.)\nStudy 1 in JMP\nâ€¢ Note that the response variable, PERF, is score on a reading comprehension test, on a scale of 0 to 20 points.\nNotes on Study 1 random effects output\nâ€¢ â€œThe random effects covariance parameter estimatesâ€ table shows that there is much more residual (error) variance than there is variance across classrooms or across schools.\nâ€¢ One very odd result: the classroom variance estimate is negative! But variances cannot be negative.\nâ€¢ If we had deselected â€œunbounded variance estimatesâ€ under Fit Model, this would be zeroed out. It is a strange quirk of maximum likelihood variance estimation that negative estimates are possible. We can ignore this, and treat the classroom variance as just very small.\nâ€¢ We see strong â€œmain effectsâ€ for visibility condition (X1), social class (PCS), and their interaction.\nâ€¢ A general principle for models with strong interactions is that our interpretation should focus on the interactions.\nâ€¢ Here, the interaction coefficient is -2.69, and the coefficient for X1 is 1.12. â€¢ For the Effect of X1â€-1 - 1â€ refers to the â€œdifferences not visibleâ€ condition, in which students do not raise their hands when they have the answer. â€¢ For the Effect of PCS â€œ3 - 1â€ refers to change in score for upper class students vs.Â working class students. â€¢ So, the â€œeffectâ€ of visibility seems to apply to working class students but not to upper class students, since â€“2.69 and 1.12 nearly cancel each other out.\nâ€¢ Here is the plot reported in the paper, and the same plot in JMP:\nâ€¢ The practice of using a bar plot to show means is common, but flawed. The bars donâ€™t mean anything; all they do is go up to the means. â€¢ Here is what this looks like using boxplots instead.\nâ€¢ Notice that this plot shows variability in the data, where the bar plot does not.\n\n\n7.4.2 Study 3\nâ€¢ In Study 2, social class is not used. Rather, some students are given better preparation for the reading comprehension test than others.\nâ€¢ Similar results are found: those with worse preparation perform more poorly when students are told to raise their hands after determining the answer.\nâ€¢ In Study 3, the authors attempt to â€œundoâ€ this effect by informing the class that some students were given better preparation than others. Half the classrooms are made aware of this; the other half are not.\nâ€¢ The variables in this study are almost the same as in study 1, with these changes:\nâ€¢ X1 is â€œcontextâ€: -1 = awareness of the disadvantage 1 = unawareness of the disadvantage\nâ€¢ X2 is â€œlevel of familiarityâ€ with the reading material, based on intentional preparation given by the researchers: -1 = high level 1 = low level\nStudy 3 model\nğ‘ƒğ¸ğ‘…ğ¹ğ‘– = ğ›½0 + ğ›½1ğ‘‹1 + ğ›½2ğ‘‹2 + ğ›½3ğ‘‹1 âˆ— ğ‘‹2 + ğ›¼ğ‘—ğ¶ğ¿ğ´ğ‘†ğ‘†ğ¸ğ‘— + ğœ€ğ‘–\nğ›¼ğ‘—~ğ‘ğ‘œğ‘Ÿğ‘šğ‘ğ‘™\n, ğœ€ğ‘–\n~ğ‘ğ‘œğ‘Ÿğ‘šğ‘ğ‘™(0, ğœ2)\nâ€¢ In this data set, there is no variable for â€œschoolâ€, only one for â€œclassroomâ€. This isnâ€™t explained in the paper.\nâ€¢ As with before, the variance across classrooms is very small relative to the error variance.\nâ€¢ The largest overall effect is X2: level of familiarity with the material. Students who are better prepared score higher.\nâ€¢ The negative interaction shows that the difference in performance between those with higher vs.Â low familiarity is lower when students are aware of the difference in familiarity.\nâ€¢ The interaction of -5.81 just about cancels out the estimated slope for X1[-1] of 4.24.\nâ€¢ This shows that there is very little difference in scores between students with high preparation who are and are not aware of the advantage.\nâ€¢ It helps to look at the data.\nâ€¢ The generic â€œX1â€ and X2â€ have been replaced with more meaningful titles:\nâ€¢ And here are the bar plots shows in the paper.\nâ€¢ Again, the boxplots show more information.\nâ€¢ In this case, the boxplots reveal a potentially concerned â€œceiling effectâ€: many students earned the maximum possible score. This canâ€™t be seen from the bar plot. 24"
  },
  {
    "objectID": "Ch8_Mind_the_Gap.html#the-replication-crisis",
    "href": "Ch8_Mind_the_Gap.html#the-replication-crisis",
    "title": "8Â  Chapter 8: Minding the gap between science and statistics",
    "section": "8.1 The â€œreplication crisisâ€",
    "text": "8.1 The â€œreplication crisisâ€\nâ€¢ â€œThe replication crisisâ€ refers to a recent realization in many areas of science that previously published results often fail to replicate.\nâ€¢ Arguably, this started with Daryl Bemâ€™s 2011 paper â€œFeeling the Futureâ€, published in the Journal of Personality and Social Psychology.\nâ€¢ This paper used standard statistical tools to show strong evidence for pre-cognition, a.k.a. ESP. Many scientists were bothered by this, because they did not believe in ESP but they did believe in the statistical methods used in this paper!\nâ€¢ 2015â€™s â€œReproducibility Project: Psychologyâ€ by the Center for Open Science found that a large number of published experimental results in top Psychology journals failed to replicate.\nâ€¢ Similar studies in Cancer Biology, Medicine, Economics, Marketing, and Sports Science have found high rates of non-replication.\nâ€¢ Why do so many studies fail to replicate? There are many reasons, and statistical analysis plays a prominent role. These notes cover the role of statistics in the replication crisis.\nâ€¢ Many of the statistical issues surrounding the replication crisis concern â€œpowerâ€.\nâ€¢ Statistical power is the probability of rejecting a null hypothesis (??0), in the case that ??0 is false.\nâ€¢ In terms of real-world effects, if power is high, then there is a good chance of â€œdetectingâ€ an effect â€“ i.e.Â of declaring statistical significance.\nâ€¢ If power is low, then even if a real-world effect exists, the result of a hypothesis test will likely be â€œfail to rejectâ€ ??0; i.e.Â non-significance.\nâ€¢ A â€œType Iâ€ error occurs when a true ??0 is rejected. In other words, if we declare a result â€œstatistically significantâ€ even though no real-world effect exists, we are committing a Type I error. This is sometimes called a â€œfalse positiveâ€ outcome.\nâ€¢ A â€œType IIâ€ error occurs when a false ??0 is not rejected. In other words, if we declare a result â€œnot significantâ€ even though a real-world effect does exist, we are committing a Type II error. This is sometimes called a â€œfalse negativeâ€ outcome.\nâ€¢ Low statistical power implies a high chance of a false negative.\nâ€¢ The â€œtrueâ€ power of a statistical test is almost never known. To calculate power, one must assume the â€œtrueâ€ size of the effect being studied.\nâ€¢ For instance, power to reject the null hypothesis that a new drug is equally effective as a previous drug depends in part upon how different the two drugs are in effectiveness. But if we knew that, we wouldnâ€™t need to conduct a study!\nâ€¢ There has been research estimating average statistical power in various research fields, and reason to believe that low power studies are not uncommon. Which means that a lot of these replication failures might be â€œfalse negativesâ€, rather than the original studies being â€œfalse positivesâ€."
  },
  {
    "objectID": "Ch8_Mind_the_Gap.html#some-history-on-nhst",
    "href": "Ch8_Mind_the_Gap.html#some-history-on-nhst",
    "title": "8Â  Chapter 8: Minding the gap between science and statistics",
    "section": "8.2 Some history on â€œNHSTâ€",
    "text": "8.2 Some history on â€œNHSTâ€\nâ€¢ The form of hypothesis testing used today is sometimes called Null Hypothesis Significance Testing (NHST). It combines what used to be two different methods created by two â€œcampsâ€. The camps disagreed with one another, and did not get along personally.\nRonald Fisher Jerzy Neyman and\nâ€¢ Introduced the null hypothesis and p-value\nâ€¢ On interpreting small p-values: â€œEither an exceptionally rare chance has occurred, or the theory of random distribution is not trueâ€\nâ€¢ On the use of p &lt; 0.05 as a standard of evidence: â€œIn order to assert that a natural phenomenon is experimentally demonstrable we need, not an isolated record, but a reliable method of procedure. In relation to the test of significance, we may say that a phenomenon is experimentally demonstrable when we know how to conduct an experiment which will rarely fail to give us a statistically significant resultâ€\nâ€¢ Introduced the alternative hypothesis, Type I and II errors, and power.\nâ€¢ Reported only Type I and Type II error probabilities in testing; e.g.Â p-values of 0.001 and 0.04 would both be reported as falling under a pre-specified ?? = 0.05 Type I error rate. A Type II error rate should also be reported.\nâ€œWe are inclined to think that as far as a particular hypothesis is concerned, no test based upon a theory of probability can by itself provide any valuable evidence of the truth or falsehood of a hypothesisâ€\nâ€¢ NHST is a mix between the two approaches.\nâ€¢ Hypothesis testing comes from Neyman and Pearson. They did not believe p-values should be interpreted directly as quantifying evidence. They saw their procedure as simply a method for making a decision.\nâ€¢ The direct interpretation of p-values as quantifying the probability of obtaining results at least as extreme, assuming the null hypothesis is true, comes from Fisher. He did not use an alternative hypothesis, and he did not accept formal power analysis."
  },
  {
    "objectID": "Ch8_Mind_the_Gap.html#when-p-0.05-isnt-that-meaningful",
    "href": "Ch8_Mind_the_Gap.html#when-p-0.05-isnt-that-meaningful",
    "title": "8Â  Chapter 8: Minding the gap between science and statistics",
    "section": "8.3 When â€œp < 0.05â€ isnâ€™t that meaningful",
    "text": "8.3 When â€œp &lt; 0.05â€ isnâ€™t that meaningful\nâ€¢ We say results are â€œstatistically significantâ€ when we calculate a p-value less than the ?? level of significance, which is commonly 0.05.\nâ€¢ So, â€œsignificantâ€ results are the kind that would be unlikely to occur by chance, if the null hypothesis were true.\nâ€¢ There are some issues hereâ€¦\nâ€¢ The reason small p-values are considered â€œevidenceâ€ for a research hypothesis is that they are supposed to be unlikely to occur by chance alone.\nâ€¢ If there is flexibility in how to conduct an analysis, then it becomes more likely that small p-values will occur by chance.\nâ€¢ This has a derogatory name: â€œP-hackingâ€.\nâ€¢ Less derogatory alternatives: â€œResearcher degrees of freedomâ€, â€œThe garden of forking pathsâ€\nâ€¢ Examples of flexibility:\nâ€¢ Trying out different combinations of independent and dependent variables â€¢ Trying out different models and testing methods â€¢ Redefining variables (e.g.Â averaging over different combinations of survey responses, choosing different cut points for placing responses into categories)\nâ€¢ Discarding / retaining outliers â€¢ Transforming variables â€¢ Collecting more data than originally planned â€¢ Ceasing data collection earlier than originally planned\nCapitalizing upon flexibility, so that ??(?? &lt; 0.05) &gt; 0.05\nâ€¢ Flexible practices are perfectly justifiable in many contexts\nâ€¢ But â€“ to interpret a p-value as â€œthe probability of obtaining results at least this extreme, assuming the null is trueâ€, there can be no flexibility, unless accounted for using a correction (e.g.Â Bonferroni). Flexibility renders the classical p-value interpretation invalid.\nâ€¢ Another way of putting it: if you report a p-value as â€œthe probability of obtaining results this extreme, assuming the null is trueâ€, you are implicitly claiming you would have conducted the identical analysis, even if the data had been different in any way.\nâ€¢ Perhaps â€œ??0 is falseâ€ does not imply that the version of â€œnot ??0â€ you have in mind is true. â€¢ In observational studies, we must always think about possible confounders. Maybe there is a â€œsignificant associationâ€ between variables for some reason that isnâ€™t being considered. â€¢ In experimental studies, we must consider all the possible effects of our interventions. Did the experiment produce â€œsignificantâ€ results for a reason we didnâ€™t consider, e.g.Â poor control, biased question wording?\nâ€¢ Example: CSU has found that students who complete their Math and Composition AUCC requirements during their first year earn higher grades and have higher graduation rates than those who do not.\nâ€¢ Does this suggest that completing these requirements in the first year results in higher grades and a higher chance of graduation?\nâ€¢ Could this â€œstatistically significantâ€ effect be due to confounding factors? Is it even reasonable to suspect that students who complete these classes during their first year should have identical grades and graduation rates as those who donâ€™t? This is what the rejected ??0 states. Are we impressed that it is rejected?\nâ€¢ â€œAll we know about the world teaches us that the effects of A and B are always different â€“ in some decimal place â€“ for any A and B. Thus asking â€˜Are the effects different?â€™ is foolishâ€ - John Tukey\nâ€¢ The null can be used as a â€œStraw Manâ€ that no one really believes. Overturning a straw man null may not be that impressive.\nâ€¢ A good question to ask: â€œwould we expect this null to be true?â€\nâ€¢ If we are performing an experiment, the null is that the treatment does literally nothing. Is this common?\nâ€¢ If we are analyzing observational data and testing for â€œsignificant correlationâ€, the null is that there is precisely zero correlation at the population level. Do we think this is possible?\nâ€¢ Psychology/Philosopher/Statistician Paul Meehl called this the â€œcrud factorâ€: the extent to which everything is correlated with everything, at least at some small level.\nâ€¢ But, there are times when the null is plausible. â€¢ In manufacturing quality control analysis, the null is that â€œeverything is being produced the normal wayâ€. Defects show up as large deviations from this observed null distribution. â€¢ In Ronald Fisherâ€™s â€œThe Lady Tasting Teaâ€, the subject of the story claimed she could tell whether milk or tea had been poured into a cup first, simply by tasting the result. The null is that she couldnâ€™t tell. Seems plausible. â€¢ My personal rule is that I am only interested in p-values when I think the null is plausible.\nâ€¢ A significance test returns a binary outcome: the results are significant, or they are non-significant.\nâ€¢ Binary outcomes can produce binary thinking: the temptation to think â€œsignificantâ€ means â€œrealâ€ and â€œnon-significantâ€ means â€œdue to chanceâ€.\nâ€¢ Is coming to a binary choice even necessary? Why not just report a point estimate and standard error or 95% CI?\nâ€¢ If there is an actionable outcome, a binary choice might be necessary.\nâ€¢ Example: deciding whether to continue investing money into a research program.\nâ€¢ If the analysis is being done for the purpose of decision making, then a decision rule must be established, and â€œsignificanceâ€ can be such a rule. If the analysis is being done for the purpose of conveying information in data, I personally see no reason to add in a declaration of â€œsignificantâ€ or â€œnot significantâ€.\nâ€¢ Consider the t-test statistic:\n?? =\n???1 ? ??2?\n??????. ?????????? ????\n???1?\n???2\nâ€¢ As sample size increases, standard error decreases, the t-test statistic increases, and the p-value decreases.\nâ€¢ So, the bigger the sample size, the smaller the value of to achieve statistical significance.\n???1?\n???2 that is needed\nâ€¢ Upshot: if ?? is larger, very small effects will be â€œsignificantâ€.\nâ€œWomen had a slightly higher percentage of transactions for which positive feedback had been given in the year preceding the current transaction (99.60% for women and 99.58% for men, P &lt; 0.05)â€\n(http://advances.sciencemag.org/content/2/2/e1500599.full)\nâ€¢ Going back to Type I errors, here is the â€œType I error rateâ€:\n?? ??0 ???? ????????)\nâ€¢ What if weâ€™re actually interested in the reverse?\n?? ???????????? ??0)\nâ€¢ Classical hypothesis testing controls the probability of rejecting ??0, given ??0 is false, typically at 5%.\nâ€¢ It says nothing about the probability ??0 is false, given that ??0 has been rejected. ??(???????????? ??0 | ??0 ??????????) ? ??(??0 ?????????? | ???????????? ??0)\nâ€¢ Upshot: donâ€™t imagine that a Type I error rate of 0.05 implies that only 5% of significant results you see should be Type I errors!\nâ€¢ Jacob Cohen:\nWhatâ€™s wrong with NHST? Well, among many other things, it does not tell us what we want to know, and we so much want to know what we want to know that, out of desperation, we nevertheless believe that it does! What we want to know is â€œGiven these data, what is the probability that H0 is true?â€ But as most of us know, what it tells us is â€œGiven that H0 is true, what is the probability of these (or more extreme) data?â€ - The Earth is Round (p&lt;0.05) (1994)\nâ€¢ â€œStatistical tests, P values, confidence intervals, and power: a guide to misinterpretationsâ€ (2016) lists a large number of popular misinterpretations. Some highlightsâ€¦\nâ€¢ â€œThe p-value is the probability that the test hypothesis is true; for example, if a test of the null hypothesis gave P = 0.01, the null hypothesis has only a 1 % chance of being true; if instead it gave P = 0.40, the null hypothesis has a 40 % chance of being true.â€\nâ€¢ â€œThe p-value for the null hypothesis is the probability that chance alone produced the observed associationâ€\nâ€¢ â€œA null-hypothesis p-value greater than 0.05 means that no effect was observed, or that absence of an effect was shown or demonstrated.â€\nâ€¢ â€œStatistical significance is a property of the phenomenon being studied, and thus statistical tests detect significance.â€\nâ€¢ â€œWhen the same hypothesis is tested in two different populations and the resulting p-values are on opposite sides of 0.05, the results are conflicting.â€\n(note: all of these are wrong)"
  },
  {
    "objectID": "Ch8_Mind_the_Gap.html#the-sampling-distribution-of-the-p-value",
    "href": "Ch8_Mind_the_Gap.html#the-sampling-distribution-of-the-p-value",
    "title": "8Â  Chapter 8: Minding the gap between science and statistics",
    "section": "8.4 The sampling distribution of the p-value",
    "text": "8.4 The sampling distribution of the p-value\nâ€¢ It is intuitive to think that larger p-values are more likely to occur than smaller p-values when the null is true. â€¢ This is intuitive, but it is false. In most testing scenarios, all p-values are equally likely when the null is true. The distribution of p under ??0 is uniform:\nâ€¢ Simulation: https://csu-statistics.shinyapps.io/visualize_power/\nâ€¢ When the null is false, the distribution of p is right skewed. The greater the statistical power, the greater the skew.\nâ€¢ Here is the distribution of p for Cohenâ€™s d = 0.5 and n = 30, implying power ? 75%\nâ€¢ When null is false, smaller p-values are more likely than larger ones.\nâ€¢ Some people have the intuition that for weaker effects or lower power, significant p-values should be close to 0.05.\nâ€¢ This is false; even for low power, very small p-values are more likely than â€œmarginally significantâ€ p-values. Here is the distribution of p for Cohenâ€™s d = 0.2 and n = 30, impying power ? 19%\nâ€¢ Upshot: we should never see lots of p-values just below 0.05, even under low power.\nâ€¢ But â€“ there are papers in which many studies are performed, all of which produce p-values just below 0.05. There are bodies of published research in which p- values just below 0.05 occur too frequently.\nâ€¢ This suggests some combination of â€œpublication biasâ€ and â€œp-hackingâ€\nâ€¢ Formal test: â€œP-curveâ€ (www.p-curve.com)\nâ€¢ â€œP-curveâ€ takes a collection of p-values less than 0.05 and compares them to the uniform distribution expected under ??0\nâ€¢ If p-values close to 0.05 occur too frequently, the p-curve is consistent with a true ??0, despite all p-values being less than 0.05.\nobvious effects? â€¢ Simmons, Simonsohn, and Leif conducted an Amazon M-Turk study (n = 697) to estimate required sample size to detect a variety of â€œeffectsâ€ at ?? = 0.05. Examples:\nâ€¢ Men are taller than women (n = 12; i.e.Â n = 6 per group) â€¢ People who like spicy food eat more Indian food than people who donâ€™t like spicy food (n = 52) â€¢ People who like eggs eat more egg salad than people who donâ€™t like eggs (n = 96) â€¢ Smokers think smoking is less likely to kill someone than do non-smokers (n = 288) â€¢ Men weigh more than women (n = 92) (!!!!!!!!!!!)\nâ€¢ â€œPublication biasâ€ is the phenomenon by which statistically significant results are more likely to be published in a journal than results that are not statistically significant.\nâ€¢ There are many tools for trying to diagnose this (including P-curve)\nâ€¢ Test of Insufficient Variance: convert p-values to z-statistics. Expected variance of z-statistics is 1. Variance less than 1 could suggest â€œmissingâ€ studies.\nâ€¢ Plot effect size vs.Â standard error. Variance in effect sizes should decrease as standard error decreases, but effect size and standard error should not be correlated.\nâ€¢ Correlation could suggest â€œmissingâ€ studies.\nâ€œDo the results hit you between the eyes?â€\n(z = 1.96 is the threshold for statistical significance)\nâ€¢ Despite the fact that large sample sizes are needed to detect what seem like â€œmediumâ€ effects, we see lots of studies will small sample sizes reporting significant effects.\nâ€¢ Also, significant effects from small sample sizes are always large.\nâ€¢ A likely culprit: the combination of publication bias and low power.\nâ€¢ Low power: a typical consequence ofâ€¦\nâ€¢ Small sample sizes â€¢ Small effects â€¢ Noisy or imprecise measurements â€¢ Noisy or imprecise manipulations\nâ€¢ A nasty consequence of publication bias combined with low power: published effect sizes are biased upward. The lower the power, the greater the bias.\nâ€¢ A diagram of low power:\nâ€¢ Notice: â€œthe truthâ€ is NOT in the â€œreject the nullâ€ region. So when power is low, â€œthe truthâ€ is not statistically significant!\nâ€¢ Low power =&gt; wide CIs â€¢ Here, wide CI centered on true effect =&gt; not significant â€¢ For CI to exclude zero, sample effect must greatly overestimate true effect\nâ€¢ Left histogram: sampling distribution of the Cohenâ€™s d statistic (standardized diff. in means). Simulated so that power = 0.27 and population d = 0.5. â€¢ Right histogram: same thing, but after removing all d statistics that do not achieve statistical significance. â€¢ Upshot: conditioning an unbiased estimator on p &lt; 0.05 creates a biased estimator. The lower the power, the greater the bias.\nFrom Andrew Gelman:\nâ€¢ â€œPost-hocâ€ power analysis describes performing a power analysis on a data set after having conducted a significance test.\nâ€¢ Sometimes researchers will get a non-significant result, and suspect low power is the reason. So, they do a power analysis using the effect size and standard error from the data, and find low power.\nâ€¢ THIS IS INVALID! If p &gt; 0.05, then post-hoc power must be lower than 50%.\nâ€¢ So, â€œnon-significantâ€ results will always be identified as â€œlow powerâ€, post-hoc.\nâ€¢ R-Index: calculate observed power for each study. Compare average power to proportion of studies showing significance. Lower observed power could suggest missing studies.\ne.g.Â observed power = 0.6, 10 / 10 results significant; 6 / 10 expected if power = 0.6.\nâ€¢ If a collection of studies all show significant results, then average observed power must be greater than 50%. But, it might not be much greater.\nâ€¢ There are statistical problems that arise when only reporting significant results.\nâ€¢ There is also a scientific problem: are non-significant results really uninteresting?\nâ€¢ If the question is worth asking (â€œdo I have evidence for this substantive hypothesis?â€), isnâ€™t the answer worth knowing?\nâ€¢ The practice of throwing away non-significant results goes hand in hand with a false interpretation of NHST results: that non-significance implies â€œno effectâ€.\nâ€¢ p-value &lt; 0.05 is commonly interpreted as evidence for an effect.\nâ€¢ But, p-value &gt; 0.05 should not necessarily be interpreted as evidence for no effect.\nâ€¢ Example: suppose three studies (A, B, and C) all aimed to estimate a standardized difference in means in terms of Cohenâ€™s d:\n?? =\n???1 ? ????\n???2\nâ€¢ Suppose that for each study, a p-value is also computed, testing against the null of ??0: ??1 ? ??2 = 0\nâ€¢ A 95% CI for d is also constructed.\nâ€¢ In this example, all CIs contain zero, so all p-values exceed 0.05. All three tests are consistent with â€œno effectâ€.\nâ€¢ However, the first CI is also consistent with a very large effect, which could be positive or negative. The third CI is consistent with no effect or a very small effect . â€¢ But - the p-values are the same! In all three cases, p = 0.51.\nâ€¢ The article â€œArthroscopic partial meniscectomy versus placebo surgery for a degenerative meniscus tear: a 2-year follow-up of the randomized controlled trialâ€ assesses the effectiveness of a surgical procedure for treating a degenerative knee tear relative to a sham â€œplaceboâ€ surgery (!!!). https://ard.bmj.com/content/77/2/188\nâ€¢ The article finds a non-significant difference between treatment and placebo, and interprets this as the treatment being â€œno betterâ€ than placebo. Author conclude there is â€œno evidenceâ€ in favor of the treatment.\nâ€¢ However, later in the paper the authors make a much stronger argument: that the 95% CI for the difference in means is fully below the minimum clinically meaningful difference, which they established a priori:\nâ€¢ Note the sharp difference between these arguments:\no â€œSurgery was not effective because the difference between surgery and placebo was not statistically significant.â€\no â€œSurgery was not effective because the 95% CI for the difference between surgery and placebo fell entirely below the minimum clinically significant difference.\nâ€¢ The first argument says â€œthe estimated difference is no bigger than what would be expected by chance.â€ The second says â€œthe largest plausible value for the difference is still too small to be interesting.â€"
  },
  {
    "objectID": "Ch8_Mind_the_Gap.html#debates-over-statistical-signficance",
    "href": "Ch8_Mind_the_Gap.html#debates-over-statistical-signficance",
    "title": "8Â  Chapter 8: Minding the gap between science and statistics",
    "section": "8.5 Debates over statistical signficance",
    "text": "8.5 Debates over statistical signficance\nâ€¢ The use of statistical significance has always been controversial.\nâ€¢ Three recent high profile papers have argued for some different viewpoints on statistical significance.\n\n8.5.1 â€œRedefine statistical significanceâ€\nâ€¢ â€œRedefine Statistical Significanceâ€ (2017) calls for lowering the standard threshold for significance to p &lt; 0.005\nâ€¢ The argument: p &lt; 0.05 is too easy to obtain from noise.\nâ€¢ This paper proposes labeling p-values between 0.005 and 0.05 as â€œsuggestiveâ€, and p-values less than 0.005 as â€œsignificantâ€.\nâ€¢ An exception: if the procedure is pre-registered, p &lt; 0.05 can be labeled â€œsignificantâ€. So a distinction is drawn between exploratory and confirmatory data analyses.\n\n\n8.5.2 â€œJustify your alphaâ€\nâ€¢ â€œJustify Your Alphaâ€ (2017), written in response, calls for allowing flexibility in alpha levels rather than defaulting to p &lt; 0.05.\nâ€¢ The argument: alpha (a.k.a. the significance level) sets a trade-off between Type I errors and Type II errors.\nâ€¢ Smaller values of alpha lower Type I error rates but increase Type II error rates, and vice versa.\nâ€¢ The optimal trade-off will be different for different research fields. FDA drug trials should not use the same trade-off as exploratory research in brand new research fields.\n\n\n8.5.3 â€œAbandon statistical significanceâ€\nâ€¢ â€œAbandon Statistical Significanceâ€ (2017) calls for the elimination of thresholds entirely.\nâ€¢ The argument: â€œsignificanceâ€ is just a way of taking continuous phenomena (e.g.Â differences in means, probabilities, correlations) and forcing them into one of two categories.\nâ€¢ Instead, why not report the evidence on its own terms? No need to force it into an artificial and simplistic â€œeither / orâ€ distinction."
  },
  {
    "objectID": "Ch8_Mind_the_Gap.html#proposed-reforms",
    "href": "Ch8_Mind_the_Gap.html#proposed-reforms",
    "title": "8Â  Chapter 8: Minding the gap between science and statistics",
    "section": "8.6 Proposed reforms",
    "text": "8.6 Proposed reforms\nâ€¢ Pre-registration and registered reports: data analysis plans are stated ahead of time. This removes flexibility in analysis.\nâ€¢ With registered reports, data analysis plans are peer reviewed, and papers can be accepted for publication before results are known. This removes publication bias.\nâ€¢ Rewarding â€œopenâ€ practices.\nâ€¢ The Association for Psychological Science now does this using badges:\nâ€¢ â€œEquivalence testingâ€: an alternative to â€œacceptingâ€ a null that has not been rejected.\nâ€¢ Idea: establish a minimum effect size of interest (e.g.Â â€œweâ€™re not interested in this drug if it doesnâ€™t reduce blood pressure by at leastâ€¦â€)\nâ€¢ Make the null of the equivalence test be that the true effect is smaller than the minimum effect size of interest.\nâ€¢ If the null is rejected, then observed results are â€œequivalentâ€ to the null insofar as they are too small to be interesting.\nâ€¢ Visualization of equivalence testing, using confidence intervals:\nâ€¢ Note that results can be both â€œnot significantly differentâ€ and â€œnot significantly equivalentâ€.\nâ€¢ They can also be both â€œsignificantly equivalentâ€ and â€œsignificantly differentâ€!\nâ€¢ â€œThe New Statisticsâ€ proposes that we emphasize confidence intervals over p-values, as they are easier to understand and less noisy (i.e.Â they donâ€™t change as much across repeated samples)\nâ€¢ The Peer Reviewersâ€™ Openness Initiative calls on reviewers to require open data, open methods, and code that will reproduce analyses, so that reviewers can double check the analyses and results.\nâ€¢ The GRIM test, SPRITE test, and Statcheck are algorithms that check for internal consistency of reported results. They provide a â€œsanity checkâ€ that can detect potentially p-hacked data analyses.\nâ€¢ As stated at the outset, there is great controversy over the appropriate use of statistical methods!\nâ€¢ Some wise words from participants in this controversy:\n\nâ€œWe often hear itâ€™s too easy to obtain small p-values, yet replication attempts find it difficult to get small p-values with preregistered results. This shows the problem isnâ€™t p-values but failing to adjust them for cherry picking, multiple testing, post- data subgroups and other biasing selection effects.â€\n\n-Deborah Mayo, â€œDonâ€™t throw out the error control baby with the bad statistics bathwaterâ€\n\nâ€œIt seems to me that statistics is often sold as a sort of alchemy that transmutes randomness into certainty, anâ€uncertainty launderingâ€ that begins with data and concludes with success as measured by statistical significance â€¦ the solution is not to reform p-values or to replace them with some other statistical summary or threshold, but rather to move toward a greater acceptance of uncertainty and embracing of variation.\n\nAndrew Gelman, â€œThe problems with p-values are not just p-valuesâ€"
  },
  {
    "objectID": "Ch9_Other_Topics.html#formal-model-selection-tools",
    "href": "Ch9_Other_Topics.html#formal-model-selection-tools",
    "title": "9Â  Brief looks at major topics we didnâ€™t cover",
    "section": "9.1 Formal model selection tools",
    "text": "9.1 Formal model selection tools\n\n9.1.1 AIC / BIC\n\n\n9.1.2 Backwards and forwards selection\n\n\n9.1.3 Penalized regression (LASSO and Ridge)"
  },
  {
    "objectID": "Ch9_Other_Topics.html#bayesian-statistics",
    "href": "Ch9_Other_Topics.html#bayesian-statistics",
    "title": "9Â  Brief looks at major topics we didnâ€™t cover",
    "section": "9.2 Bayesian statistics",
    "text": "9.2 Bayesian statistics\n\n9.2.1 Probability as rational degree of belief\n\n\n9.2.2 Priors\n\n\n9.2.3 Things Bayes permits that classical methods donâ€™t"
  },
  {
    "objectID": "Ch9_Other_Topics.html#non-parametric-methods",
    "href": "Ch9_Other_Topics.html#non-parametric-methods",
    "title": "9Â  Brief looks at major topics we didnâ€™t cover",
    "section": "9.3 Non-parametric methods",
    "text": "9.3 Non-parametric methods\n\n9.3.1 Alternatives for t-tests and ANOVA\n\n\n9.3.2 Alternatives for regression"
  },
  {
    "objectID": "Ch9_Other_Topics.html#meta-analysis",
    "href": "Ch9_Other_Topics.html#meta-analysis",
    "title": "9Â  Brief looks at major topics we didnâ€™t cover",
    "section": "9.4 Meta-analysis",
    "text": "9.4 Meta-analysis\n\n9.4.1 Combining lots of studies\n\n\n9.4.2 Heterogeneity\n\n\n9.4.3 Interpretability"
  },
  {
    "objectID": "Ch9_Other_Topics.html#regression-to-the-mean",
    "href": "Ch9_Other_Topics.html#regression-to-the-mean",
    "title": "9Â  Brief looks at major topics we didnâ€™t cover",
    "section": "9.5 Regression to the mean",
    "text": "9.5 Regression to the mean"
  }
]
[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "STAT 331",
    "section": "",
    "text": "Introduction to STAT 331: Intermediate Applied Statistical Methods"
  },
  {
    "objectID": "index.html#purpose-and-intended-audience",
    "href": "index.html#purpose-and-intended-audience",
    "title": "STAT 331",
    "section": "Purpose and intended audience",
    "text": "Purpose and intended audience\nSTAT 331, as the title states, is an ‚Äúapplied‚Äù statistics course. It is intended for anyone who has taken at least one introductory level statistics course, and who wants to learn more about the use of statistical methods in quantitative research.\nIt covers many statistical tools that are usually considered too advanced for an introductory level class, but are nonetheless very popular. It also provides guidance on making data analysis decisions.\nMost assignments will involve looking up a published scientific paper for which the data are available and reproducing the main results. There are many worked examples that go through the statistical analyses in used in specific published papers.\nSTAT 331 doesn‚Äôt require any coding; the software we use is jamovi, a GUI-based (meaning ‚Äúgraphical user interface‚Äù, aka ‚Äúpoint-and-click‚Äù) statistical analysis package. Jamovi is built on to of R, and for those interested in using R it has the ability to display the R code it creates under the hood.\nSTAT 331 is not mathematically heavy in the traditional sense, but it isn‚Äôt math-free either. My approach is to present mathematical formulas and expressions when they are necessary or at least helpful for understanding the statistical it‚Äôs being covered. There are no mathematical character-building exercises or examples, and we won‚Äôt be computing things by hand - the software does all the computational work. Our job is to make sense of the results. And that usually requires looking at formulas and figuring out what they do. We will be constantly answering the question ‚Äúwhat does this number mean?‚Äù"
  },
  {
    "objectID": "index.html#structure-of-these-notes",
    "href": "index.html#structure-of-these-notes",
    "title": "STAT 331",
    "section": "Structure of these notes",
    "text": "Structure of these notes\nThese notes are broken up into 9 chapters (or ‚Äúmodules‚Äù):\n\nChapter 1: Review of classical inference\nChapter 2: Model building with linear regression\nChapter 3: Assessing and improving model fit\nChapter 4: ANOVA-based methods\nChapter 5: Analyzing categorical data\nChapter 6: Generalized Linear Models (GLMs)\nChapter 7: Mixed-effects models\nChapter 8: Minding the gap between science and statistics\nChapter 9: Brief looks at major topics we didn‚Äôt cover\n\nYou can access chapters and subsections directly through the table of contents."
  },
  {
    "objectID": "index.html#really-good-online-books",
    "href": "index.html#really-good-online-books",
    "title": "STAT 331",
    "section": "Really good online books",
    "text": "Really good online books\nThese notes will frequently reference some other freely available online statistics books:\nLearning statistics with jamovi, by Danielle Navarro and David Foxcroft\nIntroduction to Regression Analysis in R, by Kayleigh Keller\nAnswering questions with data, by Matthew J. C. Crump, Danielle J. Navarro, and Jeffrey Suzuki\nStatistical Analysis with The General Linear Model, by Jeff Miller and Patricia Haden"
  },
  {
    "objectID": "Ch1_Review.html#module-1-overview",
    "href": "Ch1_Review.html#module-1-overview",
    "title": "1¬† Chapter 1: Review of classical inference",
    "section": "1.1 Module 1 overview:",
    "text": "1.1 Module 1 overview:\nThese notes briefly cover material from your introductory statistics course that will be relevant in STAT 331.\nFor a more in-depth review, consult the OpenIntro text, or just do an internet search."
  },
  {
    "objectID": "Ch1_Review.html#distributions",
    "href": "Ch1_Review.html#distributions",
    "title": "1¬† Chapter 1: Review of classical inference",
    "section": "1.2 Distributions",
    "text": "1.2 Distributions\nThe term ‚Äúdistribution‚Äù will be used a lot.\nA distribution gives the values a variable takes on, and how often it takes them on.\nExamples: normal distribution, uniform distribution, distribution of exam scores, distribution of heights‚Ä¶"
  },
  {
    "objectID": "Ch1_Review.html#commonly-used-statistics",
    "href": "Ch1_Review.html#commonly-used-statistics",
    "title": "1¬† Chapter 1: Review of classical inference",
    "section": "1.3 Commonly used statistics",
    "text": "1.3 Commonly used statistics\n\n1.3.1 Mean\nMean and median identify the center of a data set or distribution.\nThe mean of a variable \\(X\\) is denoted \\(\\bar{X}\\).\nTo calculate the mean of a data set, add up all the values of a variable and divide by how many there are.\n\\[\n\\bar{X} = \\frac{\\sum^n_{i=1}x_i}{n}\n\\]\n\n\n1.3.2 Median\nMedian is the ‚Äúmiddle‚Äù number in a data set. To find the median, put the data values in order from smallest to largest, and identify the number in the middle.\nIf there are an even number of data points, the median is the average of the middle two numbers:\n\n\n\n\n\n\n\n1.3.3 Mean vs.¬†Median\nIn statistical inference (the process of generalizing from sample to population), we most often draw inference on the population mean.\nSometimes, though, the median is a more sensible statistic than the mean.\nThis is usually the case when we are studying a ‚Äúskewed‚Äù distribution.\nSkewed distributions are distributions that take on values that are extreme (or outlying) values.\nExample: income. Most households have incomes between $20,000/yr and $100,000/yr. A handful of households have incomes in the millions or billions of dollars per year.\nThe mean is affected by outliers. The median is not. This is why we often hear about ‚Äúmedian household income‚Äù rather than ‚Äúmean household income‚Äù.\n\n\n1.3.4 jamovi example: mean vs.¬†median\nTry creating a skewed data set in jamovi, then analyzing it by selecting for mean and median in the Statistics drop down menu in Descriptives, found under Exploration in the Analysis tab\nTo create a new dataset, enter data into the blank data table jamovi creates by default:\n\n\n\n\n\n\n\n\n\n\n\n\n1.3.5 Variance and standard deviation\nThe center of a distribution is quantified by the mean or the median.\nThe variability (i.e.¬†spread) of a distribution is quantified by the standard deviation, which is closely related to the variance.\nThe variance of a distribution or data set is denoted \\(s^2\\):\n\\[\ns^2 = \\frac{\\sum^n_{i=1}(x_i - \\bar{x})^2}{n-1}\n\\]\nThe standard deviation is simply the square root of the variance\n\\[\ns = \\sqrt{\\frac{\\sum^n_{i=1}(x_i - \\bar{x})^2}{n-1}}\n\\]\nThink of this as the ‚Äústandard‚Äù amount by which values deviate from their mean.\nWe will most often look at standard deviation, because it is the more interpretable of the two statistics. It is in the same units as the original variable.\n\n\n1.3.6 The correlation coefficient\nThe correlation coefficient, ùëü, quantifies the extent to which two variables (call them X and Y) move together:\n\\[\nr = (\\frac{1}{n-1})\\sum^n_{i=1}\\frac{(x_i - \\bar{x})(y_i-\\bar{y})}{s_xs_y} = (\\frac{1}{n-1})\\sum^n_{i=1}z_{x_i}z_{y_i}\n\\]\nDon‚Äôt worry too much about the formula. The most important things to know are:\nWhen two variables move in the opposite direction (i.e.¬†when one gets bigger, the other gets smaller), \\(r\\) is negative.\nWhen they move in the same direction, \\(r\\) is positive.\n\\(r = 0\\) means no correlation. \\(r =1\\) means perfect positive correlation. \\(r = -1\\) means perfect negative correlation."
  },
  {
    "objectID": "Ch1_Review.html#statistics-and-parameters",
    "href": "Ch1_Review.html#statistics-and-parameters",
    "title": "1¬† Chapter 1: Review of classical inference",
    "section": "1.4 Statistics and parameters",
    "text": "1.4 Statistics and parameters\nA statistic is any value calculated from data.\nA parameter is any value pertaining to a population.\nThe values of unknown parameters are ‚Äúestimated‚Äù using statistics.\nExample: a sample mean can be used to estimate a population mean. i.e.¬†\\(\\bar{X}\\) estimates \\(\\mu\\)."
  },
  {
    "objectID": "Ch1_Review.html#sampling-distributions",
    "href": "Ch1_Review.html#sampling-distributions",
    "title": "1¬† Chapter 1: Review of classical inference",
    "section": "1.5 Sampling distributions",
    "text": "1.5 Sampling distributions\nA sampling distribution is the distribution of values a statistic takes on, under repeated sampling.\nFor example, the Central Limit Theorem states that the sampling distribution of \\(\\bar{X}\\) will be normal, so long as the sample size (\\(n\\)) is large enough.\nSampling distributions are important because most methods used in statistical inference invoke long run frequency properties.\nExample: if the sampling distribution of \\(\\bar{X}\\) is not very spread out, then the value of \\(\\bar{X}\\) should not change much if we take a new sample.\n\n1.5.1 Standard error\nStandard error is the standard deviation of a sampling distribution.\nIn other words, it is the amount of variability in the values a statistic takes on under repeated sampling.\nSo, if a statistic we calculate has a small standard error, we can infer that the value of that statistic is close to the value of the population parameter it is estimating. If it has a large standard error, its value might be very far away from the value of the parameter.\nExample: the standard error of \\(\\bar{X}\\) is \\(\\frac{s}{\\sqrt{n}}\\) i.e.¬†\\(s_{\\bar{X}} = \\frac{s}{\\sqrt{n}}\\)\n\n\n1.5.2 Sampling dist. and standard error, visually\nOn Canvas there is a Central Limit Theorem simulator.\nWhen the sample size is large, the distribution of the sample mean is not very spread out. In other words, its standard error is small.\nWhen the sample size is small, the standard error is large."
  },
  {
    "objectID": "Ch1_Review.html#confidence-intervals",
    "href": "Ch1_Review.html#confidence-intervals",
    "title": "1¬† Chapter 1: Review of classical inference",
    "section": "1.6 Confidence intervals",
    "text": "1.6 Confidence intervals\nA confidence interval (‚ÄúCI‚Äù) is an interval (i.e.¬†a left endpoint and a right endpoint) constructed around a statistic, when that statistic is being treated as an estimate for the value of an unknown parameter.\nTypical confidence intervals are constructed by adding a ‚Äúmargin of error‚Äù to, and subtracting it from, an estimate:\nùê∂ùêº ùëìùëúùëü ùëùùëéùëüùëéùëöùëíùë°ùëíùëü = ùëíùë†ùë°ùëñùëöùëéùë°ùëí ùëúùëì ùëùùëéùëüùëéùëöùëíùë°ùëíùëü ¬± ùëöùëéùëüùëîùëñùëõ ùëúùëì ùëíùëüùëüùëúùëü\nTypical margins of error are calculated by multiplying the standard error of the estimate by a ‚Äúcritical value‚Äù. A critical value comes from a known distribution and is given by a confidence level.\nExample: a 95% CI for a population mean uses a critical value from the \\(t\\) distribution (we won‚Äôt cover why this is).\nùê∂ùêº ùëìùëúùëü ùúá = \\(\\bar{x}\\) ¬± \\(ùë°_{ùëêùëüùëñùë°ùëñùëêùëéùëô}\\) ‚àó \\(s_{\\bar{x}}\\)\nAs long as \\(n\\) isn‚Äôt tiny, the 95% critical value from a \\(t\\) distribution is approximately 2:\nùê∂ùêº ùëìùëúùëü ùúá ‚âà \\(\\bar{x}\\) ¬± \\(2\\) ‚àó \\(s_{\\bar{x}}\\)\nConfidence intervals should capture the unknown parameter value being estimated. The confidence level gives how often the interval captures the parameter under repeated sampling.\nExample: 95% of all 95% CIs for \\(\\mu\\) capture \\(\\mu\\) .\nThe confidence level can also be thought of as the success rate of the method being used.\nSo, 95% confidence intervals have a 95% success rate in capturing the value of the unknown parameter.\nJust as with sampling distributions, we are invoking repeated sampling here. We say that a 95% CI can be trusted because it is created using a method that would ‚Äúwork‚Äù 95% of the time, if we were to keep taking new samples and keep constructing 95% CIs."
  },
  {
    "objectID": "Ch1_Review.html#confidence-intervals-quantify-uncertainty",
    "href": "Ch1_Review.html#confidence-intervals-quantify-uncertainty",
    "title": "1¬† Chapter 1: Review of classical inference",
    "section": "1.7 Confidence intervals quantify uncertainty",
    "text": "1.7 Confidence intervals quantify uncertainty\nThe most important characteristic of a CI is its width.\nWe are typically willing to believe that the unknown value of a parameter lies inside the confidence interval constructed from our data.\nIf the confidence interval is wide, there is a lot of uncertainty as to the true value of the parameter.\nIf the confidence interval is narrow, then our estimate for the value of the parameter is ‚Äúprecise‚Äù, in that it shouldn‚Äôt be wrong by much.\nCIs are narrow when the sample size is large and / or the standard deviation of our data is small.\nCIs are wide with the sample size is small and / or the standard deviation of our data is large.\nIMPORTANT: CIs, like all inferential statistical methods, are created under assumptions. We make distributional assumptions about our data (e.g.¬†normality). We assume our statistic is an unbiased estimate of the parameter, i.e.¬†it will not systematically differ from the parameter value under repeated sampling."
  },
  {
    "objectID": "Ch1_Review.html#confidence-interval-simulation-apps",
    "href": "Ch1_Review.html#confidence-interval-simulation-apps",
    "title": "1¬† Chapter 1: Review of classical inference",
    "section": "1.8 Confidence interval simulation apps",
    "text": "1.8 Confidence interval simulation apps\nThere is a confidence interval simulation app on Canvas, that demonstrates creating confidence intervals ‚Äúunder repeated sampling‚Äù. This app is from Brown University‚Äôs ‚ÄúSeeing theory‚Äù series .\nThere is also a ‚Äúsampling distribution and standard error‚Äù app on Canvas. It shows a population distribution for a normally distributed variable, a sample of data from that distribution, and the sampling distribution of the mean.\nThis app also super-imposes the standard error in pink."
  },
  {
    "objectID": "Ch1_Review.html#confidence-intervals-in-jamovi",
    "href": "Ch1_Review.html#confidence-intervals-in-jamovi",
    "title": "1¬† Chapter 1: Review of classical inference",
    "section": "1.9 Confidence intervals in jamovi",
    "text": "1.9 Confidence intervals in jamovi\nJamovi can create a 95% CI and any other summary statistics selected for using the Statistics menu of Descriptives\nFor example, when producing summary statistics for a variable using the Statistics drop down menu in Descriptives, you will need to select Confidence Interval for Mean found under Mean Dispersion. Jamovi will report ‚Äú95% CI mean lower bound‚Äù and ‚Äú95% CI mean upper bound‚Äù. These are the endpoints for the 95% CI for \\(\\bar{X}\\)."
  },
  {
    "objectID": "Ch1_Review.html#hypothesis-testing",
    "href": "Ch1_Review.html#hypothesis-testing",
    "title": "1¬† Chapter 1: Review of classical inference",
    "section": "1.10 Hypothesis testing",
    "text": "1.10 Hypothesis testing\nHypothesis testing is an inferential method in which a null hypothesis (\\(H_0\\)) is ‚Äútested‚Äù against. If the data are in strong enough disagreement with \\(H_0\\) , then \\(H_0\\) is rejected.\n\\(H_0\\) typically represents the proposition that ‚Äúthere is nothing of interest at the population level‚Äù, or ‚Äúthe proposed research hypothesis is not true‚Äù.\nIf \\(H_0\\) is rejected, then the result of the test is described as ‚Äústatistically significant‚Äù.\nExample: if we have data from a controlled experiment in which \\(\\mu_1\\) represents the population mean for the control group and \\(\\mu_2\\) represents the population mean for the treatment group, then we might test against the null hypothesis:\n\\[\nH_0: \\mu_1 = \\mu_2,\\text{which is equivalent to }H_0: \\mu_1 ‚àí \\mu_2 = 0\n\\]\nIf we reject \\(H_0\\), we say that the sample means, \\(\\bar{x}_1\\) and \\(\\bar{x}_2\\), are ‚Äúsignificantly different‚Äù. Or, equivalently, that \\(\\bar{x}_1 - \\bar{x}_2\\) is ‚Äúsignficantly different‚Äù from zero."
  },
  {
    "objectID": "Ch1_Review.html#the-test-statistic",
    "href": "Ch1_Review.html#the-test-statistic",
    "title": "1¬† Chapter 1: Review of classical inference",
    "section": "1.11 The test statistic",
    "text": "1.11 The test statistic\nThe strength of the evidence against \\(H_0\\) is quantified by a ‚Äútest statistic‚Äù, from which a ‚Äúp-value‚Äù is calculated.\nTest statistics are set up so that, the more the inconsistent the data are with \\(H_0\\), the larger the test statistic will be.\nExample: when testing \\(H_0: \\mu_1 ‚àí \\mu_2 = 0\\), we use the test statistic:\n\\[\nt = \\frac{\\bar{x}_1 - \\bar{x}_2}{s_{(\\bar{x}_1 - \\bar{x}_2)}} = \\frac{\\bar{x}_1 - \\bar{x}_2}{\\sqrt{\\frac{s_1^2}{n_1} + \\frac{s_2^2}{n_2}}} \\text{(where } s_{(\\bar{x}_1 - \\bar{x}_2)}\\text{ is the standard error of }\\bar{x}_1 - \\bar{x}_2)\n\\]"
  },
  {
    "objectID": "Ch1_Review.html#the-p-value",
    "href": "Ch1_Review.html#the-p-value",
    "title": "1¬† Chapter 1: Review of classical inference",
    "section": "1.12 The p-value",
    "text": "1.12 The p-value\nThe p-value is defined as the probability of getting a test statistic at least as large as the one calculated, if we assume \\(H_0\\) is true.\nVisually, the p-value is the area in the tail of the sampling distribution of the test statistic under \\(H_0\\)\n\n\n\n\n\nIf the p-value is less than the ‚Äúlevel of significance‚Äù (\\(\\alpha\\)), then \\(H_0\\) is rejected.\nBy far the most typical level of significance is \\(\\alpha = 0.05\\)\n\n\n\n\n\n\n1.12.1 Interpreting ‚Äústatistical significance‚Äù\n‚ÄúStatistically significant‚Äù results are those that produce a small p-value.\nSmall p-values result from data that would be unlikely to be obtained just by chance, if the null hypothesis were true.\nSo, when you hear that results are ‚Äústatistically significant‚Äù, you can interpret this as meaning ‚Äúthe data we obtained don‚Äôt look like the kind of data we‚Äôd expect to see just by chance‚Äù.\n\n\n1.12.2 Cautions regarding ‚Äústatistical significance‚Äù\nAs with confidence intervals, hypothesis tests require assumptions.\nThese will be covered in detail in the next module.\nThe most important distinction to be made right now is the distinction between statistical significance and practical importance.\nResults can be statistically significant, but still seem weak or unimpressive by practical standards.\nExample: this is a statistically significant correlation:\n\n\n\n\n\n(\\(r = 0.22\\), p-value = 0.011)\nExample: this is a statistically significant difference in means:\n\n\n\n\n\n(\\(\\bar{x}_1 = 19.7\\), \\(\\bar{x}_2 = 22.7\\), p-value = 0.0013)\nThe use of hypothesis testing is controversial.\nI personally do not like hypothesis testing, and I think that statistical significance is usually uninteresting.\nWe‚Äôll explore the debates surrounding statistical significance in module 2.\n\n\n1.12.3 Confidence intervals vs.¬†p-values\nFor now, we‚Äôll note that in many cases, confidence intervals can be used in place of p-values to perform a hypothesis test.\nIf a 95% CI excludes the null value (typically zero), then \\(H_0\\) is rejected at the \\(\\alpha = 0.05\\) level of significance.\nThe advantage of using a confidence interval rather than a p-value is that it is easier to make sense out of, and it quantifies uncertainty: the wider the CI, the more uncertainty there is regarding the value of the unknown parameter.\n\n\n1.12.4 Confidence intervals vs.¬†p-values example\n\n\n\n\n\n\\(\\bar{x}_1 = 19.7\\), \\(\\bar{x}_2 = 22.7\\), p-value = 0.0013\n95% CI for \\(\\mu_1 - \\mu_2: (-4.72,-1.19)\\)\n\n\n\n\n\nHere we see 95% CIs for differences in means, along with their corresponding p-values.\nNote how much the p-values change for small changes in the CIs.\nNote also that different CIs can correspond to the same p-value."
  },
  {
    "objectID": "Ch1_Review.html#cis-and-p-values-in-jamovi",
    "href": "Ch1_Review.html#cis-and-p-values-in-jamovi",
    "title": "1¬† Chapter 1: Review of classical inference",
    "section": "1.13 CIs and p-values in jamovi",
    "text": "1.13 CIs and p-values in jamovi\nCIs and p-values can be calculated for a huge variety of statistics.\nFor now, we will consider testing for a difference in means.\nIn jamovi, select an Independent Samples T-Test from T-Tests under the Analyses tab\nMake Max_Temp_Challenge be the Dependent variable (the one containing measurements), and make Vaccine be the Grouping variable (the one identifying which group the measurement belongs to).\nHere is an example using the Vaccine data set. This example uses sheet 3 of the Excel file, titled ‚ÄúH3N2_Clinical_Max‚Äù: Data will need to be prepared for test by swapping the rows so that Vaccine occurs first."
  },
  {
    "objectID": "Ch1_Review.html#cis-and-p-values-in-jmp",
    "href": "Ch1_Review.html#cis-and-p-values-in-jmp",
    "title": "1¬† Chapter 1: Review of classical inference",
    "section": "1.14 CIs and p-values in JMP",
    "text": "1.14 CIs and p-values in JMP\n\n\n\n\n\nHere, the p-value is \\(0.002\\)\nThe 95% CI for the difference in population mean max_temp is \\((‚àí2.07, ‚àí0.58)\\)\nThe confidence interval excludes zero and \\(p &lt; 0.05\\), so the difference in sample means is statistically significant."
  },
  {
    "objectID": "Ch1_Review.html#data-format-in-jamovi",
    "href": "Ch1_Review.html#data-format-in-jamovi",
    "title": "1¬† Chapter 1: Review of classical inference",
    "section": "1.15 Data format in jamovi",
    "text": "1.15 Data format in jamovi\nA final note on data formatting: this data set is in \"long form\", meaning each row is a single observation and each column is a variable.\njamovi's Independent Samples T-test requires long form data.\nSometimes you'll have data in \"wide form\", where each column is a group, and the rows do not correspond to single observations\nExample: here's some fake data in wide form:\n\n\n\n\n\nFit Independent Samples T-test cannot be used to compare these means.¬† jamovi thinks there are 4 observations, each with a measurement on Var1 and Var 2.¬†\n\n\n\n\n\nThis isn't what we want!\nTo transform the data into long form, we need to install the Rj ‚Äì Editor module which will allow us to run R-code in jamovi"
  },
  {
    "objectID": "Ch1_Review.html#installing-rj-in-jamovi",
    "href": "Ch1_Review.html#installing-rj-in-jamovi",
    "title": "1¬† Chapter 1: Review of classical inference",
    "section": "1.16 Installing Rj in jamovi",
    "text": "1.16 Installing Rj in jamovi\n\nNavigate to the Analyses tab\n\nClick on Modules in the top right of the jamovi window\nClick jamovi library\nScroll until you see \"Rj ‚Äì Editor to run R code\", click install\nYou should now see an R logo under the Analyses tab"
  },
  {
    "objectID": "Ch1_Review.html#wide-form-to-long-form-in-jamovi",
    "href": "Ch1_Review.html#wide-form-to-long-form-in-jamovi",
    "title": "1¬† Chapter 1: Review of classical inference",
    "section": "1.17 Wide form to long form in jamovi",
    "text": "1.17 Wide form to long form in jamovi\nTo switch from wide to long form, we will write a simple line of R-code using the Rj ‚Äì Editor module:¬†\nClick on Rj, it will open an empty window where we can enter R - code\n\n\n\n\n\nThe code below transforms the data into long form by stacking the data within Var1 and Var2 into a new column Data and creates a new column Labels to identify if data is from Var1 or Var2.\nThe data will output a csv file which we can import from a new session of jamovi\n\nImporting the transformed csv file to a new jamovi window shows our transformed long form data table. Note: column names will need to be updated\nCompare the two:\n\n‚ÄúLong form‚Äù\n\n\n\n\n\n\n\n‚ÄúWide form‚Äù\n\n\n\n\n\n\nNow Independent Samples T-test can be used to compare means:"
  },
  {
    "objectID": "Ch2_Model_Building.html#outline-of-notes",
    "href": "Ch2_Model_Building.html#outline-of-notes",
    "title": "2¬† Chapter 2: Model building with linear regression",
    "section": "Outline of notes:",
    "text": "Outline of notes:\n\nThe linear regression equation\nRegression analysis in jamovi\nSums of squares and mean squares\nInterpreting regression results\nApplied example (‚ÄúThe Binary Bias‚Äù)\nInteraction between variables\nApplied example: arthritis treatment data\nCentering predictor variables\nStandardizing predictor variables"
  },
  {
    "objectID": "Ch2_Model_Building.html#the-linear-regression-equation",
    "href": "Ch2_Model_Building.html#the-linear-regression-equation",
    "title": "2¬† Chapter 2: Model building with linear regression",
    "section": "2.1 The linear regression equation",
    "text": "2.1 The linear regression equation\nLinear regression, in its simplest form, is a method for finding the ‚Äúbest fitting‚Äù line through a set of bivariate (two variable) data:\n\nWhat is meant by ‚Äúbest fitting‚Äù will be addressed shortly. For now, think of the line as showing the underlying linear trend through a set of data.\nThe vertical distance between each data point and the line is called a ‚Äúresidual‚Äù. On the plot above, the red lines represent residuals. They quantify the amount by which a data point deviates from the underlying linear trend. Every point has a residual; the plot above only shows a few of them.\n\n2.1.1 The linear regression equation as a statistical model\nThe line that is drawn through data comes from a statistical model. A statistical model is a mathematical expression describing how data are generated. It has a fixed component and a random component. Think of the fixed component as describing the underlying relationship between variables, and the random component as describing any additional variability in data beyond what the fixed component describes.\nBelow is the standard linear regression model. The random component is represented by \\(``\\varepsilon_i\"\\). Everything from ‚Äú\\(\\beta_0\\)‚Äù up until ‚Äú\\(\\varepsilon_i\\)‚Äù is the fixed component.\n\\[\nY_i=\\beta_0+\\beta_1x_{1i} + \\beta_2x_{2i} + \\dots + \\beta_kx_{ki} + \\varepsilon_i \\\\\ni = 1, \\dots, n \\\\\n\\varepsilon_i \\sim \\text{Normal}(0,\\sigma^2)\n\\]\nHere‚Äôs what each term represents:\n\n\\(i\\) is the index term. It counts through the data, starting at \\(i = 1\\) and going through \\(i=n\\), where \\(n\\) is the sample size.\n\\(Y_i\\) is the \\(i^{th}\\) value of the outcome variable. When written in upper-case, \\(Y_i\\) is treated as a random variable whose value has not been observed. When written lower-case, \\(y_i\\) represents an observed data value.\n\\(Y_i\\) is often referred to as the ‚Äúresponse‚Äù variable, or the ‚Äúdependent‚Äù variable. These notes will use the term ‚Äúoutcome‚Äù variable. I prefer this term on the grounds that the others seem to imply causality: if \\(Y\\) is ‚Äúresponding‚Äù to \\(x\\), or ‚Äúdependent‚Äù on \\(x\\), then it sounds like changing the value of \\(x\\) will induce a change in the value of \\(Y\\).\n\\(x_{1i}\\) is the \\(i^{th}\\) value of the first predictor (i.e.¬†independent) variable. \\(x_{2i}\\) is the \\(i^{th}\\) value of the second predictor, etc. The \\(x's\\) are always written lower-case, and technically are assumed to be fixed values, either set prior to data collection or measured without error.\n\\(\\beta_1\\) is the slope (i.e.¬†regression coefficient) for the first predictor variable. \\(\\beta_2\\) is the slope of the second predictor, etc. The \\(\\beta's\\) are parameters, meaning their values are treated as fixed (existing at the ‚Äúpopulation‚Äù level) but unknown.\nWe use data to calculate estimated values for the \\(\\beta's\\), and these estimates are written using hat notation. For example, \\(\\hat{\\beta_1}\\) is the estimated value for \\(\\beta_1\\).\n\\(\\varepsilon_i\\) is the \\(i^{th}\\) random error value. This is the amount by which \\(Y_i\\) differs from \\(\\beta_0+\\beta_1x_{1i} + \\beta_2x_{2i} + \\dots + \\beta_kx_{ki}\\), i.e.¬†the fixed component of the model.\nThe amount by which \\(y_i\\) (the \\(i^{th}\\) observed value of \\(y\\)) differs from \\(\\hat{\\beta_0}+\\hat{\\beta_1}x_{1i} + \\hat{\\beta_2}x_{2i} + \\dots + \\hat{\\beta_k}x_{ki}\\) is called the \\(i^{th}\\) residual, which we can denote \\(e_i\\).\nThe errors are modeled as random values that are drawn from a normal distribution with mean zero and variance \\(\\sigma^2\\).\n\n\n\n\n\n\n\nNote\n\n\n\nThe errors in a regression model do not have to come from a normal distribution. This assumption is made in order to justify inferences about the coefficients; more on this soon.\n\n\nWhen a regression model has only one predictor variable, it is called a ‚Äúsimple‚Äù regression model. If it has more than one predictor variable, it is called a ‚Äúmultiple regression‚Äù model.\n\n\n2.1.2 Assumptions of the regression model\nThis model implies some assumptions:\n\nThe response variable \\(Y\\) is an additive, linear function of the predictors (the \\(x\\) variables)\nIf we fix the value(s) of the \\(x\\) variable(s), all values of \\(Y\\) will be normally distributed. In other words, the errors are normally distributed.\nThe errors have the same variance regardless of the values of the \\(x's\\). This variance is denoted \\(\\sigma^2\\)\n\n\n\n\n\n\n\nNote\n\n\n\nThe square root of the variance is the standard deviation, denoted \\(\\sigma\\). Standard deviation is expressed in the same units of the original variable, whereas variance is expressed in squared units.\nFor this reason, standard deviation is typically referred to when interpreting statistical results. Variance has desirable mathematical properties, and so is more often referred to in statistical theory\n\n\nVisually, this model treats values of Y as being generated randomly from normal distributions centered on the line:\n\n(figure derived from OpenStax Introductory Business Statistics, section 13.4)\nThe ‚Äúerrors‚Äù are the distances between the line and the values generated from the normal distributions.\nThe errors are treated as random and uncorrelated: knowing the value of one error tells you nothing about the likely value of the next."
  },
  {
    "objectID": "Ch2_Model_Building.html#regression-analysis-in-jamovi",
    "href": "Ch2_Model_Building.html#regression-analysis-in-jamovi",
    "title": "2¬† Chapter 2: Model building with linear regression",
    "section": "2.2 Regression analysis in jamovi",
    "text": "2.2 Regression analysis in jamovi\n\n2.2.1 Simulating the regression model in jamovi\nWe noted earlier that a statistical model is data generating. It describes, mathematically, how values of the outcome variable \\(Y\\) can be created. Consider the ‚Äúsimple‚Äù (single \\(x\\) ) regression model:\n\\[\nY_i=\\beta_0+\\beta_1x_{i} + \\varepsilon_i \\\\\n\\varepsilon_i \\sim \\text{Normal}(0,\\sigma^2)\n\\]\nIf we have values for \\(\\beta_0\\), \\(\\beta_1\\), and \\(\\sigma^2\\), when we can plug in values for \\(x_i\\) to generate values for \\(Y_i\\). Let‚Äôs do this using jamovi.\nIn jamovi, we first create X values by double-clicking an empty column, choosing ‚ÄúNew Computed Variable‚Äù then the \\(f_x\\) drop down menu and double click UNIF.\n\nHere, we are generating 100 random values from a \\(Uniform(0,100)\\) distribution. The uniform distribution is a distribution where all values are equally likely, so we should get an even spread of values between 0 and 100.\nTo simulate values of the response variable, we‚Äôll need to make up values for each parameter in the model. Say we want to generate values from this model:\n\\(Y_i=10+0.7x_i+\\varepsilon_i \\quad \\varepsilon_i \\sim Normal(0,8^2)\\)\nThis means we‚Äôve decided that \\(\\beta_0=10\\), \\(\\beta_1=0.7\\), and \\(\\sigma=8\\). And since we‚Äôve generated 100 values for \\(x_i\\), we‚Äôve also decided that \\(i=1\\dots 100\\)\nDouble click an empty column and choose ‚ÄúNew Computed Variable‚Äù:\n\nNow make the formula look like the right side of the regression equation from the previous slide:\n\nNow we can take a look using Scatterplot, a downloadable jamovi module. Click the icon of the plus sign labeled ‚ÄúModules‚Äù to bring up a list of available modules you can install. We will use many modules in STAT 331.\n\nAfter installation, Scatterplot is available under Exploration in the Analyses tab. You can assign the X and Y axis variables, and get a plot that looks something like this:\n\nThese are random data, so yours will look a little bit different. But the scales of the axes and vertical spread of the data should be similar.\nNext, we‚Äôll fit a regression model to this data. In practice, we do not know the values of the parameters in our model, so we estimate them using data. This is known as ‚Äúfitting‚Äù the model to the data. The point of this simulation is to look at what kind of results we get when fiting a regression model to fake data that was produced by a mechanism we fully understand.\n\n\n2.2.2 Fitting a regression model in jamovi\nWe can use Regression / Linear Regression to fit a ‚Äúsimple‚Äù regression model, which is a regression model with just one predictor.\n\nThe response variable is ‚ÄúDependent Variable‚Äù.\nThe predictor variable goes under ‚ÄúCovariates‚Äù.\n\nAfter selecting variables, model will automatically be fit, and output will be generated to the right under ‚ÄúResults‚Äù.\n\nBased on these results, here is the estimated regression model:\n\\[\n\\hat{y_i}=\\hat{\\beta_0}+\\hat{\\beta_1}x_i=9.1+0.71x_i\n\\]\n\\[\n\\hat{\\sigma}=\\sqrt{MSE}=\\sqrt{64.8}=7.93\n\\]\nNote that it is standard to denote estimated values using ‚Äúhats‚Äù. The ‚Äúestimate‚Äù column is where we find the values for the estimated regression coefficients \\(\\hat{\\beta_0}\\) and \\(\\hat{\\beta_1}\\). ‚ÄúRMSE‚Äù (‚Äúroot mean square error‚Äù) is the estimated standard deviation of the errors, assumed to have come from a normal distribution. Compare these results to the values used to generate our fake data:\n\\[\n\\begin{align}\n&\\beta_0=10 &\\beta_1=0.7 \\quad &\\sigma=8 \\\\\n&\\hat{\\beta_0}=9.099 &\\hat{\\beta_1}=0.712 \\quad &\\hat{\\sigma}=7.93 \\\\\n&s_{\\hat{\\beta_0}}=1.776 &s_{\\hat{\\beta_1}}=0.034\n\\end{align}\n\\]\n\n\n2.2.3 The \\(R^2\\) statistic\nNote that the output tells us \\(R^2=0.874\\). This is a statistic quantifying how well this model can ‚Äúpredict‚Äù the data used to fit it. It is found from the ‚Äúsum of squares‚Äù values in the ANOVA table, Generically:\n\\[\nR^2=\\frac{\\text{model sum of squares}}{\\text{total sum of squares}}=\\frac{\\text{model sum of squares}}{\\text{model sum of squares + residual sum of squares}}\n\\]\n\n\n\n\n\n\nNote\n\n\n\n‚ÄúSums of squares‚Äù are used to quantify variance. You can think of this as being short for ‚Äúsum of the squared distances between some values and their mean‚Äù. For example, the variance statistic is\n\\[\ns^2=\\frac{\\sum_{i=1}^n (y_i-\\bar{y})}{n-1}\\\n\\] The numerator, \\(\\sum_{i=1}^n (y_i-\\bar{y})\\), is a ‚Äúsum of squares‚Äù - the sum of the squared deviations between all the values of \\(y_i\\) and their mean, \\(\\bar{y}\\).\n\n\nIn this example:\n\\[\nR^2=\\frac{29162}{29162+4211}=0.874\n\\]\nSo, in this case, \\(87.4\\%\\) of the total variance in \\(Y\\) can be accounted for using the values of \\(x\\). Here‚Äôs the data again, with the estimated regression line added:\n\n\nThe total variance in \\(Y\\) quantifies how much the data vary vertically around the horizontal red line, which is the mean of \\(Y\\).\nThe residual (or ‚Äúerror‚Äù) variance quantifies how much the data vary vertically around the regression line.\n\nHere, the data are relatively much closer to the regression line than to the the horizontal mean line, and so residual sum of squares is only a small portion of the total sum of squares, making \\(R^2\\) fairly large.\nAn alternative interpretation of \\(R^2\\) is that is quantifies the proportional decrease in residual variance when using the regression line rather than using only the mean of \\(Y\\).\nLooking at the plot above, you can imagine drawing vertical lines from each data point to the horizontal red line representing the mean of \\(Y\\). If you squared these lines and added them up, you‚Äôd have the total sum of squares, which would also be the residual sum of squares if you were using only the mean of \\(Y\\) to calculate residuals. In this case, using the regression line instead of just the mean to calculate residuals would represent an \\(87.4\\%\\) decrease in residual sum of squares.\nSaid differently, \\(R^2\\) tells you how much better your predictions for \\(Y\\) would be if you use the regression line rather than only the mean.\n\n\n2.2.4 There is no ‚Äúgood‚Äù or ‚Äúbad‚Äù value for \\(R^2\\)\nWhen residual variance in \\(Y\\) is larger, \\(R^2\\) is smaller. Visually, when the data are more spread out around the regression line, \\(R^2\\) is smaller. Is this ‚Äúbad‚Äù? I want you to resist such an interpretation. A small \\(R^2\\) tells you that \\(Y\\) is being influenced by a lot more than just what is in your model. And this is often to be expected.\nFor instance, if I‚Äôm trying to predict how many tomatoes are produced per tomato plant in different parts of the country and my only predictor variable is average daily outdoor temperature, I should not expect a large \\(R^2\\). This is because there are many many more variables that influence how many tomatoes will grow (e.g.¬†properties of soil, watering schedule, fertilizer, pests‚Ä¶). But, a small \\(R^2\\) should not be interpreted as ‚Äúaverage daily outdoor temperature doesn‚Äôt matter when growing tomatoes‚Äù. It should be interpreted as ‚Äúthere are way more other things that matter when growing tomatoes, and their combined influence is much greater than average daily outdoor temperature alone‚Äù."
  },
  {
    "objectID": "Ch2_Model_Building.html#sums-of-squares-and-mean-squares",
    "href": "Ch2_Model_Building.html#sums-of-squares-and-mean-squares",
    "title": "2¬† Chapter 2: Model building with linear regression",
    "section": "2.3 Sums of squares and mean squares",
    "text": "2.3 Sums of squares and mean squares\nWe will look at some formulas in this section. Some are based on sums of squares, which are reported in the ANOVA table.\nTotal sum of squares quantifies total variability in \\(y\\):\n\\[\nSS_{Total} = \\sum^n_{i=1}(y_i - \\bar{y})^2\n\\]\nNote that this has nothing to do with the regression line, or the predictor variable. It quantifies variability in the response variable alone.\n\n2.3.1 Total sum of squares\nVisually, \\(SS_{Total}\\) is the sum of the squared vertical deviations between each data point and the mean of ùë¶, shown here as a horizontal line.\n\n\n\n\n\nThe two blue lines drawn are two such instances of these deviations. If we drew these for every data point, squared them, and added them up, we‚Äôd have \\(SS_{Total}\\)\nAgain, note that this quantity has nothing to do with the regression line!\n\n\n2.3.2 Residual / Error sum of squares\nError sum of squares quantifies total variability in ùë¶ around the regression line:\n\\[\nSS_{Error} = \\sum^n_{i=1}(y_i - \\hat{y}_i)^2 = \\sum^n_{i=1}[y_i - (\\hat{\\beta}_0 + \\hat{\\beta}_1)]^2\n\\]\nThis is also known as the ‚Äúsum of the squared residuals‚Äù, where a residual is the vertical distance between a data point and the regression line. The values of \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\) are chosen so as to minimize \\(SS_{Error}\\).\nIn other words, the regression line drawn through the data produced a smaller \\(SS_{Error}\\) than any other line we could possibly draw.\nVisually, \\(SS_{Error}\\) is the sum of the squared vertical deviations between each data point and the regression line.\n\n\n\n\n\nHere, the blue lines are two instances of these deviations\n\\(SS_{Error}\\) will be larger when the data are more spread out around the line, and vice versa.\n\n\n2.3.3 Mean square error\nError mean square (aka mean square error) is given by\n\\[\nMSE = MS_{Error} = \\frac{SS_{Error}}{n-k +1}\n\\]\n\\(k\\) is the number of predictor variables. Example: for simple regression, \\(k = 1\\), so \\(MSE = \\frac{SS_{Error}}{N-2}\\)\nAs seen earlier, \\(\\sqrt{MSE}\\) is the estimate for the standard deviation of the residuals around the line: \\(\\hat{\\sigma} = \\sqrt{MSE}\\)\n\n\n2.3.4 Model sum of squares\nModel sum of squares (aka ‚Äúregression sum of squares‚Äù) is given by:\n\\[\nSS_{Model} = \\sum^n_{i=1}(\\hat{y}_i - \\bar{y})^2\n\\]\nThis can be thought of as quantifying how much better the model is than \\(\\bar{y}\\) alone at accounting for variation in the values of \\(y\\).\nVisually, \\(SS_{Model}\\) is the sum of the squared vertical deviations between the regression line and the horizontal line, at each value of the predictor variable.\n\n\n\n\n\nHere, the blue lines are two instances of these deviations, associated with the circled blue data points.\n\n\n2.3.5 \\(SS_{Total} = SS_{Error} + SS_{Model}\\)\nTotal sum of squares are equal to the sum of error sum of squares and model sum of squares.\nNote that this also implies:\n\\[\nSS_{Model} = SS_{Total} - SS_{Error} \\\\\nSS_{Error} = SS_{Total} - SS_{Model}\n\\]"
  },
  {
    "objectID": "Ch2_Model_Building.html#interpreting-regression-results",
    "href": "Ch2_Model_Building.html#interpreting-regression-results",
    "title": "2¬† Chapter 2: Model building with linear regression",
    "section": "2.4 Interpreting regression results",
    "text": "2.4 Interpreting regression results\nHere again is the estimated model from the previous example:\n\\[\n\\hat{y}_i = 9.1 + 0.71x_i\n\\]\nThe intercept, \\(\\hat{\\beta}_0 = 9.1\\), gives the predicted value of the response variable (\\(y\\)) when the predictor variable (\\(x\\)) equals zero. This is not typically of practical interest.\nThe slope, \\(\\hat{\\beta}_1 = 0.71\\), gives the predicted change in \\(y\\) for a one unit increase in \\(x\\).\n\n2.4.0.1 More on interpreting the slope\nThe slope is often of practical interest. It tells us how much the response variable changes, on average, when the predictor variable increases by one unit.\nThis interpretation is very common. It is also dangerous, because it is phrased in a way that suggests changes in ùë• cause changes in \\(y\\).\nHere is an alternate, non-causal sounding interpretation:\nIf we observe two values of \\(x\\) that are one unit apart, we estimate that their corresponding average \\(y\\) values will be \\(\\hat{\\beta}_1\\) units apart.\nVisually, we can choose two values of \\(x\\), go up to the line, and record the values of \\(y\\). The slope tells us how much these \\(y\\) values are expected to differ.\n\n\n\n\n\nHere, when we compare \\(x = 20\\) and \\(x = 80\\), we expect their corresponding \\(y\\) values to differ by:\n\\[\n(80 ‚àí 20) ‚àó 0.725 = 43.5\n\\]\n\n\n2.4.0.2 Inference on the coefficients\nFor each estimated coefficient (\\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_0\\)), jamovi reports a standard error, along with a t-test statistic and p-value testing the null that the parameter being estimated equals zero.\n\n\n\n\n\nExample: the above output shows the test of \\(H_0: \\beta_1 = 0\\)\n\\[\nt = \\frac{\\hat{\\beta}_1}{s_{\\hat{\\beta}_1}} = \\frac{0.712}{0.034} = 21.22\\\\\np-value &lt; 0.01 \\\\\n\\text{\"reject } H_0\"\n\\]\nWe can use these results to create an approximate 95% confidence for \\(\\beta_1\\):\n\\[\n95\\% \\text{ CI for } \\beta_1 \\approx \\hat{\\beta}_1 \\pm 2* s_{\\hat{\\beta}_1} = 0.712 \\pm 2*0.034 = (0.645,0.779)\n\\]\nThis is a very narrow interval, suggesting a ‚Äúprecise‚Äù estimate of \\(\\beta_1\\)\nFor both the hypothesis test and 95% CI, the results depend on how large the estimate of \\(\\beta_1\\) is, relative to its standard error.\nIn this case, \\(\\hat{\\beta}_1\\) is very large relative to \\(s_{\\hat{\\beta}_1}\\), so the result is ‚Äúhighly significant‚Äù and the 95% CI is narrow.\n\n\n2.4.0.3 Formulas for \\(\\hat{\\beta}_1\\) and \\(s_{\\hat{\\beta}_1}\\)\nNow that you‚Äôve seen how \\(\\hat{\\beta}_1\\) and \\(s_{\\hat{\\beta}_1}\\) are used, here are their formulas:\n\\[\n\\hat{\\beta}_1 = r_{xy} * \\frac{s_y}{s_x} \\\\\ns_{\\hat{\\beta}_1} = \\sqrt{\\frac{MSE}{\\sum^n_{i=1}(x_i - \\bar{x})^2}} = \\sqrt{\\frac{1-R^2}{n-k-1}}*\\frac{s_y}{s_x}\n\\]\n\\(\\hat{\\beta}_1\\) is larger when the correlation between \\(x\\) and \\(y\\) is stronger, and when the variability in \\(y\\) is larger relative to the variability in \\(x\\).\n\\(s_{\\hat{\\beta}_1}\\) is smaller when \\(R^2\\) is larger, when \\(n\\) is larger, and when the variability in \\(x\\) is larger relative to the variability in \\(y\\)."
  },
  {
    "objectID": "Ch2_Model_Building.html#applied-example-the-binary-bias",
    "href": "Ch2_Model_Building.html#applied-example-the-binary-bias",
    "title": "2¬† Chapter 2: Model building with linear regression",
    "section": "2.5 Applied example: ‚ÄúThe binary bias‚Äù",
    "text": "2.5 Applied example: ‚ÄúThe binary bias‚Äù\nThere is a paper up on Canvas, titled ‚ÄúThe Binary Bias: A Systematic Distortion in the Integration of Information‚Äù. This is a 2018 paper published in Psychological Science with open data.\nThe overall hypothesis is that people tend to assess continuous information using binary thinking. The experiments all involve testing to see if participants will give higher or lower assessments of where an average lies, based on the ‚Äúimbalance‚Äù of the data: i.e.¬†the comparative frequency with which very low or very high values turn up.\nHere is the section of the paper describing the results of Study 1a:\n\n\n\n\n\nThe data:\n\n\n\n\n\nTelling jamovi to fit the model:\n\n\n\n\n\nThe model, before being fit to the data:\n\\[\nRecorded_i = \\beta_0 + \\beta_1Imbalance_i + \\beta_2Mode_i + \\beta_3First_i + \\beta_4Last_i + \\epsilon_i\n\\]\nThe results in jamovi:\n\n\n\n\n\n\n\n\n\n\nNote that the estimated slope is being denoted as \\(b\\) rather than as \\(\\hat{\\beta}_1\\). \\(\\hat{\\sigma} = MSE\\). Note also how the 95% CI was created: \\(\\approx 4.62 ¬± 2 ‚àó 0.63\\)\nThe paper does not mention \\(R^2\\). We can see it in the jamovi output, and calculate it from the ANOVA table:\n\n\n\n\n\n\\(R^2 = 0.12\\). So, about 12% of the total variance in ‚ÄúRecorded‚Äù is being ‚Äúexplained‚Äù or ‚Äúaccounted for‚Äù in the model.\nIn multiple regression, each predictor is interpreted under the assumption that the values of all other predictors are held constant (i.e.¬†‚Äúcontrolled for‚Äù).\nSo, if we were to observe two participants who were equal in terms of ‚ÄúFirst‚Äù, ‚ÄúLast‚Äù, and ‚ÄúMode‚Äù, but were one unit apart in terms of ‚ÄúImbalance‚Äù, we would expect their values for the response variable (‚ÄúRecorded‚Äù) to differ by 4.62 units on average.\nOr: The predicted (or average) difference in Recorded associated with a one unit difference in Imbalance is 4.62 units, if all other predictors are held constant."
  },
  {
    "objectID": "Ch2_Model_Building.html#interaction",
    "href": "Ch2_Model_Building.html#interaction",
    "title": "2¬† Chapter 2: Model building with linear regression",
    "section": "2.6 Interaction",
    "text": "2.6 Interaction\n‚ÄúInteraction‚Äù is the phenomenon by which the association between a predictor variable and a response variable is itself dependent on the value of another predictor.\nSay we have response \\(y\\), one predictor \\(x_1\\), and another predictor \\(x_2\\). We say that \\(x_1\\) and \\(x_2\\) interact if the amount of change in \\(y\\) associated with a change in \\(x_1\\) is different for different values of \\(x_2\\), or vice versa.\nYou can think of this as saying that the ‚Äúslope‚Äù of \\(x_1\\) depends upon the value of \\(x_2\\).\nAnother way of saying it: if the answer to the question:\n‚ÄúHow much does our estimate for \\(y\\) change when \\(x_1\\) changes?‚Äú\nis:\n‚ÄúIt depends on the value of \\(x_2\\)‚Äú,\nthen \\(x_1\\) and \\(x_2\\) interact.\n\n2.6.1 Interaction example\nSuppose a drug for treating rheumatoid arthritis is more effective at reducing inflammation for younger patients than it is for older patients.\nIf we conduct an experiment in which inflammation is the response variable and the predictors are treatment group (drug vs.¬†control) and age, then we expect treatment group and age to interact.\nThis is different from saying that age and treatment both affect inflammation. It means that the extent to which treatment affects inflammation is different for patients of different age.\nSometimes interaction is referred to as ‚Äúmoderation‚Äù. This is common in the social and behavioral sciences, particularly Psychology.\nSo, a ‚Äúmoderator‚Äù variable is one that changes how the primary predictor of interest relates to the response.\nExample: suppose an experiment shows that subjects holding a pen with their teeth rate cartoons as funnier vs.¬†subjects holding a pen with their lips.\nSuppose also that this ‚Äúpen in teeth‚Äù effect disappears if subjects see a video camera in the room.\nIn this case, the presence of the video camera ‚Äúmoderates‚Äù the effect of the pen on cartoon ratings. In the language of interaction, the presence of the pen and the presence of the video camera ‚Äúinteract‚Äù.\n\nThis is based on a real study that has generated controversy, see:\nhttps://statmodeling.stat.columbia.edu/2018/11/01/facial-feedback-findings-suggest-minute-differences-experimental-protocol-might-lead-theoretically-meaningful-changes-outcomes/\n\n\n\n2.6.2 Interaction, in the regression model\nMathematically, we create an interaction variable by multiplying predictor variables by one another.\nSo, if we want to allow \\(x_1\\) and \\(x_2\\) to interact, we simply make a new variable defined as \\(x_1*x_2\\).\nThis interaction variable will be used as an additional predictor variable in the regression model:\n\\[\nY_i = \\beta_0 + \\beta_1x_{1i} + \\beta_2x_{2i} +\\beta_3(x_1x_2)_i + \\epsilon_i \\\\\n\\text{ where } \\epsilon_i \\sim Normal(0,\\sigma^2)\n\\]\nThe interaction coefficient is \\(\\beta_3\\) in this model:\n\\[\nY_i = \\beta_0 + \\beta_1x_{1i} + \\beta_2x_{2i} +\\beta_3(x_1x_2)_i + \\epsilon_i \\\\\n\\text{ where } \\epsilon_i \\sim Normal(0,\\sigma^2)\n\\]\nTo interpret this, let‚Äôs look at how it affects the coefficients (aka slopes) for \\(x_1\\) and \\(x_2\\).\nWe can think of the ‚Äúslope‚Äù of a predictor as everything it is being multiplied by.\n\\[\nY_i = \\beta_0 + \\beta_1x_{1i} + \\beta_2x_{2i} +\\beta_3(x_1x_2)_i + \\epsilon_i\n\\]\nFactoring out \\(x_1\\) from the above regression equation gives:\n\\[\nY_i = \\beta_0 + (\\beta_1 + \\beta_3x_{2i})x_{1i} + \\beta_2x_{2i} + \\epsilon_i\n\\]\nSimilarly, factoring out \\(x_2\\) gives:\n\\[\nY_i = \\beta_0 + (\\beta_2 + \\beta_3x_{1i})x_{2i} + \\beta_1x_{1i} + \\epsilon_i\n\\]\nSo, for this model, the ‚Äúslope‚Äù of \\(x_1\\) is \\(\\beta_1 + \\beta_3x_2\\), and the ‚Äúslope‚Äù of \\(x_2\\) is \\(\\beta_2 + \\beta_3x_1\\)\nIn other words, the slope of \\(x_1\\) depends on the value of \\(x_2\\), and vice versa. For different values of \\(x_2\\), the ‚Äúpredicted change in \\(y\\) for a one unit increase in \\(x_1\\)‚Äù (i.e.¬†the slope of \\(x_1\\)) will be different.\nA simpler way of saying this is that, if two predictors interact, then the effect of one predictor on the response depends on the value of the other predictor.\n(This is a simpler interpretation, but also potentially misleading in that the term ‚Äúeffect‚Äù sounds causal. Nonetheless it is commonly used language when interpreting slopes.)"
  },
  {
    "objectID": "Ch2_Model_Building.html#jamovi-example-arthritis-data",
    "href": "Ch2_Model_Building.html#jamovi-example-arthritis-data",
    "title": "2¬† Chapter 2: Model building with linear regression",
    "section": "2.7 jamovi Example: Arthritis data",
    "text": "2.7 jamovi Example: Arthritis data\nThe data set ‚Äúarthritis_data.csv‚Äù contains simulated data from a (fictional) Randomized Control Trial comparing treatments for inflammation from rheumatoid arthritis: a disease-modifying anti-rheumatic drug (DMARD) vs.¬†a non-steroidal anti-inflammatory drug (NSAID)\n\nThe variables are:\n\n-   Drug: indicator (0 / 1) variable; 1 = DMARD, 0 = NSAID\n\n-   Before: Inflammation scan score prior to treatment (scale: 0 to 50)\n\n-   After: Inflammation scan score six months after treatment\n\n-   Difference: Difference in scores, before minus after. (Note that larger values\n\n2.7.0.1 Before vs.¬†after scatterplot\nWe will fit some regression models using this data, but first let‚Äôs take a look at our data using the Scatterplot module.\n\n\n\n\n\nHere is ‚Äúafter‚Äù vs.¬†‚Äúbefore‚Äù, with a linear regression line superimposed. Note that most patients had greater inflammation before than after treatment.\n\n\n2.7.0.2 Differences by treatment type\nNote that the differences tend to be larger for the DMARD group: this corresponds to a greater reduction in inflammation.\n\n\n\n\n\nUse Descriptives to create the boxplot. Select difference as a Variable and Split by drug. Then select under Plots ‚Äì Box plot and Data Jittered to superimpose data.\n\n\n\n\n\n\n\n2.7.0.3 Before vs.¬†age scatterplot\nHere we see that age is positively correlated with inflammation before the drug trial.\n\n\n\n\n\njamovi gives options to superimpose regression output on a scatterplot.\njamovi can also plot ‚Äústandard error‚Äù bands around the line. These show the standard error for the average value of y (‚Äúbefore‚Äù), given x (‚Äúage‚Äù).\n\n\n\n\n\n\n\n2.7.0.4 Difference vs.¬†before\nWe don‚Äôt see an association between amount of inflammation before treatment and reduction in inflammation‚Ä¶\n\n\n\n\n\n‚Ä¶ but maybe we do if we add in drug! Do ‚Äúdrug‚Äù and ‚Äúbefore‚Äù interact here?\n\n\n\n\n\n\n\n2.7.0.5 Difference vs.¬†age\nWe see a small negative correlation between difference and age‚Ä¶\n\n\n\n\n\n‚Ä¶ but when we add drug, we see no correlation for the NSAID and clear negative correlation for the DMARD. Definite interaction!\n\n\n\n\n\n\n\n2.7.0.6 Now with model output‚Ä¶\nThe following slides show the plots again, plus the regression output JMP produces. First up, Difference vs.¬†Drug, using t-test:\n\n\n\n\n\nAnd using regression:\n\n\n\n\n\nOMG! The estimated slope for ‚Äúdrug‚Äù is the same as the difference in means between the drugs!\n\n\n\n\n\n\n\n2.7.0.7 Before vs.¬†age\n\n\n\n2.7.0.8 Difference vs.¬†age, with drug interaction\nHere, we are fitting the model:\n\\[\nDifference_i = \\beta_0 + \\beta_1age_i + \\beta_2drug_i + \\beta_3(age*drug)_i + \\epsilon_i\n\\]\nTo do this, create a new column in jamovi, defined as \\(age*drug\\). May as well label it ‚Äú\\(age*drug\\)‚Äú.\n\n\n\n\n\n\n\n\n\n\nNotice that the slope of ‚Äúage‚Äù by itself is for when drug = 0. The age*treatment interaction shows how much the slope of ‚Äúage‚Äù changes when treatment = 1.\n\\[\n\\widehat{difference} = 2.0114 + 0.0125(age) + 15.1586(drug) -0.2493(age*drug)\n\\]\nSlope of ‚Äúage‚Äù when drug = 1 is: \\(0.0125 ‚àí 0.2493 = ‚àí0.2368\\)\nNotice also that the slope of ‚Äúage‚Äù by itself is nowhere close to being statistically significant (the estimate is less than half the standard error), but the slope of the interaction is highly significant (the estimate is 6 times as large as the standard error)\nNow take a look at the plot. There is a clear negative correlation between age and difference when drug = 1. There is essentially none when drug = 0.\n\n\n\n\n\n\nAnother way of thinking about this interaction:\n\nIf we ask the question: ‚Äúhow does inflammation reduction differ by age?‚Äù, the answer is ‚Äúit depends on which drug the patient took.‚Äù\nSimilarly, if we ask the question: ‚Äúhow does inflammation reduction differ by drug?‚Äù, the answer is ‚Äúit depends on the age of the patient.‚Äù\n\n\n\n\n2.7.0.9 Interaction is NOT correlation!\nIn this example, we see a clear interaction between drug and age: the slope for age is negative for DMARD and flat for NSAID.\nAnother way of thinking about this: DMARD appears to be more effective for younger patients than for older patients. NSAID appears to be equally effective regardless of age.\nHOWEVER ‚Äì this does NOT mean that treatment and age are correlated!\nThis should make sense, after all patients were randomly assigned to one of the two drugs. If drug were correlated with age, there would be bias in this study. The whole point of randomization is to remove correlations!\nJust to confirm, here is the distribution of age, split by drug:\n\n\n\n\n\nAgain, age and drug ‚Äúinteract‚Äù when it comes to their associations with the differences in inflammation scores: the association between ‚Äúage‚Äù and ‚Äúdifference‚Äù is different for the two different drugs.\nSimilarly, the association between ‚Äúdrug‚Äù and ‚Äúdifference‚Äù is different for patients of different ages.\nBut, age and drug are not correlated with one another!"
  },
  {
    "objectID": "Ch2_Model_Building.html#centering-predictor-variables",
    "href": "Ch2_Model_Building.html#centering-predictor-variables",
    "title": "2¬† Chapter 2: Model building with linear regression",
    "section": "2.8 Centering predictor variables",
    "text": "2.8 Centering predictor variables\n\\[\n\\widehat{difference} = 2.0114 + 0.0125(age) + 15.1586(drug) -0.2493(age*drug)\n\\]\nThere is a serious challenge when interpreting the slope for ‚Äúdrug‚Äù: this slope is only 15.1586 if \\(age = 0\\). But \\(age = 0\\) is not of interest.\nPlug in mean age (52.096) and see what happens to the slope for drug:\n\\[\n15.1586(drug) ‚àí 0.2493(52.096*drug) \\\\\n= (15.1586 ‚àí 0.2493 ‚àó 52.096)(drug)\\\\\n= 2.171(ùëëùëüùë¢ùëî)\n\\]\nSo, for patients at mean age, the predicted difference in inflammation is 2.171 units greater under the DMARD than under the NSAID\nThis process of estimating slopes at the average value of predictors can be done via ‚Äúcentering‚Äù\nCentering means subtracting the mean from a variable‚Äôs distribution.\nIn this example, \\(centered\\text{ }age = age - \\overline{age} = age - 52.096\\)\nThis is very useful when using interaction terms in a regression model.\n\n2.8.0.1 Centered interaction in jamovi\nTo center age, create a new columns called ‚Äúcentered age‚Äù, defined as age ‚Äì VMEAN(age).\nCreate another new column for ‚Äúcentered drug‚Äù, defined as drug ‚Äì VMEAN(drug). Now create a final column defined as centered drug * centered age, call it ‚Äúcentered age*drug‚Äù\nWe can‚Äôt use MEAN() to center variables in jamovi since it works across variables, one row at a time. What we want is to take the overall mean of a variable and subtract it from each measurement.\nSo, use VMEAN() to center variables in jamovi.\nCompare the new results (on the left) to the results we saw when using the interaction term we created manually (on the right)\n\n\n\n\n\n\n\n2.8.0.2 Centered interaction in JMP\nThe ‚ÄúModel Fit Measures‚Äù are identical. Centering has no effect on \\(R^2\\) or any sums of squares.\nCentering also had no effect on the slope for the interaction, or on its standard error. But, look at the individual ‚Äúage‚Äù and ‚Äúdrug‚Äù predictors.\n\n\n\n\n\nThese slopes are different, because they are being calculated at the mean value of the other.\nCentering also reduces the standard errors of the individual slopes!\n\n\n2.8.0.3 Centered interaction in jamovi\nLook at the slope for drug when using a centered interaction: it‚Äôs the same value we calculated by plugging the mean of age into the interaction term in the non-centered model!\n\n\n\n\n\nThe slope for age in the centered model is harder to interpret. It is calculated at the ‚Äúaverage‚Äù for drug, which doesn‚Äôt make real world sense.\n\n\n2.8.0.4 Only centering one predictor in jamovi\nIt would be best if we could center ‚Äúage‚Äù but not drug.\nBut we already created centered age when we created the centered interaction.\nNow we need to create the new interaction where only age is centered. Create a new column and define it as ‚Äúdrug * centered age‚Äù.\n\n\n\n\n\n\n\n2.8.0.5 Only centering one predictor in JMP\nHere are the results. Compare them to the previous two versions:\nOnly age centered:\n\n\n\n\n\nAge and drug centered in the interaction:\n\n\n\n\n\nNothing centered:\n\n\n\n\n\n\n\n2.8.0.6 Only centering one predictor in jamovi\nFirst, the slope for the interaction is the same in all three models.\nSecond, the slope for drug is the same in both centered models, but different in the non-centered model. It is the centering of age in the interaction that changed the slope for drug.\n\n\n\n\n\nThird, the slope for age is the same in both models for which drug is not centered. This allows us to interpret it as before: slope for age is 0.0125 for the NSAID and is 0.0125 ‚àí 0.2493 = ‚àí0.2368 for the DMARD\nFourth, the standard error for drug is substantially smaller when age is centered in the interaction. This is typically the case when centering a continuous variable in an interaction.\n\n\n2.8.0.7 Interpreting the intercept when centering\nFinally, the intercept is different in all three models.\nNormally we don‚Äôt care about the intercept, but centering allows the intercept to be meaningfully interpreted.\nSince \\([Center]age = age - \\overline{age}, [Center]age = 0 \\text{ when } age = \\overline{age}\\)\nRemember that the intercept is interpreted as the ‚Äúpredicted value of the response when the predictors equal zero‚Äù.\nSo, when centering, the intercept is the predicted value of the response when the centered predictors equal their mean.\nGoing back to our example, the predicted reduction in inflammation (before minus after) for a patient at the average age in our data set who got the NSAID is 2.665.\n\n\n\n\n\nThe predicted reduction in inflammation for a patient at the average age who got the DMARD is 2.665 + 2.171 = 4.836"
  },
  {
    "objectID": "Ch2_Model_Building.html#standardizing-predictor-variables",
    "href": "Ch2_Model_Building.html#standardizing-predictor-variables",
    "title": "2¬† Chapter 2: Model building with linear regression",
    "section": "2.9 Standardizing predictor variables",
    "text": "2.9 Standardizing predictor variables\nWe will briefly consider an extension on centering: standardizing.\nRecall from your introductory statistics course the standardization ‚Äúz‚Äù formula:\n\\[\nz = \\frac{x - \\mu}{\\sigma}, \\text{ or in words: } z = \\frac{\\text{value - mean}}{\\text{standard deviation}}\n\\]\nTo center a variable, we subtract the mean. To standardize, we subtract the mean and then divide by the standard deviation.\n\n2.9.0.1 Why standardize?\nJust like with a centered variable, the mean of a standardized variable is zero. So, standardizing has all the same benefits as centering when it comes to interpretation of interactions.\nStandardizing has an additional potential benefit: the slope can be interpreted as the predicted change in \\(Y\\) for a one standard deviation increase in \\(X\\) (while holding all other predictors constant).\n\\(Z\\) values are interpreted as ‚Äúnumber of standard deviations from the mean‚Äù. And so increasing \\(Z\\) by \\(1\\) implies increasing \\(X\\) by \\(1\\) standard deviation.\n\n\n2.9.0.2 Standardizing in jamovi\nTo standardize in jamovi we will need to create a new column defined by (age ‚Äì VMEAN(age)) / VSTDEV(age).\nWe will also need to create a column for the new interaction and define it as drug*Standardized age\n\n\n\n\n\nHere are the results when age is standardized:\n\n\n\n\n\nCompare to the results when age is centered:\n\n\n\n\n\nNote that the intercepts are the same. In both cases, the predicted reduction in inflammation for a patient at average age getting NSAID is \\(2.665\\). Likewise, in both cases this prediction is \\(4.836\\) for DMARD.\nWhat has changed is the slope for terms involving age. Now, a one unit increase in standardized age is a one standard deviation increase in age.\nSo, for NSAID, the predicted difference in inflammation reduction for two people whose ages are one standard deviation apart is \\(0.14\\). For DMARD, it is \\(0.14 ‚Äì 2.79 = -2.64\\).\nHow much is a standard deviation? We can look it up‚Ä¶\n\n\n\n\n\n\n\n2.9.0.3 Interaction and centering / standardizing summary\nThis has been a long example. I encourage you to load up the data yourself and play around with it in jamovi. At the minimum, make sure you can re-create the results in these notes.\n\nThe most important take-aways:\n\nInteraction terms allow the slope of predictor to change for different values of another predictor.\nCentering helps make regression coefficients (slopes and intercepts) more interpretable. Standardizing allows you to interpret them in terms of one standard deviation changes, rather than ‚Äúone unit‚Äù changes."
  },
  {
    "objectID": "Ch3_Model_Fit.html#part-1-assumptions-and-assumption-violations",
    "href": "Ch3_Model_Fit.html#part-1-assumptions-and-assumption-violations",
    "title": "3¬† Chapter 3: Assessing and improving model fit",
    "section": "Part 1: assumptions and assumption violations",
    "text": "Part 1: assumptions and assumption violations"
  },
  {
    "objectID": "Ch3_Model_Fit.html#outline-of-notes",
    "href": "Ch3_Model_Fit.html#outline-of-notes",
    "title": "3¬† Chapter 3: Assessing and improving model fit",
    "section": "Outline of notes:",
    "text": "Outline of notes:\n\nRegression assumptions\nLinearity\nNormality of residuals\nHomogeneity of variance\nInfluential observations\n(Multi)collinearity\nLog transformation\nNon-linearity\nOver-fitting\nBack to basics: is the model sensible?"
  },
  {
    "objectID": "Ch3_Model_Fit.html#violating-model-assumptions",
    "href": "Ch3_Model_Fit.html#violating-model-assumptions",
    "title": "3¬† Chapter 3: Assessing and improving model fit",
    "section": "3.1 Violating model assumptions",
    "text": "3.1 Violating model assumptions\nThe previous notes outlined the fundamentals of specifying, fitting, and interpreting a linear regression model.\nThese notes focus on the assumptions that our models are based upon, and how we check to see if they are being violated.\nIf model assumptions are violated, DON‚ÄôT PANIC! There is nearly always a remedy. In these notes we will look at how to spot violations of assumptions. In the next set of notes we will look at ways to remedy these violations, and how to decide if they pose a serious problem to the usefulness of the model.\n\n3.1.1 The regression model and what it assumes\nOnce again, here is the regression model:\n\\[\nY_i = \\beta_0 + \\beta_1x_{1i} + \\beta_2x_{2i} + \\dots + \\beta_px_{pi} + \\epsilon_i, \\text{ where } \\epsilon_i \\sim Normal(0,\\sigma)\n\\]\n\nThis assumes:\n\nThat the response variable is a linear (straight line) function of the predictor variables\nThat the residuals will be normally distributed\nThat the standard deviation of the residuals does not vary\nThat the residuals are independent"
  },
  {
    "objectID": "Ch3_Model_Fit.html#linearity",
    "href": "Ch3_Model_Fit.html#linearity",
    "title": "3¬† Chapter 3: Assessing and improving model fit",
    "section": "3.2 Linearity",
    "text": "3.2 Linearity\nRemember the ‚Äúsimple‚Äù (i.e.¬†single predictor) regression model\n\\[\nY_i = \\beta_0 + \\beta_1x_{i} + \\epsilon_i, \\text{ where } \\epsilon_i \\sim Normal(0,\\sigma)\n\\]\nThis is linear in that it fits a straight line to the two-dimensional data.\nA two-predictor model would fit a flat plane to the three-dimensional data, and so on\nHere‚Äôs a bad idea: fitting a linear model to non-linear data!\n\n\n\n\n\n\n3.2.1 Diagnosing non-linearity\nWhen running ‚ÄúLinear Regression‚Äù in jamovi, a ‚Äúresiduals by predicted‚Äù plot can be created by selecting ‚ÄúResidual plots‚Äù under ‚ÄúAssumption Checks‚Äù\nThe residuals are the differences between each observed values of the response variable and the value that the model predicts:\n\\[\nresidual_i = y_i - \\hat{y}_i = y_i - (\\hat{\\beta}_0 + \\hat{\\beta}_1x_1 + \\hat\\beta_2x_2 + \\dots)\n\\]\n\n\n\n\n\nFor simple regression, this plot just looks like the regression plot with the line turned horizontally.\nFor multiple regression, there is no (two dimensional) ‚Äúregression plot‚Äù, so the residual plot will be very useful!\n\n\n\n\n\nIn this example, there is clear curvature in the data. A straight line model is not appropriate.\nHere‚Äôs an example of what a linear relationship might look like:\n\n\n\n\n\nWhen there is non-linearity, you will see the residuals mostly on one side of zero, then on the other size of zero, then back again, and so on.\nWhen there is linearity, the residuals should randomly fall on either side of zero.\n\n\n\n\n\n\n\n3.2.2 What to look for in a residual plot\nWe will look at many more examples of residual plots in these notes.\n\nWe want a residual plot that appears to agree with the model assumptions:\n\nStraight line relationship between the predictors and response\nNormally distributed random residuals around this line\nEqual variance in residuals across line"
  },
  {
    "objectID": "Ch3_Model_Fit.html#normality-of-residuals",
    "href": "Ch3_Model_Fit.html#normality-of-residuals",
    "title": "3¬† Chapter 3: Assessing and improving model fit",
    "section": "3.3 Normality of residuals",
    "text": "3.3 Normality of residuals\nThe ‚Äúerror term‚Äù in a regression model is that \\(+ \\epsilon_i\\) on the end\nWhen we write \\(\\epsilon_i \\sim Normal(0, \\sigma)\\), we are saying that the errors (aka residuals) are normally distributed, with mean zero and some standard deviation \\(\\sigma\\).\nThis can be assessed visually: run the regression plot the residuals to see if they appear roughly normally distributed.\n\n3.3.1 The QQ plot\nWhen fitting a model using ‚ÄúLinear Regression‚Äù in jamovi, there is an option to save residuals. This will create a new column with a residual for each row.\nThe option is under the last drop-down menu, under ‚ÄúSave‚Äù.\n\n\n\n\n\nTo create a plot of the residuals, select ‚ÄúQ-Q plot of residuals‚Äù under ‚ÄúAssumption Checks‚Äù in ‚ÄúLinear Regression‚Äù\nThe Normal Quantile plot is also known as the QQ plot, for ‚Äúquantile quantile‚Äù.\nIt is easier to assess normality with a QQ plot than with a histogram.\n\n\n\n\n\n\n\n3.3.2 Assessing normality with a QQ plot\nOn Canvas under Simulations there is a ‚ÄúQQ plot generator‚Äù app.\nThis app allows you to manipulate a distribution, sample from it, and then view a histogram next to a QQ plot. It is meant to give you an idea of how QQ plots work.\nBy default, it draws data from a normal distribution. But, you can add ‚Äúskewedness‚Äù or ‚Äúpeakedness‚Äù (aka kurtosis) to make the distribution non-normal. You can also adjust sample size.\n\nA QQ plot shows you how much the distribution of your data ‚Äúagree‚Äù with a normal distribution.\n\n\n\n\n\nThe horizonal axis gives the distribution data would follow if it were perfectly normal.\nThe vertical axis gives the distribution your data actually follows.\nThe diagonal line shows perfect agreement between the two.\n\n\n\n\n\n\n\n\n\n\nThe big advantage of the QQ plot vs.¬†the histogram is that very often data that come from a normal distribution don‚Äôt look normal, especially if the sample size is small.\nIn this case, the histogram isn‚Äôt clearly normal. But, on the QQ plot the data are close to the line.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNotice that the data still veer from the diagonal line to some extent, even though we know for a fact they came from a normal distribution.\n\n\n3.3.3 Limitations of QQ plots\nAs you can see from the app, sometimes data that come from a normal distribution don‚Äôt sit right on the line.\nSometimes data that come from a skewed distribution look similar to data that come from a normal distribution\nIt‚Äôs easier to assess normality when sample sizes are larger.\nAs it turns out, the assumption of normality is not vital to the validity of a regression model. If the QQ plot is vague, you‚Äôre probably fine. We only worry when we see extreme non-normality.\n\n\n3.3.4 Tests for normality (not recommended)\nThere are statistical tests, such as ‚ÄúShapiro-Wilks‚Äù or ‚ÄúKolmgorov-Smirnov‚Äù, for which the null hypothesis is that the data come from some specified distribution, like the normal.\nRejecting this null means that the data ‚Äúsignificantly‚Äù disagree with the assumption of normality.\nI do not recommend using this test. The problem is that, when the sample size is large enough, even small deviations from normality will be statistically significant. But small deviations from normality are OK. Only major deviations are concerning."
  },
  {
    "objectID": "Ch3_Model_Fit.html#the-homogeneity-of-variance-assumption",
    "href": "Ch3_Model_Fit.html#the-homogeneity-of-variance-assumption",
    "title": "3¬† Chapter 3: Assessing and improving model fit",
    "section": "3.4 The homogeneity of variance assumption",
    "text": "3.4 The homogeneity of variance assumption\nBack to the error term:\n\\(\\epsilon_i \\sim Normal(0, \\sigma)\\)\nNotice that \\(\\sigma\\) is just one number. This suggests that the standard deviation of the residuals should be the same across all values of predictor variables. In other words, there is homogeneity of variance.\nAnother name for this is ‚Äúhomoscedasticity‚Äù. If this assumption is violated, then we have ‚Äúheteroscedasticity‚Äù.\n\n3.4.1 Heterogeneity of variance\nTo show heterogeneity of variance, I‚Äôll simulate data that is a function of an X variable, plus random values from a normal distribution with standard deviation equal to X.\nThus, the standard deviation of residuals will get bigger as X gets bigger:\n\n\n\n\n\n\n\n3.4.2 The residuals vs fitted plot\nHere is the regression plot and residual plot when this simulated variable (called ‚ÄúW‚Äù here) is the response and X is the predictor:\n\nNotice that the residuals are more spread out for larger X\nWe also see ‚Äúheavy tails‚Äù when plotting the residuals with a histogram and QQ plot:\n\n\n\n\n\nHeavy tails refers to a distribution with outliers on both ends.\nThis shows up on the QQ plot as the residuals being too flat in the middle and then curving out on both ends."
  },
  {
    "objectID": "Ch3_Model_Fit.html#influential-observations-outliers",
    "href": "Ch3_Model_Fit.html#influential-observations-outliers",
    "title": "3¬† Chapter 3: Assessing and improving model fit",
    "section": "3.5 Influential observations (outliers)",
    "text": "3.5 Influential observations (outliers)\nOutliers in regression can be seen on a residual plot, or on a QQ plot, or on just a regular plot of the data.\nExample: the Florida election data.\n\n\n\n\n\nOutliers can ‚Äúpull‚Äù on the regression line, especially if they are far away from the mean of the predictor(s).\nThere are many statistics that assess influence. jamovi will calculate one of the most popular: a Cook‚Äôs Distance\n\n3.5.1 Cook‚Äôs Distances\nAs we saw in the Florida election example, removing the outlier (Palm Beach County) had a substantial effect on the regression results.\nThe logic behind Cook‚Äôs Distance is to quantify what happens to the regression model when a single observation is removed. This is sometimes referred to as a ‚Äúleave one out‚Äù method.\nCook‚Äôs Distances quantify how much the predicted values of the response variable change when an observation is removed.\nRecall that, in simple regression, the predicted values are the values on the regression line.\nIt is hard to interpret the actual values for Cook‚Äôs Distances. Values greater than 1 are often considered ‚Äúinfluential‚Äù.\nThe formula shows that it is based on the sum of the differences in predicted values between a model with the data point included and a model with it removed:\n\\[\n\\text{Cook's Distance for data point } \"i\" = D_i = \\frac{\\sum^n_{j=1}(\\hat{y}_i - \\hat{y}_{j(i)})^2}{MSE * p}\n\\]\nWhere \\(\\hat{y}_{j(i)}\\) is the predicted value of the response variable when the model is re- fit with the \\(i^{th}\\) data point removed, and \\(p\\) is the number of predictor variables in\nA Cook‚Äôs Distance is calculated for every data point. The option to do this in jamovi is under ‚ÄúSave‚Äù in ‚ÄúLinear Regression‚Äù. This creates a new column with a Cook‚Äôs distance for each row.\nThe saved Cook‚Äôs Distances can then be plotted. Any possibly influential points will usually stand out clearly from the rest.\n\n\n\n\n\nTo quickly narrow in on the influential counties, we can filter out all the small Cook‚Äôs distances.\nAfter implementing the filter, we can see that all rows which do not meet the criteria of the filter are excluded.\n\n\n\n\n\nOnly rows 13 and 50 should be highlighted for Dade and Palmbeach county which have cook‚Äôs distances of 1.983 and 3.786."
  },
  {
    "objectID": "Ch3_Model_Fit.html#multicollinearity",
    "href": "Ch3_Model_Fit.html#multicollinearity",
    "title": "3¬† Chapter 3: Assessing and improving model fit",
    "section": "3.6 (Multi)collinearity",
    "text": "3.6 (Multi)collinearity\nIn regression analysis, we want our predictor variables to be correlated with the response variable.\nBut we don‚Äôt want our predictor variables to be (highly) correlated with one another!\nWhen two predictor variables are highly correlated, we say our model has ‚Äúcollinearity.‚Äù\nWhen more than two predictor variables are mutually highly correlated, we say our model as ‚Äúmulticollinearity‚Äù.\n\n3.6.1 Why don‚Äôt we want correlated predictors?\nTo understand why we don‚Äôt want correlated predictors, recall that multiple regression models estimate the association between each individual predictor variable and the response, \\(\\textit{while holding all other predictor variables constant}\\).\nThis can be thought of as asking ‚Äúwhat is the difference in the response variable when we observe data points that have different values of one predictor but the same values of the other predictors?‚Äù\nIf \\(Y\\) is the response and \\(x_1\\) and \\(x_2\\) are predictors, we want to know how different \\(Y\\) is when \\(x_1\\) values differ but \\(x_2\\) values are the same, or vice versa.\nBut, if \\(x_1\\) and \\(x_2\\) are highly correlated, then we don‚Äôt get to observe cases where values of one variable differ substantially while values of the other are the same! Consider instances of no correlation vs.¬†heavy correlation:\n\n\n\n\n\nWhen \\(x_1\\) and \\(x_2\\) are uncorrelated, we see lots of instances of \\(x_1\\) values differing a lot when \\(x_2\\) values are equal.\n\n\n\n\n\nWhen \\(x_1\\) and \\(x_2\\) are highly correlated, we never see instances where \\(x_2\\) values are equal but \\(x_1\\) values are highly correlated.\n\n\n\n\n\nThe upshot is that, when \\(x_1\\) and \\(x_2\\) are highly correlated, the regression procedure has a difficult time distinguishing between the ‚Äúeffect‚Äù of \\(x_1\\) on \\(Y\\) and the ‚Äúeffect‚Äù of \\(x_2\\) on \\(Y\\).\nExtreme example: if we had degrees Celsius and degrees Fahrenheit as predictors in the same model, it would be impossible to tell their effects apart. You can‚Äôt change Celsius while holding Fahrenheit constant!\nThe practical consequence is that the standard errors for the slopes of (multi)collinear predictors are much larger than they would be if it were not for the (multi)collinearity.\nIf two or more predictors are perfectly correlated (\\(r=1\\)), then the model cannot be fit and jamovi produces an error:\n\n\n\n\n\nHere, \\(X1\\) and \\(X2\\) are perfectly correlated. jamovi cannot estimate a slope for \\(X2\\).\n\n\n3.6.2 Collinearity example: Florida election data\nIn the Florida election data, we used total votes for each county as our predictor variable.\nThere is another variable called ‚ÄúTotal_Reg‚Äù. This is the total number of registered voters in each county.\nUnsurprisingly, Total_Votes and Total_Reg are highly correlated:\n\n\n\n\n\nIf we run two separate simple regression models, we get very similar results:\n\\[\nBuchanin_i = \\beta_0 + \\beta_1Total\\_Votes_i + \\epsilon_i\n\\]\n\n\n\n\n\n\\[\nBuchanin_i = \\beta_0 + \\beta_1Total\\_Reg_i + \\epsilon_i\n\\]\n\n\n\n\n\nBut look what happens if we use Total_Votes and Total_Reg as predictors in the same model:\n\\[\nBuchanin_i = \\beta_0 + \\beta_1Total\\_Votes_i + \\beta_2Total\\_Reg_i + \\epsilon_i\n\\]\n\n\n\n\n\nTwo important things to note:\nP-values on slopes are much larger than for the individual models\n\\(R^2\\) is larger than on either individual model!\nLooking at the estimates and standard errors in all three models, we see that the standard errors are much larger in the multiple regression model. These estimates are ‚Äúunstable‚Äù ‚Äì their values will change a lot if the data change a little.\n\n\n\n\n\nWe know the association between Total_Reg are Buchanan is actually positive. But with so high a standard error, the slope for Total_Reg turned out negative!\n\n\n3.6.3 Variance inflation factor (VIF)\n(Multi)collinearity can be assessed using a ‚ÄúVariance Inflation Factor‚Äù, or VIF. A VIF is calculated for the \\(j^{th}\\) predictor variable as:\n\\[\nVIF_j = \\frac{1}{1-R^2_j}\n\\]\nWhere \\(R^2_j\\) is the \\(R^2\\) from a regression model with predictor \\(j\\) as the response variable and all other predictors still as predictors.\nIn the Florida election example, the VIF for Total_Votes can be found using the \\(R^2\\) for the model:\n\\[\nTotal\\_Votes_i = \\beta_0 + \\beta_1Total\\_Reg_i + \\epsilon_i\n\\]\n\n\n\n\n\n\nThis \\(R^2\\) is huge! Plugging it into the formula:\n\\[\nVIF_{Total\\_Votes}=\\frac{1}{1-0.997} = 333.33\n\\]\nThankfully we don‚Äôt have to do this by hand. In jamovi, under ‚ÄúLinear Regression‚Äù select ‚ÄúCollinearity statistics‚Äù:\n\n\n\n\n\nVIF &gt; 10 typically is considered large (note that this would imply \\(R^2 = 0.9\\) between predictor variables).\nThe most obvious thing to do in the presence of (multi)collinearity is to remove one or more correlated predictor variable. From a scientific standpoint, you also may not want highly correlated predictors in the same model.\n\n\n3.6.4 When should we worry about (multi)collinearity?\n(Multi)collinearity is a potentially huge problem if the goal of the regression model is to interpret the estimated slopes.\nThis is because it increases the standard error of these slopes, making their values less reliable. Some people say it makes slopes ‚Äúunstable‚Äù.\nIt may also complicate the interpretation of slopes: you are trying to statistically ‚Äúhold constant‚Äù a predictor variable that doesn‚Äôt naturally stay constant when the other predictor varies. This isn‚Äôt necessarily a problem, but it is something to be aware of.\nHowever, (multi)collinearity does not negatively impact the predicted values themselves. Remember that it didn‚Äôt hurt the \\(R^2\\) value in the Florida election example. \\(R^2\\) tells you how good your predictions are.\nSo, if the model is only for predicting, you probably don‚Äôt need to worry about using correlated predictor variables. Just beware when interpreting the slopes."
  },
  {
    "objectID": "Ch3_Model_Fit.html#part-2-improving-models",
    "href": "Ch3_Model_Fit.html#part-2-improving-models",
    "title": "3¬† Chapter 3: Assessing and improving model fit",
    "section": "Part 2: Improving models",
    "text": "Part 2: Improving models\nIn the the first part of chapter 3, we looked at some things that can go wrong in regression modeling, including:\n-    Non-linear relationships between predictor(s) and response\n\n-   Non-normality of residuals\n\n-   Non-constant (heterogeneous) variance of residuals\n\n-   Influential outliers\n\n-   Multicollinearity\nNow we‚Äôll look at some tools available for dealing with these problems."
  },
  {
    "objectID": "Ch3_Model_Fit.html#log-transformation",
    "href": "Ch3_Model_Fit.html#log-transformation",
    "title": "3¬† Chapter 3: Assessing and improving model fit",
    "section": "3.7 Log transformation",
    "text": "3.7 Log transformation\nRecall the regression model:\n\\[\nY_i = \\beta_0 + \\beta_1x_{1i} + \\beta_2x_{2i} + \\dots + \\beta_px_{pi} + \\epsilon_i, \\text{ where } \\epsilon_i \\sim Normal(0,\\sigma)\n\\]\nSometimes we can correct violations of model assumptions by applying a mathematical transformation to the response or predictor variables.\nThe most common transformation in Statistics is the log transformation:\n\\[\nln(x) = log_e(x)\n\\]\n\\(ln(ùë•)\\) is the inverse function of \\(e^ùë•\\), where \\(ùëí = 2.718 \\dots\\)\nIn other words, \\(ln(e^x) = x\\)\nExample: \\(ùëí^3 = 20.086; ln(20.086) = 3\\)\nSo, the natural log of \\(x\\) is the number you would have to raise \\(e\\) to so that you‚Äôd get \\(x\\).\nNote: in statistics, when we say ‚Äúlog‚Äù, we usually mean ‚Äúnatural log‚Äù. It turns out that the distinction is not very important. I‚Äôll say ‚Äúlog transform‚Äù\n\n3.7.1 Why log transform?\n\nThere are two main reasons for log transforming a variable:\n\nTo correct for skew in data or residuals\nTo interpret increases in a variable as multiplicative rather than additive.\n\n\nBoth can be understood by recognizing an important property of logarithms; they ‚Äúturn addition into multiplication‚Äù\n\\[\nlog(ùê¥) + log(ùêµ) = log(ùê¥ùêµ)\n\\]\nIn this sense, logarithms turn addition into multiplication.\nExample: suppose we have data for a skewed variable \\(X_1\\):\n\n\n\n\n\nNow we define \\(x_2 = ln(x_1)\\):\n\n\n\n\n\nThis is a toy ‚Äúdata set‚Äù. I chose \\(x_1\\) so that \\(x_2 = ln(x_1)\\) would just be the integers \\(1\\) through \\(10\\).\nNote: there is no more skew.\nAlso note: increasing \\(x_2\\) by one unit results in multiplying \\(x_1\\) by \\(e\\). Addition in \\(x_2 = ln(x_1)\\) is the same thing as multiplication in \\(x_1\\).\n\n\n3.7.2 Same again, with log base 2\nEven simpler: define \\(x_2\\) as log base \\(2\\) of \\(x_1\\), i.e.¬†\\(log_2(x_1)\\)\n\n\n\n\n\nNow increasing \\(x_2\\) by one unit is equivalent to multiplying \\(x_1\\) by \\(2\\). Addition in \\(x_2 = log_2(x_1)\\) is the same thing as multiplication in \\(x_1\\).\n\n\n3.7.3 Log transforming right-skewed data\n\nSkewed data can be bad for regression, in that it can lead to:\n\n-   Non-linear relationship between X and Y\n\n-   Influential outliers\n\n-   Non-normal residuals\n\n-   Non-constant variance in residuals\nSo a simple log transformation can sometimes go a long way toward making the regression model fit the better!\nIt is most common to log transform a response variable, because assumptions about residuals apply to \\(Y\\), not \\(X\\).\nBut if \\(X\\) is skewed, the model can benefit from a log transformation of \\(X\\).\nBear in mind that log transformation will affect the interpretation of slope coefficients!\nIf \\(X\\) is log transformed, then a one unit increase in \\(ln(ùëã)\\) corresponds to multiplying \\(X\\) by \\(e \\approx 2.72\\). So the slope for \\(ln(ùëã)\\) tells you how much \\(Y\\) increases when \\(X\\) is multiplied by \\(2.72\\). Or, even better, use log base \\(2\\) and the slope will give how much \\(Y\\) changes when \\(X\\) is doubled.\nIf \\(Y\\) is log transformed, then the interpretations of slopes get more complicated. Here‚Äôs the math, with the error term omitted for convenience:\n\\[\nln(y_i) = \\beta_0 + \\beta_1X_i\n\\]\n\\[\n\\therefore y_i = e^{\\beta_0 + \\beta_1X_i}\n\\]\nIncrease \\(X\\) by \\(1 \\dots\\)\n\\[\ny_i^* = e^{\\beta_0 + \\beta_1(X_i + 1)} = e^{\\beta_0 + \\beta_1X_i} \\cdot e^{\\beta_1}\n\\]\nSo, when \\(Y\\) is log transformed, a one unit increase in \\(X\\) multiplies predicted \\(Y\\) by \\(e^{\\beta_1}\\)\n\n\n3.7.4 Interpreting slope as a % change in outcome\nRecall the heights vs.¬†wages data from group project 1. The paper reported this estimated model:\n\\[\nln(\\text{wage}) = \\hat{\\beta}_0 + 0.002\\text{(Adult Height)} + 0.027\\text{(Youth Height)} + 0.024\\text{(Age)}\n\\]\nSo, when comparing two adults \\(1\\) inch apart in height but with the same youth height and age predicted wage is multiplied by \\(e^{0.027} = 1.027\\) for the taller adult.\nMultiplying by \\(1.027\\) can be thought of as increasing by \\(2.7\\%\\)\n\n\n3.7.5 Log transformation applied example\nHere is the percent change formula:\n\\[\n\\%\\text{ change (from A to B)} = \\frac{B-A}{A}*100\\%\n\\]\nIf B is \\(1.027*\\)A, then\n\\[\n\\%\\text{ change} = \\frac{1.027*A - A}{A}*100 = \\frac{0.027A}{A}*100 = 2.7\\%\n\\]\nSo, when comparing two adults 1 inch apart in height but with the same youth height and age, predicted wage is \\(2.7\\%\\) higher for the taller adult.\n\n\n3.7.6 Log transformation in \\(Y\\) vs.¬†in \\(X\\)\nRemember that log transformation ‚Äúturns addition into multiplication‚Äù. So, to keep track of how log transforming \\(Y\\) vs.¬†log transforming \\(X\\) affects your model:\n\\[\n\\text{log}(Y_i) = \\beta_0 + \\beta_1X_i + \\epsilon_i\n\\]\n\\[\n\\text{vs.}\n\\]\n\\[\nY_i = \\beta_0 + \\beta_1\\text{log}(X_i) + \\epsilon_i\n\\]\nIf you log transform \\(Y\\) but not \\(X\\), your model estimates the multiplicative change in predicted \\(Y\\) for an additive change in \\(X\\).\nIf you log transform \\(X\\) but not \\(Y\\), your model estimates the additive change in predicted \\(Y\\) for a multiplicative change in \\(X\\)."
  },
  {
    "objectID": "Ch3_Model_Fit.html#non-linearity",
    "href": "Ch3_Model_Fit.html#non-linearity",
    "title": "3¬† Chapter 3: Assessing and improving model fit",
    "section": "3.8 Non-linearity",
    "text": "3.8 Non-linearity\nSometimes data show obvious curvature, in the sense that \\(Y\\) is clearly not a straight line function of \\(X\\).\nThis will be visible on a plot of \\(Y\\) vs.¬†\\(X\\). It will also be visible on a residuals vs.¬†predicted values plot after running a regression.\nIf there is curvature in the relationship between \\(Y\\) and \\(X\\), then it might be sensible to add a polynomial \\(X\\) term:\n\\[\nY_i = \\beta_0 + \\beta_1x_i + \\beta_2x_i^2 + \\epsilon_i\n\\]\n\n3.8.1 ‚ÄúPolynomial‚Äù review\nA ‚Äúpolynomial‚Äù expression is typically one in which variables are included at different powers. For example, a generic third degree polynomial equation might look like:\n\\[\ny = a + bx + cx^2 + dx^3\n\\]\nA ‚Äúsecond degree‚Äù polynomial is one in which an \\(x\\) and \\(x^2\\) term are both included. This is by far the most common type of polynomial seen in regression models.\n\n\n3.8.2 \\(2^{nd}\\) degree and \\(3^{rd}\\) degree polynomials\n\\(2^{nd}\\) degree polynomials are often called ‚Äúquadratic‚Äù. \\(3^{rd}\\) degree polynomials are often called ‚Äúcubic‚Äù. Here are visual examples of simulated quadratic and cubic relationships between \\(Y\\) and \\(X\\):\n\n\n\n\n\n\n\n3.8.3 Curvature in residuals\nHere is regression output comparing a linear model to a quadratic model when the relationship between \\(Y\\) and \\(X\\) is quadratic:\n\n\n\n\n\n\n\n3.8.4 Example: Florida election data\nHere is what the Florida election data look like with the Palm Beach County outlier removed, along with regression results for the simple linear regression model:\n\\[\nBuchanan_i = \\beta_0 + \\beta_1Total\\_Votes_i + \\epsilon_i\n\\]\n\n\n\n\n\nNow we will fit a quadratic polynomial model to the same data:\n\\[\nBuchanan_i = \\beta_0 + \\beta_1Total\\_Votes_i + \\beta_2Total\\_Votes^2_i + \\epsilon_i\n\\]\nTo create this polynomial predictor in jamovi we first need to install the GAMLj module. Then, select ‚ÄúGeneralized Linear Models‚Äù and last our ‚ÄúDependent Variable‚Äù and ‚ÄúCovariates‚Äù.\nUnder the ‚ÄúModel‚Äù drop down menu, click on Total_Votes in the ‚ÄúComponents‚Äù table.\nAn up and down arrow will appear with the degree of Total_Votes, click the up arrow so the 1 changes to 2. Then click the right arrow to add Total_Votes2 to ‚ÄúModel Terms‚Äù\n\n\n\n\n\n\\[\nBuchanan_i = \\beta_0 + \\beta_1Total\\_Votes_i + \\beta_2Total\\_Votes^2_i + \\epsilon_i\n\\]\n\n\n\n\n\nThis is better, but we still see curvature in the residual plot.\nLet‚Äôs try a cubic model:\n\\[\nBuchanan_i = \\beta_0 + \\beta_1Total\\_Votes_i + \\beta_2Total\\_Votes^2_i +\\beta_3Total\\_Votes_i^3 + \\epsilon_i\n\\]\n\n\n\n\n\n\\[\nBuchanan_i = \\beta_0 + \\beta_1Total\\_Votes_i + \\beta_2Total\\_Votes^2_i +\\beta_3Total\\_Votes_i^3 + \\epsilon_i\n\\]\n\n\n\n\n\n\n\n3.8.5 Example: Florida election data: check note\nIt‚Äôs debatable whether this is much better. For one, the \\(Total\\_Votes^2\\) term is non-significant.\nBut think back to multicollinearity. Each polynomial term will be correlated with the other terms ‚Äì after all, \\(Total\\_Votes\\), \\(Total\\_Votes^2\\), and \\(Total\\_Votes^3\\) must all be correlated.\nNote that jamovi will not produce VIFs in GLM. Intuitively we know these will be highly correlated with each other.\nIt turns out that centering helps in polynomial models:\n\n\n\n\n\nBy default jamovi centers polynomials which makes their standard errors smaller than if they were not centered. But, these estimates are not interpretable; you cannot hold \\(Total\\_Votes^2\\) constant while increasing \\(Total\\_Votes\\).\nIn this example, the two counties with the highest total votes are heavily pulling on the regression line."
  },
  {
    "objectID": "Ch3_Model_Fit.html#over-fitting",
    "href": "Ch3_Model_Fit.html#over-fitting",
    "title": "3¬† Chapter 3: Assessing and improving model fit",
    "section": "3.9 Over-fitting",
    "text": "3.9 Over-fitting\nThis model might be ‚Äúover-fit‚Äù.\nOver-fitting is when a model fits the data so well that it ends up fitting random variation that is not of interest.\n\n\n\n\n\nImagine if the two data points on the right had slightly higher vote counts for Buchanan. Or if the next two to their left had slightly lower vote counts. The curve would look very different.\nAt this point, we might just be modelling noise.\nHere is an extreme example of over-fitting: fitting a ‚Äúsmoother‚Äù curve to data and giving it permission to move dramatically up and down through the data.\n\n\n\n\n\nThis line fits the data very well, but does it represent the general trend between total votes and votes for Buchanan? Definitely not!\n(Side note: ‚Äúsmoothers‚Äù are great tools for visualizing and summarizing data, but they can be extremely sensitive to degree of smoothing. We won‚Äôt use them in STAT 331)\nCompare the over-fit model to the linear model.\n\n\n\n\n\nThe linear model may be missing out on some curvature. But it might also make better predictions.\nIf we were to observe a new county with \\(450,000\\) total votes, would we be better guessing that votes for Buchanan fall on the highly curved line or on the straight line?"
  },
  {
    "objectID": "Ch3_Model_Fit.html#back-to-basics-is-the-model-sensible",
    "href": "Ch3_Model_Fit.html#back-to-basics-is-the-model-sensible",
    "title": "3¬† Chapter 3: Assessing and improving model fit",
    "section": "3.10 Back to basics: is the model sensible?",
    "text": "3.10 Back to basics: is the model sensible?\n\nBack to basics: regression models are typically used for two purposes:\n\nPredicting values of the response variable, using the predictor variables. This is done by plugging values for the predictor variables into the estimated model.\nEstimating the association between each individual predictor variable and the outcome while statistically holding other predictor variables constant. This is done by interpreted estimated slope coefficients.\n\n\n\n3.10.1 If you just want to make predictions\n\\(R^2\\) is the easiest to understand statistic for assessing how well your model makes predictions. The closer to \\(1\\), the better.\nMulticollinearity isn‚Äôt an issue. It doesn‚Äôt affect predicted values.\nBUT ‚Äì beware of overfitting! The more complex your model, the more risk you take of modeling noise instead of signal.\nAlso, be aware that \\(R^2\\) can never go down when adding predictors. You can add complete nonsense predictor variables, and the worst that will happen to \\(R^2\\) is that it stays the same.\n\n\n3.10.2 If you want to interpret slopes\nAlways remember that each slope is interpreted under the assumption that all other predictor variables are being held constant, i.e.¬†‚Äúcontrolled for‚Äù.\nThe more predictor variables in the model, the less sense this will make.\nExample: wage vs.¬†height study:\n\n\n\n\n\nIn model 4, the estimated slope for youth height can be interpreted as:\n‚ÄúThe predicted difference in ln(wage) for two people one inch apart in youth height, but equal in adult height, whose mothers have the same # of years of schooling and are both in either skilled fields or work or not, whose fathers have the same # of years of schooling and are both in either skilled fields or work or not, and who have the same number of siblings.‚Äù\n\n\n\n\n\nMaybe this is the best way to think about the association between youth height and wages. But it is fairly complicated.\nIf you‚Äôre going to try to make ‚Äúreal world‚Äù sense out of regression results, your model should be informed by theory.\nThis is necessarily subjective! You have to choose which variables you think are important. You have to think about what makes sense.\n\nThis might require:\n\nLog transforming a variable solely because you like the multiplicative interpretation better than the additive interpretation.\nKeeping a variable in a model even though it isn‚Äôt statistically significant.\nRemoving a variable you are interested in, because it doesn‚Äôt make sense to ‚Äúhold it constant‚Äù when estimating slopes for other variables.\n\n\n\n\n3.10.3 Is the model missing something important?\nThere is another variable in the Florida election data set that could be worth including: ‚ÄúReg_Reform‚Äù: the total number of voters registered with the Reform Party. Pat Buchanan was the Reform Party candidate. Let‚Äôs add it to the model:\n\\[\nBuchanan_i = \\beta_0 + \\beta_1Total\\_Votes_i + \\beta_2Reg\\_Reform_i + \\epsilon_i\n\\]\n\n\n\n\n\nThis residual plot looks great!\n\n\n\n\n\nIt turns out that the curvature in the previous residual plots went away when adding an important variable to the model. No need to mess with polynomials after all!\nAlso, the \\(R^2\\) is roughly the same as in the cubic model using only total votes as a predictor.\nSo we have roughly equal fit, without having to worry about overfitting, and without having to give up interpretability of the slopes.\nOne downside: there is some collinearity. Look at the VIFs.\n\n\n\n\n\nVIF of about \\(5\\) implies \\(\\frac{1}{1-R^2}\\approx 5\\) when using the \\(R^2\\) from:\n\\[\nTotal\\_Votes_i = \\beta_0 + \\beta_1Reg\\_Reform_i + \\epsilon_i\n\\]\nSo, this \\(R^2\\) is about \\(1 ‚àí \\frac{1}{5} = 0.8\\). And so \\(r = \\sqrt{0.8} =0.89\\) . These predictors are strongly correlated.\n\nNote also that total votes is not significant.\nBut: the slope for \\(Reg\\_Reform\\) has a nice interpretation:\nWhen comparing two counties with the same number of votes cast in the election, a county with an additional registered Reform Party member is estimated to have \\(2.24\\) additional votes, on average, for Pat Buchanan.\nShould total votes be taken out of the model? This is a subjective decision.\n\n\n3.10.4 What would you like to ‚Äúcontrol‚Äù for?\nIn regression analysis, we usually emphasize (correctly) that correlation does not imply causation.\nHowever, if you have knowledge or beliefs about causal direction, you should take these into account when choosing your variables!\nExample: in the rheumatoid arthritis study, we were looking at the effect of drugs on inflammation level. In particular, we were comparing how effective they were at reducing inflammation. Suppose we also asked patients to rate their mobility level (RA tends to reduce mobility).\nOur model might be:\n\\[\nDifference_i = \\beta_0 + \\beta_1age_i + \\beta_2drug_i + \\beta_3(age*drug)_i + \\beta_4mobility_i + \\epsilon_i\n\\]\nNow, when interpreting the previous slopes, I am comparing average reduction in inflammation (‚Äúdifference‚Äù, the response variable), between two people who have the same mobility level. But if they have the same mobility level, then they will have more similar inflammation levels than if we allowed mobility level to differ.\nIn other words, because the drug reduces inflammation and improves mobility, ‚Äúcontrolling‚Äù for mobility will make it look like the drugs are less effective than they really are.\n\n\n3.10.5 Beware the ‚Äúkitchen sink‚Äù approach\nThere‚Äôs an old saying: ‚Äútaking everything but the kitchen sink‚Äù.\nIt can be tempting to toss everything but the kitchen sink into a regression model, especially when you have loads of variables that all seem like they‚Äôd be associated with the response.\nBut beware! Adding in one predictor can have a dramatic effect on the slopes of other predictors, as well as on their standard errors.\nIt really really really matters that each slope is estimated as though all other predictors are held constant. This can reveal otherwise unseen effects, but it can also obscure otherwise obvious effects or induce apparent effects that aren‚Äôt real. There is no substitute for scientific reasoning when choosing a model.\n\n\n3.10.6 The model is simpler than what‚Äôs being modeled\nLet‚Äôs take a step back and ask: why are we fitting data to models?\nWell, we are interested in the real world. And the real world in incredibly complicated. Maybe incomprehensibly complicated.\nSo, we simplify things using models. We hope that the model captures the essence of what we care about in the real world. But we know it is a simplification; perhaps an extreme simplification.\n‚ÄúAll models are wrong; some are useful‚Äù ‚Äì George Box\nConsider how the regression model describes where data comes from:\n\\[\nY_i = \\beta_0 + \\beta_1x_{i} + \\epsilon_i, \\text{ where } \\epsilon_i \\sim Normal(0,\\sigma)\n\\]\nThis says that to get data, we pick a value for X, go to a line, then randomly draw a value from a normal distribution and add this to the value on the line. And that‚Äôs where data comes from!\nExcept, that‚Äôs not where data comes from. This is a model. It is a simplification of reality. We use these models because we think they will help us answer questions we care about (e.g.¬†make predictions, identify associations between variables). Don‚Äôt forget that the model is not the thing itself."
  },
  {
    "objectID": "Ch4_ANOVA.html#outline-of-notes",
    "href": "Ch4_ANOVA.html#outline-of-notes",
    "title": "4¬† Chapter 4: ANOVA-based methods",
    "section": "Outline of notes",
    "text": "Outline of notes\n\nWhat is ANOVA?\nOne-way ANOVA\nFactorial ANOVA\nANCOVA"
  },
  {
    "objectID": "Ch4_ANOVA.html#what-is-anova",
    "href": "Ch4_ANOVA.html#what-is-anova",
    "title": "4¬† Chapter 4: ANOVA-based methods",
    "section": "4.1 What is ANOVA?",
    "text": "4.1 What is ANOVA?\nANOVA stands for ‚Äúanalysis of variance‚Äù\nANOVA is regression with categorical predictors. That‚Äôs it.\n\n4.1.1 ANOVA is regression presented differently\nOK, there‚Äôs more to say about ANOVA than just ‚Äúregression with categorical predictors.‚Äù\nANOVA is typically used to analyze data from experiments. In experiments, the categorical predictors are usually groups to which experimental units (aka subjects) are assigned. ANOVA tends to focus on comparing means of different groups to one another. Although ANOVA is ‚Äújust‚Äù regression, there are conventions for reporting ANOVA results that are simpler and cleaner than what we‚Äôve seen for regression.\n\n\n4.1.2 Indicator variables\nWe‚Äôve seen how to incorporate a categorical predictor into a regression model when the predictor takes on two values. We create an ‚Äúindicator‚Äù (aka ‚Äúdummy‚Äù aka ‚Äúbinary‚Äù) variable that takes on the values 0 or 1. For example:\n\\[\nY_i=\\beta_0+\\beta_1x_{1i} + \\epsilon_i\n\\]\n\\[\nx_i = 1 \\text{ if \"treatment;\"} x_i = 0 \\text{ if \"control\"}\n\\]\nHere, \\(\\hat{y}_i = \\hat{\\beta}_0\\) for control, and \\(\\hat{y}_i = \\hat{\\beta}_0 + \\hat{\\beta}_1\\) for treatment.\nWhat if our categorical predictor takes on more than two categories? Suppose we have three groups: treatment 1, treatment 2, and control. We can add another indicator:\n\\[\nY_i=\\beta_0+\\beta_1x_{1i} + \\beta_2x_{2i} + \\epsilon_i\n\\]\n\\[\nx_{1i} = 1 \\text{ if treatment }1; x_{1i} = 0 \\text{ otherwise} \\\\\nx_{2i} = 1 \\text{ if treatment }2; x_{1i} = 0 \\text{ otherwise}\n\\]\n\\(\\hat{y}_i = \\hat{\\beta}_0\\) for control, \\(\\hat{y}_i = \\hat{\\beta}_0 + \\hat{\\beta}_1\\) for treatment 1, and \\(\\hat{y}_i = \\hat{\\beta}_0 + \\hat{\\beta}_1 + \\hat{\\beta}_2\\) for treatment 2.\nControl is serving as the ‚Äúbaseline‚Äù category, represented by the intercept."
  },
  {
    "objectID": "Ch4_ANOVA.html#one-way-anova",
    "href": "Ch4_ANOVA.html#one-way-anova",
    "title": "4¬† Chapter 4: ANOVA-based methods",
    "section": "4.2 One-Way ANOVA",
    "text": "4.2 One-Way ANOVA\nOne-way ANOVA models have a single categorical predictor variable. They can be written as:\n\\[\nY_i = \\mu + \\alpha_j + \\epsilon_{ij}, \\text{ where } \\epsilon_{ij} \\sim Normal(0,\\sigma^2) \\text{ and } i = 1, \\dots ,n_j, j = 1, \\dots, p\n\\]\nWhere there are ‚Äú\\(p\\)‚Äù groups (i.e.¬†the categorical predictor takes on ‚Äú\\(p\\)‚Äù values), ‚Äú$n_j$‚Äù is the sample size of the \\(j^{th}\\) group, and \\(y_{ij}\\) is the \\(i^{th}\\) observation in the \\(j^{th}\\) group.\nHere, \\(\\mu\\) is the overall mean and \\(\\alpha_j\\) is the deviation of the \\(j^{th}\\) group mean from the overall mean.\nSuppose we have 4 groups. The ANOVA model is:\n\\[\nY_i = \\mu + \\alpha_j + \\epsilon_{ij}, \\text{ where } \\epsilon_{ij} \\sim Normal(0,\\sigma^2) \\text{ and } i = 1, \\dots ,n_j, j = 1, \\dots, 4\n\\]\nWritten as a regression model instead:\n\\[\nY_i = \\beta_0 +\\beta_1Group1 + \\beta_2Group2 + \\beta_3Group3 + \\epsilon_{i}, \\text{ where } \\epsilon_{i} \\sim Normal(0,\\sigma^2) \\text{ and } i = 1, \\dots ,n\n\\]\nHere, the ‚ÄúGroup‚Äù predictors are \\(0\\) / \\(1\\) indicator variables. For group \\(4\\), the mean of \\(y\\) is the intercept, \\(\\beta_0\\)."
  },
  {
    "objectID": "Ch4_ANOVA.html#factorial-anova",
    "href": "Ch4_ANOVA.html#factorial-anova",
    "title": "4¬† Chapter 4: ANOVA-based methods",
    "section": "4.3 Factorial ANOVA",
    "text": "4.3 Factorial ANOVA\nIf we have more than one categorical predictor variable, and interactions between the predictors, we have a factorial ANOVA model. Generically:\n\\[\nY_{ijk} = \\mu + \\alpha_j + \\beta_k + (\\alpha\\beta)_{jk} + \\epsilon_{ijk}, \\text{ where } \\epsilon_{ijk} \\sim Normal(0,\\sigma^2) \\\\\n\\text{ and } i = 1, \\dots ,n_{jk}, j = 1, \\dots, p, k = 1, \\dots,q\n\\]\nNow predictor \\(1\\) is \\(\\alpha\\) and predictor \\(2\\) is \\(\\beta\\) and they interact. There are ‚Äú$p$‚Äù groups for predictor \\(1\\) and ‚Äú$q$‚Äù groups for predictor \\(2\\).\nThe subscripts start getting messy pretty fast.\n\n4.3.1 Factorial ANOVA example\nHere‚Äôs an example of a ‚Äú5x2 factorial‚Äù ANOVA, meaning that one variable has 5 groups and the other has 2.\nThe study is on memory: how many words, on average, do people recall when given certain processing tasks?\nThis example is where the graph on our Canvas home page comes from. It uses data simulated to mimic data from a 1974 Hans Eysenck study.\n\n100 subjects were split into 5 recall groups:\n\n‚ÄúCounting‚Äù: subjects counted how many letters were in each presented word\n‚ÄúRhyming‚Äù subjects thought of words that rhymed with each presented word\n‚ÄúAdjective‚Äù: subjects thought of an adjective that could be used to modify each presented word\n‚ÄúImagery‚Äù: subjects were told to form vivid images of each word\n‚ÄúIntentional‚Äù: subjects were told to memorize the word for later recall\n\n\nCounting and rhyming are lower level processing tasks, so the hypothesis was that this group would recall fewer words than the others. Subjects were also classified as ‚Äúyoung‚Äù or ‚Äúold‚Äù.\nHere‚Äôs the ANOVA model:\n\\[\nRecall_{ijk} = \\mu + Group_j + Age_k + (Group \\cdot Age)_{jk} + \\epsilon_{ijk}, \\text{ where } \\epsilon_{ijk} \\sim Normal(0,\\sigma^2) \\\\\n\\text{ and } j = 1, \\dots ,5, k = 1, 2\n\\]\nJamovi has an ‚ÄúANOVA‚Äù function and a ‚Äúregression‚Äù function. We‚Äôll use both the analyze these data.\n\n\n\n\n\nFirst we‚Äôll use ANOVA, using items recalled as the response (dependent) variable, and recall condition and age as factors. Note that jamovi automatically includes their interaction.\n\n\n\n\n\nI am usually not interested in the sums of squares or mean squares in the ANOVA table, as these are not interpretable. We do have p-values for each ‚Äúmain effect‚Äù along with the interaction. More importantly, we have effect size statistics: eta-squared (\\(\\eta^2\\)) and partial eta-squared (\\(\\eta^2p\\))\n\n\n4.3.2 ANOVA effect sizes: \\(\\eta^2\\) and partial-\\(\\eta^2\\)\n\\(\\eta^2\\) is akin to \\(R^2\\) for one factor (main effect or interaction). It gives the proportion of variance in the response attributable to the factor in question:\n\\[\n\\eta^2 = \\frac{SS_{factor}}{SS_{total}}\n\\]\n\n\n\n\n\nHere, we see that recall condition explains the most variance by far, followed by age, followed by their interaction.\n\\(\\eta^2p\\) (partial eta-squared) is like \\(\\eta^2\\), but with the variance accounted for by the other factors removed from the denominator:\n\\[\n\\eta^2p = \\frac{SS_{factor}}{SS_{factor} + SS_{residuals}}\n\\] \nExample: for recall condition,\n\\[\n\\eta^2p = \\frac{1515}{1515 + 722} = 0.677, \\text{ and } \\eta^2 = \\frac{1515}{1515 + 240 + 190 + 722} = 0.568\n\\]\nI personally prefer \\(\\eta^2\\), as their sum cannot exceed \\(1\\). Some prefer \\(\\eta^2p\\), because it quantifies the ‚Äúeffect‚Äù of a factor relative to remaining unexplained variance,\n\n\n4.3.3 The means / interaction plot\nWe can make a nice plot under ‚Äúestimated marginal means‚Äù:\n\n\n\n\n\nThis plot shows every combination of means across recall condition and age. It also has 95% CIs around each mean, and raw data displayed.\n\n\n\n\n\nThis is sometimes called a ‚Äúmeans plot‚Äù or an ‚Äúinteraction plot‚Äù. The interaction is represented by non-parallel lines, e.g.¬†a positive change going from ‚ÄúImagery‚Äù to ‚ÄúIntention‚Äù for young people, but a negative change for old people.\nThe previous plot showed means for recall condition, with separate lines for age. If we flip the order of the variables, we get this:\n\n\n\n\n\nHere we see that mean items recalled for young people is substantially higher than for old people in the last three conditions, but differs only slightly in the first two conditions.\n\n\n4.3.4 ANOVA diagnostics\nWe can also run diagnostic tests, under ‚ÄúAssumption checks‚Äù:\n\n\n\n\n\nThe QQ plot looks pretty good.\n\n\n\n\n\nI personally do not recommend paying attention to Levene‚Äôs ‚Äúhomogeneity of variances‚Äù test, nor to the Shapiro-Wilk normality tests\n\n\n\n\n\nThese tests use null hypotheses of ‚Äúpopulation variance is the same in all groups‚Äù, or ‚Äúthe residuals were drawn from a normal distribution‚Äù. As I don‚Äôt think these model assumptions could be literally true, I am not interested in whether they can be rejected by the data.\nA ‚Äúsignificant‚Äù violation of modeling assumptions does not imply a consequential violation. In particular, if sample size is large, trivial violations of assumption\n\n\n4.3.5 Doing all of this as regression\nThe beginning of these slides claimed that ANOVA is just regression with categorical predictors. Let‚Äôs see what our results look like if we use jamovi‚Äôs ‚Äúlinear regression‚Äù function rather than ‚ÄúANOVA‚Äù.\n\n\n\n\n\nThe predictor variables are entered as ‚Äúfactors‚Äù here, rather than as ‚Äúcovariates‚Äù, to ensure they are treated as categorical rather than as quantitative variables.\n\n\n\n\n\nFinally, the interaction must be specified under ‚Äúmodel builder‚Äù; jamovi does not create regression interactions by default.\nThere are five recall condition groups, giving four indicator variables, all compared against the baseline group ‚Äúcounting‚Äù, whose mean is represented by the intercept.\nThere are two ‚Äúage‚Äù groups; ‚Äúyoung‚Äù is the baseline group.\n\n\n\n\n\nSo the intercept of \\(6.500\\) is the mean words recalled for a young person in the ‚Äúcounting‚Äù condition.\nThe first indicator under RecallCondition is ‚ÄúRhyming ‚Äì Counting‚Äù. Its slope of \\(1.1\\) is the difference in mean recall for these two groups, when Age = Young\nThe interaction slope for ‚ÄúRhyming ‚Äì Counting‚Äù is \\(-1.2\\).\nSo, for Age = Old, the difference in mean recall is \\(1.1 +(-1.2) = ‚àí0.1\\)\nThe interaction terms are all indicators, that ‚Äúturn on‚Äù when Age = Old, and ‚Äúturn off‚Äù when Age = Young.\nNotice that the slope for Age (‚ÄúOld ‚Äì Young‚Äù) is 0.5, suggesting greater recall for older participants.\nHowever, the interaction slopes are all larger negative values.\nSo, when RecallCondition = Counting, the mean items recalled for Age = Old is larger than for Age = Young. But for all other recall conditions, the interaction slopes turn this negative, and mean items recalled is larger for younger participants.\n\n4.3.5.1 Main regression results\n‚ÄúEstimated Marginal Means‚Äù under regression will produce a similar plot to the one made under ANOVA, just without the connecting lines and raw data:\n\n\n\n\n\nIf you look carefully, you should be able to see how the regression results correspond to this plot. For instance, we see that the only condition where Old &gt; Young is Counting.\n\n\n\n4.3.5.2 Residual plot\nWe can get a plot of residuals vs.¬†fitted values.\n\n\n\n\n\nNotice that the residuals are all vertically stacked? This is to be expected when predictor variables are categorical.\nIn this case, there are 5x2 = 10 possible combinations of groups that participants could be assigned to. And so there are only 10 possible ‚Äúfitted‚Äù (i.e.¬†predicted) values that the model can produce."
  },
  {
    "objectID": "Ch4_ANOVA.html#ancova",
    "href": "Ch4_ANOVA.html#ancova",
    "title": "4¬† Chapter 4: ANOVA-based methods",
    "section": "4.4 ANCOVA",
    "text": "4.4 ANCOVA\nANCOVA (analysis of covariance) is ANOVA with an additional continuous predictor variable.\nTypically, this additional continuous predictor is not of primary interest; the primary interest is still comparing group means.\nThe continuous predictor is often thought of as a ‚Äúcovariate‚Äù ‚Äì a variable that should be accounted for when drawing inference on the other variables.\nA common use of ANCOVA is for modeling an outcome when ‚Äúbaseline‚Äù or ‚Äúpre-study‚Äù or ‚Äúpre-test‚Äù scores are available.\nFor instance, consider testing different educational models on different sections of a class. Some get traditional lecture, some are completely ‚Äúflipped‚Äù, and some are a combination of the two.\nIn this study, a preliminary quiz is given on the first day of class. Score on the preliminary quiz will be the covariate. Score on an end of semester quiz (‚Äúpost‚Äù) will be the response variable.\n\\[\nPost_{ij} = \\mu + type_j + \\beta(pre_{ij} - \\overline{pre}) + \\epsilon_{ij}\n\\]\nHere, \\(j\\) goes from \\(1\\) to \\(3\\), for the three teaching types being compared.\nThe pre-test predictor is centered (note that \\(\\overline{pre}\\) is mean for pretests).\nImagine we believe that the mean differences between teaching types will be larger for students with lower pretest scores. To account for this possibility, let type and (centered) pretest interact:\n\\[\nCourse\\_avg_{ij} = \\mu + type_j + \\beta(pre_{ij} - \\bar{pre}) + \\gamma_j(type_j)(pre_j - \\overline{pre}) + \\epsilon_{ij}\n\\]\n\n4.4.1 Looking at the data\nThe data file is called ‚Äútest_pretest‚Äù. Here is the formatting:\n\n\n\n\n\nThis plot was made using Analyses / Exploration / Scatterplot. Density curves are there, just for fun:\n\n\n\n\n\n\n\n\n\n\nThe three lines look pretty close to parallel, so there either isn‚Äôt an obvious interaction here, or it‚Äôs small.\n\n\n\n\n\n\n\n4.4.2 Analysis using ANCOVA\nNotice that the effect size for pre- score is far greater than the effect size for type, or for the interaction. This is not surprising.\n\n\n\n\n\nType is still significant; it‚Äôs \\(\\eta^2p\\) value is much larger than its \\(\\eta^2\\) value.\nCareful interpreting the ‚ÄúEstimated marginal means‚Äù plot ‚Äì it takes into account only ‚Äúpost‚Äù scores!\n\n\n4.4.3 Analysis using regression w/ indicators\nFor the regression analysis, we‚Äôll use two indicator variables. We don‚Äôt have to do it this way; if we just include ‚Äútype‚Äù, jamovi will create the indicators for us, using the first class type listed in the data.\n\n\n\n\n\nNotice that the interaction estimates are small relative to their standard errors (thus producing large p- values).\n\n\n\n\n\nAccording to this regression model, there is not a ‚Äúsignificant‚Äù interaction.\n\n\n4.4.4 Analysis using regression w/factor\nHere‚Äôs what happens if we put in ‚Äútype‚Äù as a factor variable rather than making our own indicators. The results are the same.\n\n\n\n\n\nThe QQ plot and residual plot both look great\n\n\n\n\n\n\n\n4.4.5 Analyzing paired data: ANCOVA vs ANOVA\nAnother approach we could take would be to compute the differences in the two scores for each person, then do a regular ANOVA or regression analysis on those.\n\n\n\n\n\nWe see that there are significant differences in mean test score change between teaching types ($F=8.94, p&lt;0.001$)\n\n\n\n\n\n\n\n\n\n\n\n\n4.4.6 Analyze differences, regression approach\nThis agrees with the ANOVA results from the previous slide: we have a statistically significant difference in mean test score differences (post ‚Äì pre) when comparing ‚Äúcombo‚Äù to ‚Äúflipped‚Äù or ‚Äútraditional‚Äù. And \\(R^2 = \\eta^2\\)!\n\n\n\n\n\nIt turns out here that taking the post ‚Äì pre differences first and then comparing mean differences across teaching types produces similar results to predicting post-test scores using pre-test and teaching type as predictors.\nBut, these methods are not answering the exact same question. Using pre-test as a covariate, we answer the question ‚Äúwhat difference do I expect in post-test scores when comparing two students with the same pre- test score but different teaching types‚Äù?\nWhen differencing first and then doing the analysis, we answer the question ‚Äúwhat differences in the mean post-pre score change do I expect when comparing class types‚Äù?\nThese question sound similar, but they aren‚Äôt the same! Whether to use ANCOVA or do the differencing first is a matter of subjective judgement, and the experts don‚Äôt all agree (see ‚ÄúLord‚Äôs Paradox‚Äù for more fun on this)."
  },
  {
    "objectID": "Ch5_Categorical.html#outline-of-notes",
    "href": "Ch5_Categorical.html#outline-of-notes",
    "title": "5¬† Chapter 5: Analyzing categorical data",
    "section": "Outline of notes",
    "text": "Outline of notes\n\nConfidence intervals and hypothesis tests for proportions\nRelative Risk\nThe chi-square test"
  },
  {
    "objectID": "Ch5_Categorical.html#confidence-intervals-and-hypothesis-tests-for-proportions",
    "href": "Ch5_Categorical.html#confidence-intervals-and-hypothesis-tests-for-proportions",
    "title": "5¬† Chapter 5: Analyzing categorical data",
    "section": "5.1 Confidence intervals and hypothesis tests for proportions",
    "text": "5.1 Confidence intervals and hypothesis tests for proportions\nAt the beginning of the class we reviewed confidence intervals and hypothesis tests for means. These methods can also be used for proportions\nA proportion is just a special kind of mean, where the data are all ones and zeros.\nExample: 8 out of 10 people say ‚Äúyes‚Äù to the question ‚ÄúIs politics too polarized?‚Äù Let yes = 1 and no = 0. Average is:\n\\[\n\\bar{x} = \\frac{1 + 1+ 1 + 1+ 1+1+1+1+0+0}{10} = \\frac{8}{10} =0.8\n\\]\n\nWe make confidence intervals and perform hypothesis tests for proportions using the exact same methods used for means. Only differences are:\n\nSampling distribution of proportions follow z, rather than t (the difference is usually trivial)\nWe denote the population proportion \\(\\pi\\) and the sample proportion \\(\\hat{\\pi}\\)\nStandard error of a sample proportion is \\(s_{\\hat{\\pi}} = \\sqrt{\\frac{\\hat{\\pi}(1-\\hat{\\pi})}{\\pi}}\\)\n\n\n\n5.1.0.1 Political polling example\nHere are results from a recent political poll:\n\n\n\n\n\nSay we want to make a confidence interval for the proportion of registered voters who say that ‚Äúthings in the country are going in the right direction‚Äù. Denote this population proportion \\(\\pi\\).\n\n\n\n\n\n\\[\n95\\% \\text{ CI for }\\pi:\n\\hat{\\pi} \\pm z_{critical} *s_{\\hat{\\pi}} \\\\\n= 0.39 ¬± 1.96 ‚àó \\sqrt{\\frac{0.39*0.61}{1978}}\\\\\n= 0.39 ¬± 0.022 \\\\\n= (0.368, 0.412)\n\\]\nIn jamovi, enter a column of frequencies for each value of the response variable, then use Frequencies / N ‚Äì Outcomes or Frequencies / 2 Outcomes:\n\n\n\n\n\n\n\n5.1.1 Toy example: skin cream and rashes\nHere is an example from the 2013 paper ‚ÄúMotivated Reasoning and Enlightened Self Government‚Äù, by Kahan et. al.:\n\n\n\n\n\n\n\n\n\n\nPutting the data into jamovi:\n\n\n\n\n\nAnalyzing the data using Frequencies / Independent Samples:\n\n\n\n\n\nRash is the response variable\nSkin cream is the predictor variable\nFrequency tells how often each combination occurred. (note: if you had raw data where each row was a single response, you would not use Freq)\nResults! There‚Äôs a lot in here‚Ä¶\n\n\n\n\n\n\n\n\n\n\nSplit bar plot: displays each cell in the contingency table as a bar:\n\n\n\n\n\nHere we can easily see that the largest number of people were those who got the skin cream and whose rash got better.\nBut, we can also see that rashes got better at a higher rate for those who did not get the skin cream.\nWe‚Äôll cover the chi-square results soon. Right now, let‚Äôs have jamovi directly compare proportions, using Frequencies / Independent Samples:\n\n\n\n\n\nHere, jamovi quantifies what we saw in the split bar plot: that the proportion of those who got better without the skin cream is greater than the proportion of those who got better with the skin cream:\n\n\n\n\n\n\\[\n\\text{\"Probability of Better, given Yes\" - \"Probability of Better, given No\" } =-0.0876 \\\\\nP(Better|Yes) - P(Better|No) = -0.0876\n\\]\nWe also see a 95% CI for this difference, which is fairly wide and just barely excludes zero\n\n\n\n\n\nAnd we see the p-value testing against:\n\\[\nH_0:\\pi_{(Better|Yes)} - \\pi_{(Better|No)} = 0\n\\]\nThe two-sided p-value is 0.047, so this result is just barely significant. Woohoo!"
  },
  {
    "objectID": "Ch5_Categorical.html#relative-risk",
    "href": "Ch5_Categorical.html#relative-risk",
    "title": "5¬† Chapter 5: Analyzing categorical data",
    "section": "5.2 Relative risk",
    "text": "5.2 Relative risk\nInstead of knowing the difference in proportions / probabilities, we may want to know their ratio. This would tell us how many times larger one is than the other.\nThis is quantified by the ‚Äúrelative risk‚Äù, a.k.a. ‚Äúrisk ratio‚Äù :\n\\[\nRR = \\frac{\\text{Proportion A}}{\\text{Proportion B}}\n\\]\nThe phrase ‚Äúrisk‚Äù is used because this method is popular for comparing the risk of a negative outcome under two conditions (e.g.¬†treatment and control, or drug A and drug B)\n\n5.2.1 Relative risk in jamovi\n‚ÄúRelative Risk‚Äù is an option under Comparative Measures. jamovi will give conditional probabilities based on the order of the data. Here, rash getting worse is Better is selected:\n\n\n\n\n\nWe see here that \\(RR = 0.895\\).\n\n\n\n\n\nNotice the 95% CI is not very wide.\nWe could also flip this ratio:\n\\[\n\\frac{P(Better|No)}{P(Better|Yes)}\n\\]\nNow everything is reciprocated, e.g.\n\\[\n\\frac{1}{0.809}=1.236\n\\]\nNotice that, for ‚ÄúBetter‚Äù, RR is small but (just barely) statistically significant, because the CI does not contain 1.\nFor ‚ÄúWorse‚Äù, RR is large but (just barely) not statistically significant, because the CI contains 1.\nThere is another very popular kind of ratio called an ‚Äúodds ratio‚Äù, which we will consider more when we cover logistic regression."
  },
  {
    "objectID": "Ch5_Categorical.html#the-chi-square-test",
    "href": "Ch5_Categorical.html#the-chi-square-test",
    "title": "5¬† Chapter 5: Analyzing categorical data",
    "section": "5.3 The chi-square test",
    "text": "5.3 The chi-square test\nThe chi-square ( \\(\\chi^2\\)) test is a popular hypothesis test for comparing observed frequencies to expected frequencies under a null hypothesis.\nThe null is typically that of ‚Äúindependence‚Äù between two categorical variables, meaning the probability an observation falls into a category for one variable does not depend on its category for the other variable\n(As a side note, \\(\\chi^2\\) is a distribution that is used for many purposes, including modeling distributions of variances. A statistic that is distributed chi-square is not necessarily being used in the context of the chi-square test outlined here)\nStaying with the skin cream example, here is the contingency table as jamovi initially reports it:\n\n\n\n\n\nThere is a lot being shown here. Top rows are observed frequencies, what jamovi calls ‚Äúobserved‚Äù.\nThe two middle rows are column % and row %.\nNotice that the column %‚Äôs sum to 100% down the columns, and the row %‚Äôs sum to 100% across the rows.\nWe can have jamovi display the components of a chi-square test.\nThe first of these are the expected counts under the null hypothesis of independence.\n\n\n\n\n\nTo see how these ‚Äúexpected‚Äù counts would suggest independence, consider the relative risk:\n\\[\nRR = \\frac{P(Worse|No)}{P(Worse|Yes)} \\\\\n= \\frac{(\\frac{28.8451}{28.8451 + 99.1549})}{(\\frac{67.1549}{67.1549 + 230.845})} \\\\\n= \\frac{(\\frac{28.8451}{128})}{(\\frac{67.1549}{298})} \\\\\n= \\frac{0.2254}{0.2254} \\\\\n=1\n\\]\n\n5.3.0.1 Chi-square statistic\nThe chi-square statistic compares the expected frequencies under the null (which we denote E) to the observed frequencies in the data (which we denote O).\n\\[\n\\chi^2 = \\sum\\frac{(O-E)^2}{E}\n\\]\nThis is a general formula that can be used when you have one variable, two variable, three variables, etc.\nMost popular use is for two variables, as in this skin cream example.\nJust to verify the math, here‚Äôs the chi-square calculation for the upper left cell of the table:\n\\[\n\\sum\\frac{(O-E)^2}{E} = \\frac{(107-99.2)^2}{230.8} + \\frac{(223-230.8)^2}{230.8} + \\frac{(75-67.2)^2}{67.2} + \\frac{(21-28.8)^2}{28.8} =3.94\n\\]\nAnd here is the jamovi output showing the full chi-square test:\n\n\n\n\n\n(‚ÄúPearson‚Äù chi-square is the classic chi-square test)\nVerifying that this chi-square statistic is indeed the sum of the chi-square values for each of the four cells:\n\\[\n\\sum\\frac{(O-E)^2}{E} = 0.6207 + 2.1336 + 0.2666 + 0.9165 = 3.937\n\\]\nThe p-value is found using the appropriate degrees of freedom for the chi-square distribution. We won‚Äôt cover this part.\nIn this case, we see that the chi-square test just barely meets the standard for statistical significance (\\(p = 0.0472\\)).\nThis is in line with the other methods we used to analyze these data.\nRecall that the test for a difference in proportions was barely significant, and the risk ratios were just on either side of significance, depending on whether ‚Äúrash got worse‚Äù or ‚Äúrash got better‚Äù was used as the outcome variable.\nThe chi-square test can be used when we have frequencies for a single variable. All we have to do is specify expected counts or probabilities.\nGoing back to the polling data, we can select Frequencies / N - Outcomes, and then make Answer ‚ÄúVariable‚Äù and Frequency ‚ÄúCounts‚Äù.\n\n\n\n\n\nSuppose the null hypothesis is that equal numbers of voters feel the country is on the right vs.¬†wrong track. Enter 0.5 for hypothesized probability:\nHere we get a very large chi-square statistic and a very small p-value.\n\n\n\n\n\nNo surprise; the frequencies were very different!\nSide note: there are lots of different ‚Äúchi-square tests‚Äù; this term refers to any test whose test statistic follows a chi-square distribution. So you may see ‚Äúchi-square tests‚Äù that are not being used to test against a null of independence for categorical variables.\n\n\n5.3.1 Wrapping up\nMy personal view is that hypothesis testing should only be used when a null hypothesis is of direct scientific interest. In general, I‚Äôm more interested in statistics that quantify the size of an ‚Äúeffect‚Äù than I am in deciding whether or not to reject a null of ‚Äúno effect‚Äù. So, I am not a big fan of this chi-square test. To compare rates for categorical variables, I prefer a 95% CI around either a relative risk or a difference in proportions, whichever seems more meaningful for the question at hand.\nIn the case of a political poll, I don‚Äôt believe that identical proportions of voters in the country would endorse two different statements, and so I‚Äôm not very interested testing this as a null hypothesis. I‚Äôm interested in how large a difference might be, and how much statistical uncertainty there is an estimate for that difference.\nThe methods we‚Äôve covered in these notes are useful for analyzing fairly simple categorical data. In the next set of notes, we will look at logistic regression, which is a way to use regression modeling to predict the outcome of a categorical variable."
  },
  {
    "objectID": "Ch6_GLMs.html#outline-of-notes",
    "href": "Ch6_GLMs.html#outline-of-notes",
    "title": "6¬† Generalized Linear Models (GLMs)",
    "section": "Outline of notes",
    "text": "Outline of notes\n\nLogistic regression\nOdds\nInterpreting slope in logistic regression\nPoisson regression\nThe structure of a GLM\nMaximum likelihood estimation, conceptually\nPoisson regression example\nNegative binomial regression"
  },
  {
    "objectID": "Ch6_GLMs.html#logistic-regression",
    "href": "Ch6_GLMs.html#logistic-regression",
    "title": "6¬† Generalized Linear Models (GLMs)",
    "section": "6.1 Logistic regression",
    "text": "6.1 Logistic regression\nAll of the regression methods we‚Äôve seen have involved models in which the response variable is normally distributed, given values for the predictor variables\nIn other words, the residuals have been modeled as normal.\nWhat if we have a different kind of response variable? In particular, consider a binary response variable. Maybe the outcomes are ‚Äúyes‚Äù and ‚Äúno‚Äù, or ‚Äúsuccess‚Äù and ‚Äúfailure‚Äù, or ‚Äúpresent‚Äù and ‚Äúabsent‚Äù.\nLogistic regression is a type of ‚Äúgeneralized linear model‚Äù (GLM) that works well for modeling binary outcome data.\nBefore we get into logistic regression, though, let‚Äôs see what happens if we use standard regression (sometimes called ‚Äúordinary least squares‚Äù, or OLS regression) with a binary response.\nWe‚Äôll use simulated data corresponding to a study of sexual harassment reporting at a university. (Brooks and Perot ‚ÄúReporting Sexual Harassment: Exploring a Predictive Model‚Äù (1991)).\nHere is data on whether or not sexual harassment at a university was reported, using the offensiveness of the behavior as a predictor variable:\n\n\n\n\n\nData points are ‚Äújittered‚Äù so that they don‚Äôt fall right on top of one another.\nSuppose we want to predict the value of ‚ÄúReport‚Äù, using ‚ÄúOffensBeh‚Äù.\nHere is the linear regression line. In this picture, the response variable takes on the values 0 and 1, and the data are not jittered.\n\n\n\n\n\nThe predicted value of ‚ÄúReport‚Äù can be thought of as the predicted probability that Report=1 (for reported behavior)\nNote that this line can go below zero and above one. We don‚Äôt want to predict probability greater than 1! A straight line is not great here. Logistic regression is an alternative to this straight line model.\n\n6.1.1 The logistic regression model\nBy now we are well familiar with the linear regression model:\n\\[\nY_i=\\beta_0+\\beta_1x_{1i} + \\beta_2x_{2i} + \\dots + \\beta_px_{pi} + \\varepsilon_i \\\\\n\\varepsilon_i \\sim \\text{Normal}(0,\\sigma^2)\n\\]\nHere is an equivalent way of writing it:\n\\[\nY_i \\sim Normal(\\mu_i, \\sigma^2) \\\\\n\\mu_i=\\beta_0+\\beta_1x_{1i} + \\beta_2x_{2i} + \\dots + \\beta_px_{pi}\n\\]\nIn other words, the response variable is normally distributed with some mean \\(\\mu\\), and the value of \\(\\mu\\) is determined by the predictor (\\(x\\)) variables.\n\n\n6.1.2 Writing a logistic regression model\nWe will take this approach to writing the logistic regression model.\nWhat we want is a regression equation that looks like this:\n\\[\nY_i=\\beta_0+\\beta_1x_{1i} + \\beta_2x_{2i} + \\dots + \\beta_px_{pi}\n\\]\nBut that will work when \\(Y_i\\) does not follow a normal distribution.\n\n\n6.1.3 The theoretical logistic regression model\nResponse variable \\(Y\\) takes on the values 0 and 1.\nDenote the probability that \\(Y = 1\\) as \\(\\pi\\).\nThis can be written \\(Y\\sim Bernoulli (\\pi)\\)\n(The Bernoulli distribution is a distribution of 1‚Äôs and 0‚Äôs, where the probability of 1 is \\(\\pi\\) and the probability of 0 is \\(1 ‚àí \\pi\\))\nWe will use regression to model \\(\\pi\\), the probability that \\(Y = 1\\). This is often thought of as the probability of a ‚Äúsuccess‚Äù.\nIf we wanted, we could use this model:\n\\[\nY_i\\sim Bernoulli (\\pi_i)\n\\\\\n\\pi_i=\\beta_0+\\beta_1x_{1i} + \\beta_2x_{2i} + \\dots + \\beta_px_{pi}\n\\]\nThe standard deviation of a Bernoulli distribution is \\(\\pi(1 ‚àí \\pi )\\). So, \\(\\pi\\) is the only parameter for this distribution. This is different from the normal distribution, which has two parameters \\(\\mu\\) and \\(\\sigma\\).\n\n\n6.1.4 Logit: log odds\nBut, as we saw in the opening example, a linear model for probability can have serious deficiencies.\nSo, instead of a linear model for probability \\(\\pi\\), we‚Äôll make a linear model for a function of \\(\\pi\\), so that the variable on the left hand side of the equation is linearly related to the variable(s) on the right.\nIn logistic regression, we use the ‚Äúlogit‚Äù function, also known as ‚Äúlog odds‚Äù\n\\[\nlogit(\\pi) = ln(\\frac{\\pi}{1-\\pi}) = \\textit{\"log odds\"}\n\\]\nApplied to our sexual harassment example, we would like to predict the probability that harassing behavior is reported. This probability is denoted \\(\\pi\\)\nPlugging this into the logit formula:\n\\[\nlogit(P(reported)) = ln(\\frac{P(reported)}{1-P(reported)}) = \\textit{\"log odds\" of reporting}\n\\]\nThis will be our response variable for logistic regression. Note that this time we wrote \\(P(reported)\\) rather than \\(\\pi\\); these mean the same thing.\nLogit vs.¬†probability, visually:"
  },
  {
    "objectID": "Ch6_GLMs.html#odds",
    "href": "Ch6_GLMs.html#odds",
    "title": "6¬† Generalized Linear Models (GLMs)",
    "section": "6.2 Odds",
    "text": "6.2 Odds\n\n6.2.1 Odds vs.¬†probability\nTo understand logistic regression, you‚Äôll need to understand odds.\nIn casual English, ‚Äúodds‚Äù and ‚Äúprobability‚Äù are often used interchangeably.\nIn statistics, they are not the same thing. Odds tell you how likely one outcome is compared to another.\nFor instance, you might hear that a football team has been given ‚Äú3 to 2‚Äù odds of winning a game. This means that their probability of winning is \\(\\frac{3}{2}= 1.5\\) times as big as their probability of losing. Or, that they‚Äôd be expected to win 3 times for every 2 times they lost.\nFormally, consider some outcome A, where the probability of A occurring is written as ‚Äú\\(P(A)\\)‚Äù. In this case,\n\\[\nodds(A) = \\frac{P(A)}{1-P(A)} = \\frac{\\textit{probability A occurs}}{\\textit{probability A does not occur}}\n\\]\nThis is the ratio of the probability A occurs to the probability A does not occur. To convert odds into probability, we use:\n\\[\nP(A) = \\frac{odds(A)}{odds(A) + 1}\n\\]\nSome probabilities and their associated odds:\n\n\n\n\\(P(A)=\\)\n\\(Odds(A)=\\frac{P(A)}{1-P(A)}\\)\n\n\n\n\n0.001\n0.001/0.999=0.001001\n\n\n0.05\n0.05/0.95=0.0526\n\n\n0.2\n0.2/0.4=0.25\n\n\n0.5\n0.5/0.5=1\n\n\n0.8\n0.8/0.2=4\n\n\n0.95\n0.95/0.05=19\n\n\n0.999\n0.999/0.001=999\n\n\n\nThink of odds(A) as ‚Äúhow many times will A occur for every time A does not occur?‚Äù\nSometimes we add ‚Äúto 1‚Äù to an odds statement, e.g.¬†‚Äúodds of 4 to 1‚Äù means ‚Äúthis outcomes occurs 4 times for every 1 time it does not occur.‚Äù\n\n\n6.2.2 Back to the logistic regression model\nThe response variable for logistic regression, again, is:\n\\[\nlogit(\\pi) = ln(\\frac{\\pi}{1-\\pi}) = \\textit{\"log odds\"}\n\\]\nSo, the full logistic regression model is :\n\\[\nY_i\\sim Bernoulli (\\pi_i)\n\\\\\nln(\\frac{\\pi_i}{1-\\pi_i})=\\beta_0+\\beta_1x_{1i} + \\beta_2x_{2i} + \\dots + \\beta_px_{pi}\n\\]\nWe are not actually interested in log odds; we only make this conversion for mathematical convenience. So, once we have \\(logit(\\hat{\\pi}_i)\\), we can \\(\\hat{\\pi}_i\\) back via the inverse logit function, \\(logit^{-1}(x)=\\frac{e^x}{1+e^x}\\):\n\\[\n\\begin{align}\nlogit^{-1}(logit(\\pi)) \\\\\n&= \\frac{e^{logit(\\pi)}}{1 + e^{logit(\\pi)}} \\\\\n&= \\frac{odds}{1 + odds} \\\\\n&= \\frac{\\frac{\\pi}{1-\\pi}}{\\frac{1-\\pi}{1 - \\pi} + \\frac{\\pi}{1-\\pi}} \\\\\n&= \\frac{\\frac{\\pi}{1-\\pi}}{\\frac{1}{1-\\pi}} \\\\\n&= \\pi\n\\end{align}\n\\]\nIn the context of the logistic regression model:\n\\[\n\\begin{align}\n\\hat{\\pi}_i & =logit^{-1}(logit(\\hat{\\pi_i}))\\\\\n&=\\frac{e^{logit(\\hat{\\pi_i})}}{1 + e^{logit(\\hat{\\pi_i})}} \\\\\n&=\\frac{e^{\\hat{\\beta}_0 + \\hat{\\beta}_1x_{1i} + \\hat{\\beta}_2x_{2i} + \\dots + \\hat{\\beta}_px_{pi}}}{1 + e^{\\hat{\\beta}_0 + \\hat{\\beta}_1x_{1i} + \\hat{\\beta}_2x_{2i} + \\dots + \\hat{\\beta}_px_{pi}}}\n\\end{align}\n\\]\n\n\n6.2.3 jamovi example\nApplying this to the harassment data, we use Linear Models / Generalized Linear Models in jamovi and select Logistic under Categorical dependent variable.\n\n\n\n\n\nNote that ‚ÄúTarget Level‚Äù defaults to zero. Changing it to 1 makes sense in this case; we want to predict ùëÉ(ùëÖùëíùëùùëúùëüùë°ùëíùëë).\njamovi also produces a Loglikelihood ratio test, but we will just focus on ‚ÄúParameter Estimates‚Äù:\n\n\n\n\n\nHere is our estimated model:\n\\[\n\\textit{log odds (Reported)} = -1.7976 + 0.4869*OffensBeh\n\\]\nPlugging in large and small values for OffensBeh:\n\\[\n\\textit{log odds (Reported)} = -1.7976 + 0.4869*1 = -1.3107\n\\] \\[\n\\textit{log odds (Reported)} = -1.7976 + 0.4869*8 = 2.0976\n\\]\n\\[\n\\hat{\\pi}|OffensBeh = 1: \\\\ \\frac{e^{-1.3107}}{1 + e^{-1.3107}} = \\frac{0.2696}{1.2696} = 0.21\n\\] \\[\n\\hat{\\pi}|OffensBeh = 8: \\\\ \\frac{e^{-2.0976}}{1 + e^{-2.0976}} = \\frac{8.1466}{9.1466} = 0.89\n\\]"
  },
  {
    "objectID": "Ch6_GLMs.html#interpreting-slope-in-logistic-regression",
    "href": "Ch6_GLMs.html#interpreting-slope-in-logistic-regression",
    "title": "6¬† Generalized Linear Models (GLMs)",
    "section": "6.3 Interpreting slope in logistic regression",
    "text": "6.3 Interpreting slope in logistic regression\nThe slope coefficient is directly interpreted as change in log odds for a one unit increase in the predictor. ‚ÄúLog odds‚Äù are not of direct interest.\nExponentiating both sides of the equation gives straight odds\n\\[\nodds = (\\frac{\\pi_i}{1-\\pi_i})=e^{\\beta_0+\\beta_1x_{1i} + \\beta_2x_{2i} + \\dots + \\beta_px_{pi}}\n\\]\nThis means that, for a one unit increase in \\(X_1\\), odds are multiplied by \\(e^\\beta_1\\)\nFor the harassment data:\n\\[\n\\textit{log odds (Reported)} = -1.7976 + 0.4869*OffensBeh\n\\]\n\\(\\hat{\\beta}_1 = 0.4869\\). So, for a one unit increase in OffensBeh, predicted odds of reporting are multiplied by \\(e^{0.4869} = 1.627\\)\nIn other words, there is about a 63% increase in odds of reporting when OffensBeh increases by one. NOTE: odds are not probabilities!\nComparing probabilities and odds from this model:"
  },
  {
    "objectID": "Ch6_GLMs.html#poisson-regression",
    "href": "Ch6_GLMs.html#poisson-regression",
    "title": "6¬† Generalized Linear Models (GLMs)",
    "section": "6.4 Poisson regression",
    "text": "6.4 Poisson regression\nWe‚Äôll now look at two other popular GLMs: Poisson (‚Äúpwa-sawn‚Äù roughly) and negative binomial.\nThese are used for modeling count data, which can be extended to how often a categorical variable takes on some value. Thus Poisson regression can be used to model contingency table data.\n\n6.4.1 The Poisson distribution\nThe Poisson distribution is a discrete probability distribution. A Poisson distributed variable takes on only positive integer values. The integer is referred to as ‚Äúcount‚Äù or ‚Äú# of events‚Äù.\nThe Poisson distribution has a single parameter, \\(\\lambda\\) (‚Äúlambda‚Äù), which is sometimes called the ‚Äúrate‚Äù parameter.\n\\(\\lambda\\) is both the mean and the variance of a Poisson distribution\nThe probability function for the Poisson is:\n\\[\nP(count = k) = \\frac{\\lambda^k}{e^\\lambda k!}\n\\]\n\n\n6.4.2 Visualizing the Poisson distribution"
  },
  {
    "objectID": "Ch6_GLMs.html#the-structure-of-a-glm",
    "href": "Ch6_GLMs.html#the-structure-of-a-glm",
    "title": "6¬† Generalized Linear Models (GLMs)",
    "section": "6.5 The structure of a GLM",
    "text": "6.5 The structure of a GLM\n\n6.5.1 The link function\nIn logistic regression, the response variable was $logit(\\pi) = ln(\\frac{\\pi}{1-\\pi}) $\nIn Poisson regression, the response variable is \\(ln(\\lambda)\\)\nThe reasoning will be that the natural log allows the estimated rate to be modeled as a linear function of some predictor variables.\nThis is how GLMs work: they allow us to use non-normal response variables by expressing a function of their mean as a linear function of the predictors.\nA GLM has three parts:\n\nA response variable with some distribution\nA ‚Äúlink function‚Äù, \\(g(\\cdot)\\), that is applied to the mean of the response variable.\nA linear expression of the predictor variables: \\(\\beta_0 + \\beta_1X_1 + \\beta_2X_2 \\dots\\)\n\n\n\n6.5.2 GLM examples\nLogistic regression uses \\(Y\\sim Bernoulli (\\pi)\\) as the response variable and \\(g(\\pi) = ln(\\frac{\\pi}{1-\\pi})\\) as the link function.\nPoisson regression uses \\(Y\\sim Poisson(\\lambda)\\) as the response variable and \\(g(\\lambda) = ln(\\lambda)\\) as the link function.\nOrdinary least squares (OLS) regression can also be considered a special case of a GLM. It uses \\(Y\\sim Normal (\\mu, \\sigma^2)\\) as the response variable and the identity function, \\(g(\\mu) = \\mu\\) as the link."
  },
  {
    "objectID": "Ch6_GLMs.html#maximum-likelihood-estimation-conceptually",
    "href": "Ch6_GLMs.html#maximum-likelihood-estimation-conceptually",
    "title": "6¬† Generalized Linear Models (GLMs)",
    "section": "6.6 Maximum likelihood estimation, conceptually",
    "text": "6.6 Maximum likelihood estimation, conceptually\nIt‚Äôs worth briefly noting that the mathematical method used to come up with parameter estimates for GLMs is not ‚Äúleast squares‚Äù. So, we are not getting our \\(\\beta\\)‚Äôs by minimizing sums of squared residuals.\nInstead, the estimation procedure we use is called ‚Äúmaximum likelihood‚Äù. This method finds the values of the parameter estimates that maximize (i.e.¬†make as large as possible for a given set of data) something called ‚Äúthe likelihood function‚Äù. The likelihood function takes a fixed set of data and an assumed distribution (e.g.¬†normal), and gives the ‚Äúprobability of the data‚Äù, given some set of parameter values.\nSo, the coefficient estimates that we get in GLM output would make our data ‚Äúmore likely‚Äù than our data would be under any other set of possible estimates. They are the estimates that maximize the likelihood of our data."
  },
  {
    "objectID": "Ch6_GLMs.html#poisson-regression-example",
    "href": "Ch6_GLMs.html#poisson-regression-example",
    "title": "6¬† Generalized Linear Models (GLMs)",
    "section": "6.7 Poisson regression example",
    "text": "6.7 Poisson regression example\nWe‚Äôll use some General Social Survey data for this example.\nPoisson is good for modeling count data, so we‚Äôll use a response variable that takes the form of counts.\nFor this example, the goal will be to look at the relationship (if any) between the number of sibling a person has, and the number of children that person has.\nOur question will be: do people with more siblings tend to have more children? And if so, can we quantify the relationship?\nIt would be wise to collect data on covariates that we expect will also be related to the number of children someone has.\nAn obvious one is age. Older people will have more children than younger people.\nWe might also want to control for ‚Äúculture‚Äù. If people from different cultural backgrounds tend to have more or fewer children, then this would definitely induce a relationship between # of siblings and # of children.\nThere are lots of possible ways to try account for cultural background. I‚Äôm choosing rate of attending religious services.\n\nSo, the variables will be:\n\n# of children\n# of siblings\nAge\nFrequency of attending religious services\n\n\nWe‚Äôll just look at 2018 data. The GSS lets us choose any years we want, going back to 1972.\nThis data set is on Canvas, as GSS_Children_Siblings.jmp\n\n6.7.1 Poisson regression EDA\nFirst thing to do is plot our variables.\n\n\n\n\n\nYikes! There‚Äôs some cleaning to do. The instances of 98 siblings are not real data points.\nGSS data explorer website lets us look in detail at each variable. Here is part of the coding for ‚ÄúSIBS‚Äù:\n\n\n\n\n\nAnd here‚Äôs the coding for the variable CHILDS.\n\n\n\n\n\nSo, CHILDS = 9 is a value for missing data. These should also be excluded.\nThis should feel familiar ‚Äì remember how messy the NLSY data was in the heights analysis?\nKeep in mind that data in public databases often have idiosyncrasies like this.\nHere are the row selection options that will select all rows with invalid responses.\n\n\n\n\n\nOnce selected, they can be excluded.\nHere is the distribution of CHILDS.\n\n\n\n\n\nThis looks a lot like a Poisson distribution. Hooray!\nTo run the regression, use Linear Models / Generalized Linear Models and choose Poisson(overdispersion) for Frequencies.\n\n\n\n\n\nJMP will automatically choose the log link\nSelect the ‚ÄúOverdispersion tests and intervals‚Äù box\nHere are results for a simple model, where # of siblings is the sole predictor of # of children. We‚Äôll just look at the parameter estimates and the overdispersion statistic:\n\n\n\n\n\nLetting \\(\\hat{\\lambda}\\) represent the predicted mean # of children, we have:\n\\[\nln(\\hat{\\lambda}) = 0.375 + 0.0631(ùëÜùêºùêµùëÜ)\n\\]\n\n\n6.7.2 Interpreting the slope\nYou might not be surprised to learn that, due to the log link, the exponentiated slope is interpreted as the multiplicative change in the estimated value of the response variable, given a one unit increase in the predictor variable: \\(e^{0.0627} = 1.065\\)\n\nSo, increasing # of siblings by one is associated with an 6.5% increase in # of children.\nWe can see that this is statistically significant, but it is also small.\nIt is also not obvious that % change is the best way to quantify this. Maybe an OLS model would have been more interpretable.\nIt might be more desirable to relate an additive change in siblings to an additive change in children.\nDownside is that # of children is not normally distributed.\nAs is often the case, we are trading some interpretability for a better fitting model.\n\n\n6.7.3 Overdispersion\nThe Poisson distribution makes a strong assumption: the mean should be equal to the variance.\nOften, we observe real data in which the variance is greater than the mean.\nThis is referred to as ‚Äúoverdispersion‚Äù.\nJamovi reports an overdispersion estimate:\n\n\n\n\n\nThe overdispersion statistic is the ratio of the Pearson chi-square statistic to its degrees of freedom.\nIf mean = variance, this should be equal to 1. But it rarely is. If there is strong overdispersion, a negative binomial model should fit better.\n\n\n6.7.4 Fitting a larger model\nFor these data, it turns out that # of siblings, age, frequency of attending religious services, and the interactions between age and the other two variables are all statistically significant and all improve model fit. Here are the parameter estimate results for this model:\n\n\n\n\n\nThe other predictor variables and the interactions can be interpreted in the usual ways.\nOne thing to notice is that the estimate for SIBS has not changed much. So, while the other covariates and interactions matter, they don‚Äôt substantially change our interpretation of the SIBS predictor."
  },
  {
    "objectID": "Ch6_GLMs.html#negative-binomial-regression",
    "href": "Ch6_GLMs.html#negative-binomial-regression",
    "title": "6¬† Generalized Linear Models (GLMs)",
    "section": "6.8 Negative binomial regression",
    "text": "6.8 Negative binomial regression\nThere is also still some overdispersion, though less than there was before:\n\n\n\n\n\nRemember that the Poisson distribution only has one parameter. This limits its flexibility.\nThe negative binomial distribution is similar to the Poisson distribution, but it is more flexible, and may be a better choice in the presence of overdispersion.\nThe negative binomial distribution is also a distribution for count data. It is interpreted as giving the number of ‚Äúsuccess‚Äù before a certain number of ‚Äúfailures‚Äù occur.\nThere are two parameters: \\(p\\), the probability of success, and \\(r\\), the number of failures at which counting stops.\nIf a variable \\(Y\\) is distributed negative binomial, we denote it:\n\\[\nY\\sim NB(r,p)\n\\]\nThe mean of the negative binomial distribution is \\(\\frac{rp}{1-p} = r*odds(success)\\)\nExample: suppose \\(p = 0.8\\) and \\(r = 2\\). We expect \\(\\frac{2‚àó0.8}{0.2} = 8\\) success before we observe two failures.\nOr suppose \\(p= 0.5\\) and \\(r= 1\\). We expect \\(\\frac{1‚àó0.5}{0.5} = 1\\) success before we observe 1 failure.\nFor practical purposes, negative binomial regression will show better fit than Poisson regression in the presence of overdispersion.\nThe tradeoff is that the interpretation is less generally applicable. It might not make sense to think of your count variable as # of successes for a certain # of failures.\nAs with Poisson regression, negative binomial regression uses a GLM with a log link.\n\n6.8.1 Negative binomial example\nHere is where you select negative binomial regression in jamovi:\n\n\n\n\n\nUnder ‚ÄúGeneralized Linear Models‚Äù, under ‚ÄúFrequencies‚Äù choose ‚ÄúNegative Binomial‚Äù.\n\n\n6.8.2 Negative binomial results\n\n\n\n\n\nThese results are awfully similar to the Poisson results.\nIt is often the case that different statistical methods designed for the same purpose will with similar results."
  },
  {
    "objectID": "Ch7_Mixed.html#outline-of-notes",
    "href": "Ch7_Mixed.html#outline-of-notes",
    "title": "7¬† Chapter 7: Mixed-effects models",
    "section": "Outline of notes",
    "text": "Outline of notes\n\nRepeated measures\nThe independence assumption\nCheating with repeated measures\nCheating ourselves with repeated measures\nChallenges and opportunities with repeated measures data\nRandom effects\nThe mixed effects model\nApplied example: ‚ÄúThe liking gap‚Äù\nReplicating a published mixed model analysis"
  },
  {
    "objectID": "Ch7_Mixed.html#repeated-measures",
    "href": "Ch7_Mixed.html#repeated-measures",
    "title": "7¬† Chapter 7: Mixed-effects models",
    "section": "7.1 Repeated measures",
    "text": "7.1 Repeated measures\n‚ÄúRepeated measures‚Äù refers to measuring the same subjects (people, trees, dogs, cities, cells, widgets, etc) more than once.\nStudies that use repeated measures are often referred to as ‚Äúwithin subjects‚Äù studies. The idea is that multiple measurements within the same subject will be compared.\nMixed models are popular tools for analyzing repeated measures data; we‚Äôll get to these later in the notes. For now, we‚Äôll consider the problems and opportunities that arise from the use of repeated measures data."
  },
  {
    "objectID": "Ch7_Mixed.html#the-independence-assumption",
    "href": "Ch7_Mixed.html#the-independence-assumption",
    "title": "7¬† Chapter 7: Mixed-effects models",
    "section": "7.2 The independence assumption",
    "text": "7.2 The independence assumption\nWe have looked at model assumptions in this class, e.g.\n\\[\n\\epsilon_i \\sim Normal(0, \\sigma)\n\\]\nThere is one that has been left out. The complete way to write this statement is:\n\\[\n\\epsilon_i \\overset{iid}\\sim Normal(0, \\sigma)\n\\]\nThe ‚Äúiid‚Äù means ‚Äúindependent and identically distributed.\n\\(\\epsilon_i \\overset{iid}\\sim Normal(0, \\sigma)\\) says that \\(\\epsilon_i\\) takes on values that are independently sampled from the same normal distribution.\n‚Äúindependently sampled‚Äù means that the value of the next data point is not dependent upon the value of the previous data point.\nEvery method we have used assumes independence of the response variable, conditional on the predictor(s).\nBut, if the same subjects are measured multiple times, then the multiple observations on each subject will not be independent.\nExample: suppose we take a random sample of 10 people, have each solve a maze, and record how much time it takes each person to solve it. Call this time \\(T\\).\nWe might model the times as i.i.d. normal:\n\\[\nT_i \\overset{iid}\\sim Normal( \\mu, \\sigma)\n\\]\nThis means that, given a mean and standard deviation, each observed time \\(T_i\\) is uncorrelated with any previous times. This should make sense: there is no reason that the \\(3^{rd}\\) person‚Äôs maze time should depend upon the \\(2^{nd}\\) person‚Äôs maze time.\nBut, if we take repeated measures on each subject, then our data will not be independent.\nFor instance, suppose we have 10 observations on maze completion from 2 people, where the first 5 come from one person and the last 5 come from the other.\nThen I would expect correlation (non-independence) between the first 5 maze times, and between the last 5 times. Observation 5 should be closer to observations 1-4 than it is to observations 6-10."
  },
  {
    "objectID": "Ch7_Mixed.html#cheating-with-repeated-measures",
    "href": "Ch7_Mixed.html#cheating-with-repeated-measures",
    "title": "7¬† Chapter 7: Mixed-effects models",
    "section": "7.3 Cheating with repeated measures",
    "text": "7.3 Cheating with repeated measures\nWe will look at a fake data simulation of repeated measures data.\nIn this simulation, suppose we have cows infected with the Staph bacteria.\nWe are comparing two treatments for Staph. We have 6 cows, and each treatment is randomly assigned to 3 cows. We will do a t-test to compare mean infection levels (quantified on some arbitrary scale).\nWe will also assume that the two treatments have an identical effect. Therefore the null hypothesis of no difference in means is true.\nThe file cows_ttest.jmp has some fake data, simulated as \\(y_i \\sim Normal(12,4)\\) for both groups:\n\n\n\n\n\n\n\n\n\n\nNot surprisingly, the t-test gives no significant difference in means:\n\\[\n95\\% \\textit{ CI for } \\mu_1 - \\mu_2 = (-18.00, 8.07), \\text{ p-value} = 0.321\n\\]\n\n\n\n\n\nNow suppose we measure infection level for each cow seven times after receiving treatment.\nThe file cows_repeated_ttest.csv contains data simulating this scenario.\n\n\n\n\n\nFor each cow, there are 7 observations. These all average out to the one observation per cow from the last data set.\n\n\n\n\n\nBut check out these t-test results!\n\n\n\n\n\nNotice where it says degrees of freedom = 29.2. If your degrees of freedom exceeds your sample size, something is wrong.\n\n7.3.1 What happened?\nComparing the two t-tests:\n\n\n\n\n\n\n\n\n\n\nThe mean difference is nearly the same for both.\nThe standard error for the repeated measures data is much smaller.\nRemember that the t-test is just a simple regression model:\n\\[\ny_i = \\beta_0 + \\beta_1Group_i + \\epsilon_i \\text{ where } \\epsilon_i \\overset{iid}\\sim Normal(0, \\sigma)\n\\]\nIn this case, \\(\\hat{\\beta}_1\\) is the estimated mean difference in infection levels between the two groups.\nThere‚Äôs nothing wrong with this estimate on its own. However its standard error is computing using an incorrect assumption: that all of the observations within each group are independent.\n\n\n7.3.2 Effective sample size\nIn this case, the apparent sample size is \\(n = 42\\), or \\(n = 21\\) per group.\nBut the ‚Äúeffective‚Äù sample size is \\(n = 6\\), or \\(n = 3\\) per group.\nTaking lots of observations from each animal and doing a t-test on all the data amounts to falsely inflating the sample size without having to actually collect more data.\nMain lesson: observations that are correlated should not be treated as independent!"
  },
  {
    "objectID": "Ch7_Mixed.html#cheating-ourselves-with-repeated-measures",
    "href": "Ch7_Mixed.html#cheating-ourselves-with-repeated-measures",
    "title": "7¬† Chapter 7: Mixed-effects models",
    "section": "7.4 Cheating ourselves with repeated measures",
    "text": "7.4 Cheating ourselves with repeated measures\nTreating repeated measures as independent can fool us into thinking we‚Äôve discovered an association when really there is none.\nIt can also do the opposite: it can fool us into thinking we see no association when really there is one.\nThis will be illustrated with another toy example. In this case, we will again consider collecting data on infection level in cows under two treatments. This time, we‚Äôll imagine collecting repeated observations over the course of a week.\nHere are the data in ‚Äúwide form‚Äù. Treatment group B is highlighted to distinguish it from treatment group A.\n\n\n\n\n\n\nTwo things to note:\n\nThere is a lot of variability across cows.\nEvery cow‚Äôs infection rating goes down over time\n\n\n\n\n\n\n\nHere are the data in ‚Äúlong form.‚Äù\n\n\n\n\n\nLong form is needed for most analyses.\nWe can use jamovi‚Äôs Rj-code editor to wide form to long form. (Though using Excel might be easier)\nThe following code will convert the data from wide to long form. Note you will need to open the data in a new session of jamovi and rename the Day and Infection_level columns.\n\n\n\n\n\n\n7.4.1 Converting from wide to long form\nWe must now transform the Day column of the longform data. We want the variable Day to take on nominal integer values\nDouble click the Day column and in the Data menu select Transform / Using Transform / Create New Transformation.\n\n\n\n\n\n\n\n7.4.2 Plotting the data\nHere is a basic plot of infection rates across time.\n\n\n\n\n\nThere appears to be a small downward trend, and lots of noise in the data.\nBut we can account for this noise! It is due to different cows having different overall infection rates.\nHere is a similar plot, with the Y axis grouped by treatment.\n\n\n\n\n\nThis shows a small decrease for treatment B.\nBut, we are still treating these observations as though they are independent.\nThey are not independent; they are correlated within each cow.\nHere is the same plot, but with ‚Äúcow_ID‚Äù as the overlay variable.\n\n\n\n\n\nEach cow now gets its own line. The downward trend for B is clear.\nAlso, it is now clear that most of the variation in infection rate was due to differences between cows. Once this is accounted for, variability is low.\n\n\n\n\n\n\n\n7.4.3 Analyzing the data\nThese plots illustrate the idea behind accounting for repeated measures.\nIn this module, we will learn how to incorporate repeated measures in statistical models.\nFor now, let‚Äôs look at some different way of analyzing the data that we just plotted.\n\n\n7.4.4 Analyzing the data, assuming independence\nWe can run regression models where infection rate is the response variable and some combination of treatment and day are the predictors.\nWe know both variables matter, and that they interact ‚Äì the ‚Äúeffect‚Äù of day on infection level depends on treatment, and the ‚Äúeffect‚Äù of treatment on infection level depends on day.\nThe next slide shows output for fitting this model:\n\\[\n\\textit{Infection level}_i = \\beta_0 + \\beta_1Treatment + \\beta_2Day + \\beta_3Treatment*Day + \\epsilon_i \\\\\n\\text{ where } \\epsilon_i \\overset{iid}\\sim Normal(0, \\sigma)\n\\]\n\n\n\n\n\n\\(R^2\\) is small, suggesting this model does not explain much variability in infection rates.\nRemember, we know that a lot of variability is due to differences between cows. This is all getting ‚Äúabsorbed‚Äù by the error term.\nLarge error variance gives small \\(R^2\\) and large standard errors for slopes\n\n\n7.4.5 Analyzing the data, accounting for cows\nHere are results from analyzing these data using a mixed model. This mixed model accounts for differences between cows, and the fact that repeated measures taken on each cow are correlated.\n\n\n\n\n\nWe will cover the details of how mixed models work in the next set of notes. For now, note the much larger \\(R^2 = 0.83\\)\n\n\n\n\n\n\n\n7.4.6 Side by side comparison\nThe mixed model (on the bottom) gives smaller standard errors for Day and the interaction.\n\nNote what didn‚Äôt change: the parameter estimates!"
  },
  {
    "objectID": "Ch7_Mixed.html#challenges-and-opportunities-with-repeated-measures-data",
    "href": "Ch7_Mixed.html#challenges-and-opportunities-with-repeated-measures-data",
    "title": "7¬† Chapter 7: Mixed-effects models",
    "section": "7.5 Challenges and opportunities with repeated measures data",
    "text": "7.5 Challenges and opportunities with repeated measures data\nRepeated measures data can be challenging to analyze, if the design gets complicated. We will see some study designs in which it isn‚Äôt immediately obvious how to set up an appropriate model.\nRepeated measures data can also be analyzed incorrectly (assuming independence) to artificially increase apparent sample size and get strong looking results that are not valid.\nBut repeated measures data can also be very useful! ‚ÄúWithin subject‚Äù designs, in which subjects are measured repeatedly across time and / or conditions, can greatly enhance the precision and power of our inferences. This is because variability that would normally be accounted for by the error term can instead be attributed to overall differences between the subjects from whom repeated measurements were taken."
  },
  {
    "objectID": "Ch7_Mixed.html#random-effects",
    "href": "Ch7_Mixed.html#random-effects",
    "title": "7¬† Chapter 7: Mixed-effects models",
    "section": "7.6 Random effects",
    "text": "7.6 Random effects\nThe ‚Äúmixed‚Äù in ‚Äúmixed models‚Äù refers to a mix of random effects and fixed effects.\nAll of the predictor variables we‚Äôve seen all semester have been ‚Äúfixed effects‚Äù. What this means will only make sense in comparison to a new kind of predictor: a ‚Äúrandom effect‚Äù.\nFixed effects variables are predictor variables for which we ‚Äúestimate‚Äù a slope coefficient, usually denoted with a \\(\\beta\\). We calculate a value for this coefficient using our data, and then we treat this value (usually denoted as a \\(\\hat{\\beta}\\)) as an estimate for some fixed but unknown population-level parameter.\nSometimes, though, we want our model to account for a variable that is important for explaining variability in the response variable, but for which we do not want to treat its coefficients as estimates of unknown parameters. For instance, in the previous ‚Äúcows‚Äù examples, we‚Äôll want to calculate the mean infection levels for each cow and take these into account. But, it probably doesn‚Äôt make sense to treat each cow‚Äôs mean as a population level parameter value of interest. After all, if we conducted the study again, we‚Äôd have new cows!\nTreating this predictor variable as a random effect (or random factor) will accomplish this. Random factors take on values that are treated as having been drawn at random from a larger population of possible values that might be different if we take a new sample. These random factors have coefficients (aka slopes) that are also treated as taking on random values.\nSo, with the ‚Äúcows‚Äù examples, we imagined measuring the same cows over and over again. ‚ÄúCow‚Äù should probably be treated as a random factor in these cases. The cows themselves were drawn from a larger population; we would not get the same cows again in a new study.\nAlso, we wanted to account for differences in the mean infection levels for each individual cow, so that we could estimate standard errors appropriate to our study designs. And these mean infection levels should also be treated as random, in that they came from some population of possible mean infection levels.\nSo, the cows are random, and their coefficients are random.\nThere are many interpretations of a ‚Äúrandom effect‚Äù, and they aren‚Äôt always helpful. From a 2005 paper by Andrew Gelman:\n\nFixed effects are constant across individuals, and random effects vary.\n\n\n\nEffects are fixed if they are interesting in themselves or random if there is interest in the underlying population.\nWhen a sample exhausts the population, the corresponding variable is fixed; when the sample is a small (i.e., negligible) part of the population the corresponding variable is random‚Äù.\n‚ÄúIf an effect is assumed to be a realized value of a random variable, it is called a random effect.‚Äù\n\nSo, interpretations of what random vs.¬†fixed effects ‚Äúreally mean‚Äù will vary.\nBut, formally, it isn‚Äôt so ambiguous. ‚ÄúFixed‚Äù effects are treated as having ‚Äúfixed‚Äù coefficients whose values we estimate and draw inference on (e.g.¬†with confidence intervals and hypothesis tests).\n‚ÄúRandom‚Äù effects are treated as having ‚Äúrandom‚Äù coefficients drawn from some distribution. We won‚Äôt estimate individual coefficients, but we will estimate the variance of the distribution from which they came.\n\n7.6.1 Subjects and Nesting\nOur main motivation right now is to have a method of accounting for repeated measurements on the same subjects.\nSo, in this class, we will look at mixed models for which the random effect is ‚Äúsubject‚Äù. In other words, we will make models that account for differences between subjects measured multiple times.\nIn a study in which subjects are assigned to groups, each subject is assigned to one group.\nFrom a modeling perspective, subject is ‚Äúnested‚Äù within group. In general, one variable is nested within the other if values of the nested variable only occur in certain categories of the variable it is nested within.\nIn this case, cows are nesting within treatments because each cow is measures only in one treatment. A non-nested design would have each cow measured under each value of the other predictors. In other words, each cow would be measured under both treatment.\nAnother example: say we are doing an education research study and we randomly sample 4 districts in a state, then 3 schools in each district, then 7 classrooms in each school. In this case, classroom is nested within school, and school is nested within district. This is because each classroom exists in only one school, and each school exists in only one district.\nThe notation for ‚Äúsubject nested within group‚Äù is \\(subject(group)\\)"
  },
  {
    "objectID": "Ch7_Mixed.html#the-mixed-effects-model",
    "href": "Ch7_Mixed.html#the-mixed-effects-model",
    "title": "7¬† Chapter 7: Mixed-effects models",
    "section": "7.7 The mixed effects model",
    "text": "7.7 The mixed effects model\nA mixed effects model contains both random and fixed effects.\nApplying this to the cows example, in which cows are assigned to one of two treatment groups and measured daily for seven days:\n\\[\n\\textit{Infection rate}_i = \\beta_0 + \\beta_1Treatment_i + \\beta_2Day_i + \\beta_3Treatment_i*Day_i + \\alpha_jCow_j(Treatment_i) + \\epsilon_i \\\\\n\\text{ where } \\alpha_j \\sim Normal(0, \\sigma^2_\\alpha) \\\\\n\\text{ and } \\epsilon_i \\sim Normal(0, \\sigma^2_\\epsilon)\n\\]\nDon‚Äôt worry too much about the notation details. The most important part is that ‚ÄúCow‚Äù is random, while ‚ÄúTreatment‚Äù and ‚ÄúDay‚Äù are fixed.\nWe will not estimate coefficients for the different cows. This model treats each cow as having its own ‚Äúrandom intercept‚Äù, meaning that the intercept, \\(\\beta_0\\), gets adjusted by some amount for each individual cow.\nAs we saw in the last set of notes, the purpose of this is to let the model estimate the effects of the ‚ÄúTreatment‚Äù and ‚ÄúDay‚Äù variables, while taking accounts of the fact that different cows will have different overall mean infection rates.\n\n7.7.1 The mixed effects model in jamovi\nTo fit this model in jamovi, use ‚ÄúLinear Models‚Äù, select ‚ÄúMixed Model‚Äù, and then add the fixed predictors as factors and covariates and random predictors as cluster variables:\n\n\n\n\n\nNote that Cow_ID is the random effect. Treatment, Day, and their interaction are fixed effects.\nNote also jamovi will also require you to specify the random effects.\n\n\n\n\n\njamovi gives the usual parameter estimates output, as well as a residual plot:\n\n\n\n\n\nNotice that ‚ÄúCow_ID‚Äù is not listed under parameter estimates; only fixed coefficients are estimated.\nEach cow has its own random coefficient, modeled as having been drawn from a normal distribution.\n\n\n\n\n\n\n\n7.7.2 Slope coding in ‚ÄúMixed Model‚Äù\nNotice the categorical predictor ‚ÄúTreatment‚Äù lists ‚ÄúEffect‚Äù as ‚ÄúA ‚Äì (B, A)‚Äù. jamovi is coding this slope as the difference between Treatment A and the ‚Äúmean‚Äù of Treatments A and B.\nSo, if we were looking at Treatment B, we‚Äôd apply a slope of -0.513."
  },
  {
    "objectID": "Ch7_Mixed.html#applied-example-the-liking-gap",
    "href": "Ch7_Mixed.html#applied-example-the-liking-gap",
    "title": "7¬† Chapter 7: Mixed-effects models",
    "section": "7.8 Applied example: ‚ÄúThe liking gap‚Äù",
    "text": "7.8 Applied example: ‚ÄúThe liking gap‚Äù\nThe journal Psychological Science published many studies in which data are publicly available.\nThe remaining slides reproduce results from a recent study published in Psychological Science on the difference between how much individuals ‚Äúlike‚Äù other people and how much they perceive others ‚Äúlike‚Äù them.\nThe paper is available at: https://doi.org/10.1177%2F0956797618783714\nThe basic setup is that volunteers were paired up (each pair is called a ‚Äúdyad‚Äù) and directed to have a conversation for five minutes.\nAfter this, participants rated their partners on some survey questions that the authors take as a measure of liking the other person.\nParticipants also rated how much they thought they were liked.\nThe study is looking for a ‚Äúgap‚Äù (i.e.¬†difference) between volunteers‚Äô self-perception of how much their partners liked them, and how much their partners actually liked them.\n\n7.8.1 The liking gap example\nFrom the results section of the paper:\n\n\n\n\n\n\n\n\n\n\n\n\n7.8.2 Fitting the model in jamovi\n\n\n\n\n\n\\[\n\\textit{Liking index}_i = \\beta_0 + \\beta_1\\textit{self_other} + \\beta_2Day + \\alpha_jpid_j(did_k) + \\gamma_kdid_k + \\epsilon_i \\\\\n\\text{ where } \\alpha_{j(k)} \\sim Normal(0, \\sigma^2_\\alpha), \\\\\n\\gamma_k \\sim Normal(0, \\sigma^2_\\gamma), \\\\\n\\text{ and } \\epsilon_i \\sim Normal(0, \\sigma^2_\\epsilon)\n\\]\nThe model statement starts looking complicated. We have two different random effects, one nested in the other.\nThe \\(\\alpha\\)‚Äôs and \\(\\gamma\\)‚Äôs are modeled as random values.\nWe have two fixed effects: self_other and Day. Their coefficients will be estimated as normal.\n\\[\n\\textit{Liking index}_i = \\beta_0 + \\beta_1\\textit{self_other} + \\alpha_jpid_j(did_k) + \\gamma_kdid_k + \\epsilon_i \\\\\n\\text{ where } \\alpha_{j(k)} \\sim Normal(0, \\sigma^2_\\alpha), \\\\\n\\gamma_k \\sim Normal(0, \\sigma^2_\\gamma), \\\\\n\\text{ and } \\epsilon_i \\sim Normal(0, \\sigma^2_\\epsilon)\n\\]\nThe random effect variances, \\(\\sigma^2_\\alpha\\) and \\(\\sigma^2_\\gamma\\), are estimated in the mixed model. These variances could be interpreted, but we will stick to interpreted the fixed effects in this class.\n\n\n7.8.3 Adding an interaction‚Ä¶\nThe next section of the paper looks at personality variables as predictors:\n\n\n\n\n\nIn jamovi:\n\n\n\n\n\nWe‚Äôll stop here. The paper goes on through many additional studies, and the data are all available via the supplemental materials link."
  },
  {
    "objectID": "Ch7_Mixed.html#replicating-a-published-mixed-model-analysis",
    "href": "Ch7_Mixed.html#replicating-a-published-mixed-model-analysis",
    "title": "7¬† Chapter 7: Mixed-effects models",
    "section": "7.9 Replicating a published mixed model analysis",
    "text": "7.9 Replicating a published mixed model analysis\nThe remainder of this chapter covers the analysis from a 2017 Psychological Science paper by Goudeau and Croizet, titled:\n‚ÄúHidden Advantages and Disadvantages of Social Class: How Classroom Settings Reproduce Social Inequality by Staging Unfair Comparison‚Äù\nThis paper presents three studies on one topic. These notes cover studies 1 and 3; study 2 is reserved for a homework assignment.\n\n7.9.1 Worked example overview\nThis study was performed in France. The data are available at https://osf.io/rkj7y/ and the data file has a codebook sheet that defines the variables:\n\n\n\n\n\nIn each study, 6th grade students are given a challenging reading comprehension assignment.\nPerformance on the assignment is the dependent variable.\nThe researchers investigate whether students‚Äô performance is associated with their awareness of their classmates‚Äô performance, and whether this association can be moderated by students‚Äô awareness of the different levels of preparation given to different students.\n\n\n7.9.2 Paper abstract\n\n\n\n\n\n\n\n7.9.3 Study 1\n\n\n\n\n\n\nThe variables used in this study are:\n\nPerformance (response)\nVisibility condition (fixed effect)\nSocial class (fixed effect)\nVisibility X Social class interaction (fixed effect)\nSchool (random effect)\nClassroom, nested within School (random effect)\n\n\n\n7.9.3.1 Study 1 model\n\\[\n\\textit{PERF}_i = \\beta_0 + \\beta_1X_1 + \\beta_2X_2 + \\beta_3X_1*PCS + \\alpha_{j(k)}CLASSE_{j(k)}(ETABLISSEMENT_k) + \\gamma_kETABLISSEMENT_k + \\epsilon_i \\\\\n\\text{ where } \\alpha_{j} \\sim Normal(0, \\sigma^2_\\alpha), \\\\\n\\gamma_k \\sim Normal(0, \\sigma^2_\\gamma), \\\\\n\\text{ and } \\epsilon_i \\sim Normal(0, \\sigma^2_\\epsilon)\n\\]\nRefer to the codebook for variable definitions\nNote that in this study, subjects (students) are only measured once. The repeated measurements are on classrooms and schools.\nAlso note that PCS compares working class (PCS=1) to upper class (PCS=3.)\n\n\n7.9.3.2 Study 1 in JMP\n\n\n\n\n\nNote that the response variable, PERF, is score on a reading comprehension test, on a scale of 0 to 20 points.\n\n\n7.9.3.3 Notes on Study 1 random effects output\n‚ÄúThe random effects covariance parameter estimates‚Äù table shows that there is much more residual (error) variance than there is variance across classrooms or across schools.\nOne very odd result: the classroom variance estimate is negative! But variances cannot be negative.\nIf we had deselected ‚Äúunbounded variance estimates‚Äù under Fit Model, this would be zeroed out. It is a strange quirk of maximum likelihood variance estimation that negative estimates are possible. We can ignore this, and treat the classroom variance as just very small.\n\n\n7.9.3.4 Notes on Study 1 fixed effects output\nWe see strong ‚Äúmain effects‚Äù for visibility condition (X1), social class (PCS), and their interaction.\nA general principle for models with strong interactions is that our interpretation should focus on the interactions.\n\nHere, the interaction coefficient is -2.69, and the coefficient for \\(X_1\\) is 1.12.\nFor the Effect of \\(X_1\\) ‚Äú-1 - 1‚Äù refers to the ‚Äúdifferences not visible‚Äù condition, in which students do not raise their hands when they have the answer. For the Effect of PCS ‚Äú3 - 1‚Äù refers to change in score for upper class students vs.¬†working class students. So, the ‚Äúeffect‚Äù of visibility seems to apply to working class students but not to upper class students, since ‚Äì2.69 and 1.12 nearly cancel each other out.\n\n\n7.9.3.5 Plotting the fixed effects\nHere is the plot reported in the paper, and the same plot in JMP:\n\n\n\n\n\nThe practice of using a bar plot to show means is common, but flawed. The bars don‚Äôt mean anything; all they do is go up to the means.\nHere is what this looks like using boxplots instead.\n\n\n\n\n\nNotice that this plot shows variability in the data, where the bar plot does not.\n\n\n\n7.9.4 Study 3\nIn Study 2, social class is not used. Rather, some students are given better preparation for the reading comprehension test than others.\nSimilar results are found: those with worse preparation perform more poorly when students are told to raise their hands after determining the answer.\nIn Study 3, the authors attempt to ‚Äúundo‚Äù this effect by informing the class that some students were given better preparation than others. Half the classrooms are made aware of this; the other half are not.\nThe variables in this study are almost the same as in study 1, with these changes:\n\nX1 is ‚Äúcontext‚Äù:\n\n-1 = awareness of the disadvantage\n1 = unawareness of the disadvantage\n\nX2 is ‚Äúlevel of familiarity‚Äù with the reading material, based on intentional preparation given by the researchers:\n\n-1 = high level\n1 = low level\n\n\n\n7.9.4.1 Study 3 model\n\\[\n\\textit{PERF}_i = \\beta_0 + \\beta_1X_1 + \\beta_2X_2 + \\beta_3X_1*X_2 + \\alpha_{j}CLASSE_{j}+ \\epsilon_i \\\\\n\\text{ where } \\alpha_{j} \\sim Normal(0, \\sigma^2_\\alpha), \\\\\n\\text{ and } \\epsilon_i \\sim Normal(0, \\sigma^2_\\epsilon)\n\\]\nIn this data set, there is no variable for ‚Äúschool‚Äù, only one for ‚Äúclassroom‚Äù. This isn‚Äôt explained in the paper.\n\n\n7.9.4.2 Study 3 in jamovi\n\n\n\n\n\n\n\n\n\n\nAs with before, the variance across classrooms is very small relative to the error variance.\nThe largest overall effect is \\(X_2\\): level of familiarity with the material. Students who are better prepared score higher.\nThe negative interaction shows that the difference in performance between those with higher vs.¬†low familiarity is lower when students are aware of the difference in familiarity.\nThe interaction of -5.81 just about cancels out the estimated slope for \\(X_1\\)[-1] of 4.24.\nThis shows that there is very little difference in scores between students with high preparation who are and are not aware of the advantage.\nIt helps to look at the data.\n\n\n\n\n\nThe generic ‚Äú\\(X_1\\)‚Äù and ‚Äú\\(X_2\\)‚Äù have been replaced with more meaningful titles.\nAnd here are the bar plots shows in the paper.\n\n\n\n\n\nAgain, the boxplots show more information.\nIn this case, the boxplots reveal a potentially concerned ‚Äúceiling effect‚Äù: many students earned the maximum possible score. This can‚Äôt be seen from the bar plot."
  },
  {
    "objectID": "Ch8_Mind_the_Gap.html#outline-of-notes",
    "href": "Ch8_Mind_the_Gap.html#outline-of-notes",
    "title": "8¬† Chapter 8: Minding the gap between science and statistics",
    "section": "Outline of notes",
    "text": "Outline of notes\n\nThe ‚Äúreplication crisis‚Äù\nPower\nSome history on ‚ÄúNHST‚Äù\nWhen ‚Äúp &lt; 0.05‚Äù isn‚Äôt that meaningful\nWhat kind of Type I error are you referring to?\nWhat do people think small p-values mean?\nAssessing bias using meta-analysis\nType ‚ÄúM‚Äù errors\nShould we throw away non-significant results?\nDebates over statistical significance\nProposed reforms"
  },
  {
    "objectID": "Ch8_Mind_the_Gap.html#the-replication-crisis",
    "href": "Ch8_Mind_the_Gap.html#the-replication-crisis",
    "title": "8¬† Chapter 8: Minding the gap between science and statistics",
    "section": "8.1 The ‚Äúreplication crisis‚Äù",
    "text": "8.1 The ‚Äúreplication crisis‚Äù\n‚ÄúThe replication crisis‚Äù refers to a recent realization in many areas of science that previously published results often fail to replicate.\nArguably, this started with Daryl Bem‚Äôs 2011 paper ‚ÄúFeeling the Future‚Äù, published in the Journal of Personality and Social Psychology.\nThis paper used standard statistical tools to show strong evidence for pre-cognition, a.k.a. ESP. Many scientists were bothered by this, because they did not believe in ESP but they did believe in the statistical methods used in this paper!\n\n\n\n\n\n2015‚Äôs ‚ÄúReproducibility Project: Psychology‚Äù by the Center for Open Science found that a large number of published experimental results in top Psychology journals failed to replicate.\nSimilar studies in Cancer Biology, Medicine, Economics, Marketing, and Sports Science have found high rates of non-replication.\nWhy do so many studies fail to replicate? There are many reasons, and statistical analysis plays a prominent role. These notes cover the role of statistics in the replication crisis."
  },
  {
    "objectID": "Ch8_Mind_the_Gap.html#power",
    "href": "Ch8_Mind_the_Gap.html#power",
    "title": "8¬† Chapter 8: Minding the gap between science and statistics",
    "section": "8.2 Power",
    "text": "8.2 Power\nMany of the statistical issues surrounding the replication crisis concern ‚Äúpower‚Äù.\nStatistical power is the probability of rejecting a null hypothesis (\\(H_0\\)), in the case that \\(H_0\\) is false.\nIn terms of real-world effects, if power is high, then there is a good chance of ‚Äúdetecting‚Äù an effect ‚Äì i.e.¬†of declaring statistical significance.\nIf power is low, then even if a real-world effect exists, the result of a hypothesis test will likely be ‚Äúfail to reject‚Äù \\(H_0\\); i.e.¬†non-significance.\n\n8.2.1 Type I and Type II errors\nA ‚ÄúType I‚Äù error occurs when a true \\(H_0\\) is rejected. In other words, if we declare a result ‚Äústatistically significant‚Äù even though no real-world effect exists, we are committing a Type I error. This is sometimes called a ‚Äúfalse positive‚Äù outcome.\nA ‚ÄúType II‚Äù error occurs when a false \\(H_0\\) is not rejected. In other words, if we declare a result ‚Äúnot significant‚Äù even though a real-world effect does exist, we are committing a Type II error. This is sometimes called a ‚Äúfalse negative‚Äù outcome.\nLow statistical power implies a high chance of a false negative.\n\n\n8.2.2 Problem: power is unknown!\nThe ‚Äútrue‚Äù power of a statistical test is almost never known. To calculate power, one must assume the ‚Äútrue‚Äù size of the effect being studied.\nFor instance, power to reject the null hypothesis that a new drug is equally effective as a previous drug depends in part upon how different the two drugs are in effectiveness. But if we knew that, we wouldn‚Äôt need to conduct a study!\nThere has been research estimating average statistical power in various research fields, and reason to believe that low power studies are not uncommon. Which means that a lot of these replication failures might be ‚Äúfalse negatives‚Äù, rather than the original studies being ‚Äúfalse positives‚Äù."
  },
  {
    "objectID": "Ch8_Mind_the_Gap.html#some-history-on-nhst",
    "href": "Ch8_Mind_the_Gap.html#some-history-on-nhst",
    "title": "8¬† Chapter 8: Minding the gap between science and statistics",
    "section": "8.3 Some history on ‚ÄúNHST‚Äù",
    "text": "8.3 Some history on ‚ÄúNHST‚Äù\nThe form of hypothesis testing used today is sometimes called Null Hypothesis Significance Testing (NHST). It combines what used to be two different methods created by two ‚Äúcamps‚Äù. The camps disagreed with one another, and did not get along personally.\nRonald Fisher\n\n\n\n\n\nJerzy Neyman and Egon Pearson\n\n\n\n\n\n\n8.3.1 Ronald Fisher\nIntroduced the null hypothesis and p-value\nOn interpreting small p-values: ‚ÄúEither an exceptionally rare chance has occurred, or the theory of random distribution is not true‚Äù\nOn the use of p &lt; 0.05 as a standard of evidence: ‚ÄúIn order to assert that a natural phenomenon is experimentally demonstrable we need, not an isolated record, but a reliable method of procedure. In relation to the test of significance, we may say that a phenomenon is experimentally demonstrable when we know how to conduct an experiment which will rarely fail to give us a statistically significant result‚Äù\n\n\n8.3.2 Neyman and Pearson\nIntroduced the alternative hypothesis, Type I and II errors, and power.\nReported only Type I and Type II error probabilities in testing; e.g.¬†p-values of 0.001 and 0.04 would both be reported as falling under a pre-specified \\(\\alpha = 0.05\\) Type I error rate. A Type II error rate should also be reported.\n‚ÄúWe are inclined to think that as far as a particular hypothesis is concerned, no test based upon a theory of probability can by itself provide any valuable evidence of the truth or falsehood of a hypothesis‚Äù\n\n\n8.3.3 NHST\nNHST is a mix between the two approaches.\nHypothesis testing comes from Neyman and Pearson. They did not believe p-values should be interpreted directly as quantifying evidence. They saw their procedure as simply a method for making a decision.\nThe direct interpretation of p-values as quantifying the probability of obtaining results at least as extreme, assuming the null hypothesis is true, comes from Fisher. He did not use an alternative hypothesis, and he did not accept formal power analysis."
  },
  {
    "objectID": "Ch8_Mind_the_Gap.html#when-p-0.05-isnt-that-meaningful",
    "href": "Ch8_Mind_the_Gap.html#when-p-0.05-isnt-that-meaningful",
    "title": "8¬† Chapter 8: Minding the gap between science and statistics",
    "section": "8.4 When ‚Äúp < 0.05‚Äù isn‚Äôt that meaningful",
    "text": "8.4 When ‚Äúp &lt; 0.05‚Äù isn‚Äôt that meaningful\nWe say results are ‚Äústatistically significant‚Äù when we calculate a p-value less than the \\(\\alpha\\) level of significance, which is commonly 0.05.\nSo, ‚Äúsignificant‚Äù results are the kind that would be unlikely to occur by chance, if the null hypothesis were true.\nThere are some issues here‚Ä¶\nThe reason small p-values are considered ‚Äúevidence‚Äù for a research hypothesis is that they are supposed to be unlikely to occur by chance alone.\nIf there is flexibility in how to conduct an analysis, then it becomes more likely that small p-values will occur by chance.\nThis has a derogatory name: ‚ÄúP-hacking‚Äù.\nLess derogatory alternatives: ‚ÄúResearcher degrees of freedom‚Äù, ‚ÄúThe garden of forking paths‚Äù\n\nExamples of flexibility:\n\n-   Trying out different combinations of independent and dependent variables\n\n-   Trying out different models and testing methods\n\n-   Redefining variables (e.g. averaging over different combinations of survey responses, choosing different cut points for placing responses into categories)\n\n-   Discarding / retaining outliers\n\n-   Transforming variables\n\n-   Collecting more data than originally planned\n\n-   Ceasing data collection earlier than originally planned\n\n8.4.1 Capitalizing upon flexibility, so that \\(P(p &lt; 0.05) &gt; 0.05\\)\nFlexible practices are perfectly justifiable in many contexts\nBut ‚Äì to interpret a p-value as ‚Äúthe probability of obtaining results at least this extreme, assuming the null is true‚Äù, there can be no flexibility, unless accounted for using a correction (e.g.¬†Bonferroni). Flexibility renders the classical p-value interpretation invalid.\nAnother way of putting it: if you report a p-value as ‚Äúthe probability of obtaining results this extreme, assuming the null is true‚Äù, you are implicitly claiming you would have conducted the identical analysis, even if the data had been different in any way.\n\n\n8.4.2 The null could be false due to something you haven‚Äôt thought of\nPerhaps ‚Äú\\(H_0\\) is false‚Äù does not imply that the version of ‚Äúnot \\(H_0\\)‚Äù you have in mind is true.\nIn observational studies, we must always think about possible confounders. Maybe there is a ‚Äúsignificant association‚Äù between variables for some reason that isn‚Äôt being considered.\nIn experimental studies, we must consider all the possible effects of our interventions. Did the experiment produce ‚Äúsignificant‚Äù results for a reason we didn‚Äôt consider, e.g.¬†poor control, biased question wording?\nExample: CSU has found that students who complete their Math and Composition AUCC requirements during their first year earn higher grades and have higher graduation rates than those who do not.\nDoes this suggest that completing these requirements in the first year results in higher grades and a higher chance of graduation?\nCould this ‚Äústatistically significant‚Äù effect be due to confounding factors? Is it even reasonable to suspect that students who complete these classes during their first year should have identical grades and graduation rates as those who don‚Äôt? This is what the rejected \\(H_0\\) states. Are we impressed that it is rejected?\n\n\n8.4.3 Do we even think the null could be true?\n‚ÄúAll we know about the world teaches us that the effects of A and B are always different ‚Äì in some decimal place ‚Äì for any A and B. Thus asking ‚ÄòAre the effects different?‚Äô is foolish‚Äù - John Tukey\nThe null can be used as a ‚ÄúStraw Man‚Äù that no one really believes. Overturning a straw man null may not be that impressive.\nA good question to ask: ‚Äúwould we expect this null to be true?‚Äù\nIf we are performing an experiment, the null is that the treatment does literally nothing. Is this common?\nIf we are analyzing observational data and testing for ‚Äúsignificant correlation‚Äù, the null is that there is precisely zero correlation at the population level. Do we think this is possible?\nPsychology/Philosopher/Statistician Paul Meehl called this the ‚Äúcrud factor‚Äù: the extent to which everything is correlated with everything, at least at some small level.\nBut, there are times when the null is plausible.\nIn manufacturing quality control analysis, the null is that ‚Äúeverything is being produced the normal way‚Äù. Defects show up as large deviations from this observed null distribution.\nIn Ronald Fisher‚Äôs ‚ÄúThe Lady Tasting Tea‚Äù, the subject of the story claimed she could tell whether milk or tea had been poured into a cup first, simply by tasting the result. The null is that she couldn‚Äôt tell. Seems plausible.\nMy personal rule is that I am only interested in p-values when I think the null is plausible.\n\n\n8.4.4 Is there a decision to be made?\nA significance test returns a binary outcome: the results are significant, or they are non-significant.\nBinary outcomes can produce binary thinking: the temptation to think ‚Äúsignificant‚Äù means ‚Äúreal‚Äù and ‚Äúnon-significant‚Äù means ‚Äúdue to chance‚Äù.\nIs coming to a binary choice even necessary? Why not just report a point estimate and standard error or 95% CI?\nIf there is an actionable outcome, a binary choice might be necessary.\nExample: deciding whether to continue investing money into a research program.\nIf the analysis is being done for the purpose of decision making, then a decision rule must be established, and ‚Äúsignificance‚Äù can be such a rule. If the analysis is being done for the purpose of conveying information in data, I personally see no reason to add in a declaration of ‚Äúsignificant‚Äù or ‚Äúnot significant‚Äù.\n\n\n8.4.5 Big sample sizes give small p-values\nConsider the t-test statistic:\n\\[\nt = \\frac{\\bar{x}_1 - \\bar{x}_2}{\\textit{std. error of }\\bar{x}_1 - \\bar{x}_2}\n\\]\nAs sample size increases, standard error decreases, the t-test statistic increases, and the p-value decreases.\nSo, the bigger the sample size, the smaller the value of \\(\\bar{x}_1 - \\bar{x}_2\\) that is needed to achieve statistical significance.\nUpshot: if \\(n\\) is larger, very small effects will be ‚Äúsignificant‚Äù.\n\n\n8.4.6 eBay study example\n\n\n\n\n\n‚ÄúWomen had a slightly higher percentage of transactions for which positive feedback had been given in the year preceding the current transaction (99.60% for women and 99.58% for men, P &lt; 0.05)‚Äù\n(http://advances.sciencemag.org/content/2/2/e1500599.full)"
  },
  {
    "objectID": "Ch8_Mind_the_Gap.html#big-sample-sizes-give-small-p-values",
    "href": "Ch8_Mind_the_Gap.html#big-sample-sizes-give-small-p-values",
    "title": "8¬† Chapter 8: Minding the gap between science and statistics",
    "section": "8.5 Big sample sizes give small p-values",
    "text": "8.5 Big sample sizes give small p-values\nConsider the t-test statistic:\n\\[\nt = \\frac{\\bar{x}_1 - \\bar{x}_2}{\\textit{std. error of }\\bar{x}_1 - \\bar{x}_2}\n\\]\nAs sample size increases, standard error decreases, the t-test statistic increases, and the p-value decreases.\nSo, the bigger the sample size, the smaller the value of \\(\\bar{x}_1 - \\bar{x}_2\\) that is needed to achieve statistical significance.\nUpshot: if \\(n\\) is larger, very small effects will be ‚Äúsignificant‚Äù.\n\n8.5.1 eBay study example\n\n\n\n\n\n‚ÄúWomen had a slightly higher percentage of transactions for which positive feedback had been given in the year preceding the current transaction (99.60% for women and 99.58% for men, P &lt; 0.05)‚Äù\n(http://advances.sciencemag.org/content/2/2/e1500599.full)"
  },
  {
    "objectID": "Ch8_Mind_the_Gap.html#what-kind-of-type-i-error-are-you-referring-to",
    "href": "Ch8_Mind_the_Gap.html#what-kind-of-type-i-error-are-you-referring-to",
    "title": "8¬† Chapter 8: Minding the gap between science and statistics",
    "section": "8.5 What kind of Type I error are you referring to?",
    "text": "8.5 What kind of Type I error are you referring to?\nGoing back to Type I errors, here is the ‚ÄúType I error rate‚Äù:\n\\[\nP(\\textit{Reject } H_0|H_0\\textit{ is true})\n\\]\nWhat if we‚Äôre actually interested in the reverse?\n\\[\nP(H_0\\textit{ is true}|\\textit{Reject } H_0)\n\\]\nClassical hypothesis testing controls the probability of rejecting \\(H_0\\), given \\(H_0\\) is false, typically at 5%.\nIt says nothing about the probability \\(H_0\\) is false, given that \\(H_0\\) has been rejected. \\(P(\\textit{reject } H_0|H_0\\textit{ is false}) \\neq P(H_0\\textit{ is false}|\\textit{reject } H_0)\\)\nUpshot: don‚Äôt imagine that a Type I error rate of 0.05 implies that only 5% of significant results you see should be Type I errors!"
  },
  {
    "objectID": "Ch8_Mind_the_Gap.html#what-do-people-think-small-p-values-mean",
    "href": "Ch8_Mind_the_Gap.html#what-do-people-think-small-p-values-mean",
    "title": "8¬† Chapter 8: Minding the gap between science and statistics",
    "section": "8.6 What do people think small p-values mean?",
    "text": "8.6 What do people think small p-values mean?\nJacob Cohen:\nWhat‚Äôs wrong with NHST? Well, among many other things, it does not tell us what we want to know, and we so much want to know what we want to know that, out of desperation, we nevertheless believe that it does! What we want to know is ‚ÄúGiven these data, what is the probability that \\(H_0\\) is true?‚Äù But as most of us know, what it tells us is ‚ÄúGiven that \\(H_0\\) is true, what is the probability of these (or more extreme) data?‚Äú - The Earth is Round (p&lt;0.05) (1994)\n‚ÄúStatistical tests, P values, confidence intervals, and power: a guide to misinterpretations‚Äù (2016) lists a large number of popular misinterpretations. Some highlights‚Ä¶\n‚ÄúThe p-value is the probability that the test hypothesis is true; for example, if a test of the null hypothesis gave P = 0.01, the null hypothesis has only a 1 % chance of being true; if instead it gave P = 0.40, the null hypothesis has a 40 % chance of being true.‚Äù\n‚ÄúThe p-value for the null hypothesis is the probability that chance alone produced the observed association‚Äù\n‚ÄúA null-hypothesis p-value greater than 0.05 means that no effect was observed, or that absence of an effect was shown or demonstrated.‚Äù\n‚ÄúStatistical significance is a property of the phenomenon being studied, and thus statistical tests detect significance.‚Äù\n‚ÄúWhen the same hypothesis is tested in two different populations and the resulting p-values are on opposite sides of 0.05, the results are conflicting.‚Äù\n(note: all of these are wrong)"
  },
  {
    "objectID": "Ch8_Mind_the_Gap.html#assessing-bias-using-meta-analysis",
    "href": "Ch8_Mind_the_Gap.html#assessing-bias-using-meta-analysis",
    "title": "8¬† Chapter 8: Minding the gap between science and statistics",
    "section": "8.7 Assessing bias using meta-analysis",
    "text": "8.7 Assessing bias using meta-analysis\n\n8.7.1 The sampling distribution of the p-value\nIt is intuitive to think that larger p-values are more likely to occur than smaller p-values when the null is true.\nThis is intuitive, but it is false. In most testing scenarios, all p-values are equally likely when the null is true. The distribution of p under \\(H_0\\) is uniform:\n\n\n\n\n\nSimulation: https://csu-statistics.shinyapps.io/visualize_power/\nWhen the null is false, the distribution of p is right skewed. The greater the statistical power, the greater the skew.\nHere is the distribution of p for Cohen‚Äôs d = 0.5 and n = 30, implying power \\(\\approx\\) 75%\n\n\n\n\n\nWhen null is false, smaller p-values are more likely than larger ones.\nSome people have the intuition that for weaker effects or lower power, significant p-values should be close to 0.05.\nThis is false; even for low power, very small p-values are more likely than ‚Äúmarginally significant‚Äù p-values. Here is the distribution of p for Cohen‚Äôs d = 0.2 and n = 30, impying power \\(\\approx\\) 19%\n\n\n\n\n\nUpshot: we should never see lots of p-values just below 0.05, even under low power.\nBut ‚Äì there are papers in which many studies are performed, all of which produce p-values just below 0.05. There are bodies of published research in which p- values just below 0.05 occur too frequently.\nThis suggests some combination of ‚Äúpublication bias‚Äù and ‚Äúp-hacking‚Äù\nFormal test: ‚ÄúP-curve‚Äù (www.p-curve.com)\n‚ÄúP-curve‚Äù takes a collection of p-values less than 0.05 and compares them to the uniform distribution expected under \\(H_0\\)\n\n\n\n\n\nIf p-values close to 0.05 occur too frequently, the p-curve is consistent with a true \\(H_0\\), despite all p-values being less than 0.05.\n\n\n8.7.2 What N is required for sufficient power to detect obvious effects?\n\nSimmons, Simonsohn, and Leif conducted an Amazon M-Turk study (n = 697) to estimate required sample size to detect a variety of ‚Äúeffects‚Äù at \\(\\alpha = 0.05\\). Examples:\n\nMen are taller than women (n = 12; i.e.¬†n = 6 per group) People who like spicy food eat more Indian food than people who don‚Äôt like spicy food (n = 52)\nPeople who like eggs eat more egg salad than people who don‚Äôt like eggs (n = 96)\nSmokers think smoking is less likely to kill someone than do non-smokers (n = 288)\nMen weigh more than women (n = 92) (!!!!!!!!!!!)\n\n\n\n\n8.7.3 More on publication bias\n‚ÄúPublication bias‚Äù is the phenomenon by which statistically significant results are more likely to be published in a journal than results that are not statistically significant.\nThere are many tools for trying to diagnose this (including P-curve)\nTest of Insufficient Variance: convert p-values to z-statistics. Expected variance of z-statistics is 1. Variance less than 1 could suggest ‚Äúmissing‚Äù studies.\n\n\n8.7.4 Funnel plot\nPlot effect size vs.¬†standard error. Variance in effect sizes should decrease as standard error decreases, but effect size and standard error should not be correlated.\n\n\n\n\n\nCorrelation could suggest ‚Äúmissing‚Äù studies.\n\n\n8.7.5 Inter-ocular trauma test\n‚ÄúDo the results hit you between the eyes?‚Äù\n\n\n\n\n\n(z = 1.96 is the threshold for statistical significance)"
  },
  {
    "objectID": "Ch8_Mind_the_Gap.html#type-m-errors",
    "href": "Ch8_Mind_the_Gap.html#type-m-errors",
    "title": "8¬† Chapter 8: Minding the gap between science and statistics",
    "section": "8.8 Type ‚ÄúM‚Äù errors",
    "text": "8.8 Type ‚ÄúM‚Äù errors\nDespite the fact that large sample sizes are needed to detect what seem like ‚Äúmedium‚Äù effects, we see lots of studies will small sample sizes reporting significant effects.\nAlso, significant effects from small sample sizes are always large.\nA likely culprit: the combination of publication bias and low power.\n\nLow power: a typical consequence of‚Ä¶\n\nSmall sample sizes\nSmall effects\nNoisy or imprecise measurements\nNoisy or imprecise manipulations\n\n\nA nasty consequence of publication bias combined with low power: published effect sizes are biased upward. The lower the power, the greater the bias.\nA diagram of low power:\n\n\n\n\n\nNotice: ‚Äúthe truth‚Äù is NOT in the ‚Äúreject the null‚Äù region. So when power is low, ‚Äúthe truth‚Äù is not statistically significant!\n\n8.8.1 Visually, using confidence intervals\n\n\n\n\n\nLow power =&gt; wide CIs\nHere, wide CI centered on true effect =&gt; not significant\nFor CI to exclude zero, sample effect must greatly overestimate true effect\n\n\n8.8.2 Visually, using sampling distributions\nBelow: Sampling distribution of the Cohen‚Äôs d statistic (standardized diff. in means). Simulated so that power = 0.27 and population d = 0.5.\n\n\n\n\n\nBelow: same thing, but after removing all d statistics that do not achieve statistical significance.\n\n\n\n\n\nUpshot: conditioning an unbiased estimator on p &lt; 0.05 creates a biased estimator. The lower the power, the greater the bias.\n\n\n8.8.3 The extreme example\n\n\n\n\n\n\n\n8.8.4 Can low power be assessed empirically?\n‚ÄúPost-hoc‚Äù power analysis describes performing a power analysis on a data set after having conducted a significance test.\nSometimes researchers will get a non-significant result, and suspect low power is the reason. So, they do a power analysis using the effect size and standard error from the data, and find low power.\nTHIS IS INVALID! If p &gt; 0.05, then post-hoc power must be lower than 50%.\nSo, ‚Äúnon-significant‚Äù results will always be identified as ‚Äúlow power‚Äù, post-hoc.\n\n\n8.8.5 Estimating power meta-analytically: R-Index\nR-Index: calculate observed power for each study. Compare average power to proportion of studies showing significance. Lower observed power could suggest missing studies.\ne.g.¬†observed power = 0.6, 10 / 10 results significant; 6 / 10 expected if power = 0.6.\nIf a collection of studies all show significant results, then average observed power must be greater than 50%. But, it might not be much greater."
  },
  {
    "objectID": "Ch8_Mind_the_Gap.html#should-we-throw-away-non-significant-results",
    "href": "Ch8_Mind_the_Gap.html#should-we-throw-away-non-significant-results",
    "title": "8¬† Chapter 8: Minding the gap between science and statistics",
    "section": "8.9 Should we throw away non-significant results?",
    "text": "8.9 Should we throw away non-significant results?\nThere are statistical problems that arise when only reporting significant results.\nThere is also a scientific problem: are non-significant results really uninteresting?\nIf the question is worth asking (‚Äúdo I have evidence for this substantive hypothesis?‚Äù), isn‚Äôt the answer worth knowing?\nThe practice of throwing away non-significant results goes hand in hand with a false interpretation of NHST results: that non-significance implies ‚Äúno effect‚Äù.\np-value &lt; 0.05 is commonly interpreted as evidence for an effect.\nBut, p-value &gt; 0.05 should not necessarily be interpreted as evidence for no effect.\n\n8.9.1 ‚ÄúThere was no effect (p&gt;0.05)‚Äù\nExample: suppose three studies (A, B, and C) all aimed to estimate a standardized difference in means in terms of Cohen‚Äôs d:\n\\[\nd= \\frac{\\bar{x}_1 - \\bar{x}_2}{s_p}\n\\]\nSuppose that for each study, a p-value is also computed, testing against the null of \\(H_0: \\mu_1-\\mu_2 = 0\\)\nA 95% CI for d is also constructed.\nIn this example, all CIs contain zero, so all p-values exceed 0.05. All three tests are consistent with ‚Äúno effect‚Äù.\n\n\n\n\n\nHowever, the first CI is also consistent with a very large effect, which could be positive or negative. The third CI is consistent with no effect or a very small effect .\nBut - the p-values are the same! In all three cases, p = 0.51.\n\n\n8.9.2 Published example: knee surgery study\n\n\n\n\n\nThe article ‚ÄúArthroscopic partial meniscectomy versus placebo surgery for a degenerative meniscus tear: a 2-year follow-up of the randomized controlled trial‚Äù assesses the effectiveness of a surgical procedure for treating a degenerative knee tear relative to a sham ‚Äúplacebo‚Äù surgery (!!!). https://ard.bmj.com/content/77/2/188\nThe article finds a non-significant difference between treatment and placebo, and interprets this as the treatment being ‚Äúno better‚Äù than placebo. Author conclude there is ‚Äúno evidence‚Äù in favor of the treatment.\nHowever, later in the paper the authors make a much stronger argument: that the 95% CI for the difference in means is fully below the minimum clinically meaningful difference, which they established a priori:\n\n\n\n\n\n\nNote the sharp difference between these arguments:\n\n‚ÄúSurgery was not effective because the difference between surgery and placebo was not statistically significant.‚Äù\n‚ÄúSurgery was not effective because the 95% CI for the difference between surgery and placebo fell entirely below the minimum clinically significant difference.\n\n\nThe first argument says ‚Äúthe estimated difference is no bigger than what would be expected by chance.‚Äù The second says ‚Äúthe largest plausible value for the difference is still too small to be interesting.‚Äù"
  },
  {
    "objectID": "Ch8_Mind_the_Gap.html#debates-over-statistical-signficance",
    "href": "Ch8_Mind_the_Gap.html#debates-over-statistical-signficance",
    "title": "8¬† Chapter 8: Minding the gap between science and statistics",
    "section": "8.10 Debates over statistical signficance",
    "text": "8.10 Debates over statistical signficance\nThe use of statistical significance has always been controversial.\nThree recent high profile papers have argued for some different viewpoints on statistical significance.\n\n8.10.1 ‚ÄúRedefine statistical significance‚Äù\n‚ÄúRedefine Statistical Significance‚Äù (2017) calls for lowering the standard threshold for significance to p &lt; 0.005\nThe argument: p &lt; 0.05 is too easy to obtain from noise.\nThis paper proposes labeling p-values between 0.005 and 0.05 as ‚Äúsuggestive‚Äù, and p-values less than 0.005 as ‚Äúsignificant‚Äù.\nAn exception: if the procedure is pre-registered, p &lt; 0.05 can be labeled ‚Äúsignificant‚Äù. So a distinction is drawn between exploratory and confirmatory data analyses.\n\n\n8.10.2 ‚ÄúJustify your alpha‚Äù\n‚ÄúJustify Your Alpha‚Äù (2017), written in response, calls for allowing flexibility in alpha levels rather than defaulting to p &lt; 0.05.\nThe argument: alpha (a.k.a. the significance level) sets a trade-off between Type I errors and Type II errors.\nSmaller values of alpha lower Type I error rates but increase Type II error rates, and vice versa.\nThe optimal trade-off will be different for different research fields. FDA drug trials should not use the same trade-off as exploratory research in brand new research fields.\n\n\n8.10.3 ‚ÄúAbandon statistical significance‚Äù\n‚ÄúAbandon Statistical Significance‚Äù (2017) calls for the elimination of thresholds entirely.\nThe argument: ‚Äúsignificance‚Äù is just a way of taking continuous phenomena (e.g.¬†differences in means, probabilities, correlations) and forcing them into one of two categories.\nInstead, why not report the evidence on its own terms? No need to force it into an artificial and simplistic ‚Äúeither / or‚Äù distinction."
  },
  {
    "objectID": "Ch8_Mind_the_Gap.html#proposed-reforms",
    "href": "Ch8_Mind_the_Gap.html#proposed-reforms",
    "title": "8¬† Chapter 8: Minding the gap between science and statistics",
    "section": "8.11 Proposed reforms",
    "text": "8.11 Proposed reforms\n\n8.11.1 Some proposals for doing things differently\nPre-registration and registered reports: data analysis plans are stated ahead of time. This removes flexibility in analysis.\nWith registered reports, data analysis plans are peer reviewed, and papers can be accepted for publication before results are known. This removes publication bias.\n\n\n\n\n\nRewarding ‚Äúopen‚Äù practices.\nThe Association for Psychological Science now does this using badges:\n\n\n\n\n\n‚ÄúEquivalence testing‚Äù: an alternative to ‚Äúaccepting‚Äù a null that has not been rejected.\nIdea: establish a minimum effect size of interest (e.g.¬†‚Äúwe‚Äôre not interested in this drug if it doesn‚Äôt reduce blood pressure by at least‚Ä¶‚Äù)\nMake the null of the equivalence test be that the true effect is smaller than the minimum effect size of interest.\nIf the null is rejected, then observed results are ‚Äúequivalent‚Äù to the null insofar as they are too small to be interesting.\nVisualization of equivalence testing, using confidence intervals:\n\n\n\n\n\nNote that results can be both ‚Äúnot significantly different‚Äù and ‚Äúnot significantly equivalent‚Äù.\nThey can also be both ‚Äúsignificantly equivalent‚Äù and ‚Äúsignificantly different‚Äù!\n‚ÄúThe New Statistics‚Äù proposes that we emphasize confidence intervals over p-values, as they are easier to understand and less noisy (i.e.¬†they don‚Äôt change as much across repeated samples)\nThe Peer Reviewers‚Äô Openness Initiative calls on reviewers to require open data, open methods, and code that will reproduce analyses, so that reviewers can double check the analyses and results.\nThe GRIM test, SPRITE test, and Statcheck are algorithms that check for internal consistency of reported results. They provide a ‚Äúsanity check‚Äù that can detect potentially p-hacked data analyses.\n\n\n8.11.2 Some closing remarks\nAs stated at the outset, there is great controversy over the appropriate use of statistical methods!\nSome wise words from participants in this controversy:\n\n‚ÄúWe often hear it‚Äôs too easy to obtain small p-values, yet replication attempts find it difficult to get small p-values with preregistered results. This shows the problem isn‚Äôt p-values but failing to adjust them for cherry picking, multiple testing, post- data subgroups and other biasing selection effects.‚Äù\n\n-Deborah Mayo, ‚ÄúDon‚Äôt throw out the error control baby with the bad statistics bathwater‚Äù\n\n‚ÄúIt seems to me that statistics is often sold as a sort of alchemy that transmutes randomness into certainty, an‚Äùuncertainty laundering‚Äù that begins with data and concludes with success as measured by statistical significance ‚Ä¶ the solution is not to reform p-values or to replace them with some other statistical summary or threshold, but rather to move toward a greater acceptance of uncertainty and embracing of variation.\n\nAndrew Gelman, ‚ÄúThe problems with p-values are not just p-values‚Äù"
  },
  {
    "objectID": "Ch9_Other_Topics.html#formal-model-selection-tools",
    "href": "Ch9_Other_Topics.html#formal-model-selection-tools",
    "title": "9¬† Brief looks at major topics we didn‚Äôt cover",
    "section": "9.1 Formal model selection tools",
    "text": "9.1 Formal model selection tools\n\n9.1.1 AIC / BIC\n\n\n9.1.2 Backwards and forwards selection\n\n\n9.1.3 Penalized regression (LASSO and Ridge)"
  },
  {
    "objectID": "Ch9_Other_Topics.html#bayesian-statistics",
    "href": "Ch9_Other_Topics.html#bayesian-statistics",
    "title": "9¬† Brief looks at major topics we didn‚Äôt cover",
    "section": "9.2 Bayesian statistics",
    "text": "9.2 Bayesian statistics\n\n9.2.1 Probability as rational degree of belief\n\n\n9.2.2 Priors\n\n\n9.2.3 Things Bayes permits that classical methods don‚Äôt"
  },
  {
    "objectID": "Ch9_Other_Topics.html#non-parametric-methods",
    "href": "Ch9_Other_Topics.html#non-parametric-methods",
    "title": "9¬† Brief looks at major topics we didn‚Äôt cover",
    "section": "9.3 Non-parametric methods",
    "text": "9.3 Non-parametric methods\n\n9.3.1 Alternatives for t-tests and ANOVA\n\n\n9.3.2 Alternatives for regression"
  },
  {
    "objectID": "Ch9_Other_Topics.html#meta-analysis",
    "href": "Ch9_Other_Topics.html#meta-analysis",
    "title": "9¬† Brief looks at major topics we didn‚Äôt cover",
    "section": "9.4 Meta-analysis",
    "text": "9.4 Meta-analysis\n\n9.4.1 Combining lots of studies\n\n\n9.4.2 Heterogeneity\n\n\n9.4.3 Interpretability"
  },
  {
    "objectID": "Ch9_Other_Topics.html#regression-to-the-mean",
    "href": "Ch9_Other_Topics.html#regression-to-the-mean",
    "title": "9¬† Brief looks at major topics we didn‚Äôt cover",
    "section": "9.5 Regression to the mean",
    "text": "9.5 Regression to the mean"
  }
]